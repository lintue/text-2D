{

  "title": [
    "text-2D test",
    "(scroll down for details)"
  ],

  "footer": "© 2021 pixel-tree MIT",

  "vis": [
    {
      "id": "textH",
      "body": [
        "t-SNE visualisation of OII research"
      ]
    },
    {
      "id": "textB",
      "body": [
        "This interactive graph can be used to explore thematic overlaps across research groups at the Oxford Internet Institute. (Note: I'm not affiliated with the OII and the documents presented here represent only a portion of their work.)",
        "Each data point represents an individual research article. Click on labels (bottom right) to highlight a group, and hover over data for publication details. The method is described below."
      ]
    },
    {
      "id": "textH",
      "body": [
        "[1]"
      ],
      "links": [
        {
          "Link to GitHub repository (code and data).": "https://github.com/pixel-tree/text-2D"
        }
      ]
    },
    {
      "id": "textH",
      "body": [
        "1. TF*IDF (using [1])"
      ],
      "links": [
        {
          "scikit-learn": "https://scikit-learn.org/stable/about.html#citing-scikit-learn"
        }
      ]
    },
    {
      "id": "textB",
      "body": [
        "The dataset consists of 900 publication abstracts. These are processed using the term frequency--inverse document frequency (TF*IDF) algorithm to produce scores that represent the relevance of words in documents.",
        "The algorithm tokenises the corpus by n-grams (unigrams and bigrams -- a feature dimension is allocated for each unique word and combination of two consecutive words). This excludes English stop words as well as words which sit outside of set frequency thresholds.",
        "N-gram frequency is recorded for each document, resulting in a sparse matrix, \\(\\mathbf{X}\\), with shape 900 (total number of samples; that is, abstracts) x 13000+ (total number of unique features). These values are multiplied by “global term weights” -- logarithmically scaled fractions that penalise for high term frequency across the corpus. Consequently, the highest score is obtained when there exists high local frequency (abstract-specific count) and low global prevalence (dataset-wide), which makes sense in terms of categorisation (we wish to de-emphasise words occurring frequently across the corpus).",
        "The TF*IDF formula used here (computed elementwise, feature by feature, for every document in the dataset):",
        "\\[tf(t, d) \\times idf(t),\\]",
        "where \\(tf(t, d)\\) represents the n-gram frequency for any given document and",
        "\\[idf(t) = \\log{\\frac{1 + N}{1 + df(t)}} + 1,\\]",
        "where \\(N\\) represents the total number of documents. Note that the constant is added to avoid zero division.",
        "The vectors are L2 normalised. In other words, for each document in \\(\\mathbf{X}\\):",
        "\\[\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n{\\lvert \\mathbf{x}_i \\rvert^2}} = 1,\\]",
        "and so, we have a similarity measure for comparing the TF*IDF scores.",
        "Furthermore, we compute a cosine similarity matrix by taking dot products of vectors in \\(\\mathbf{X}\\) and \\(\\mathbf{X}^T\\)(ransposed). The formula is explained below:",
        "\\[\\cos(\\theta) = \\frac{(\\mathbf{a} \\cdot \\mathbf{b})}{\\lvert \\mathbf{a} \\rvert \\lvert \\mathbf{b} \\rvert},\\]",
        "where \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) are arbitrary vectors and \\(\\theta\\) represents the angle between them.",
        "One nice thing here: the feature dimensions are assumed to be orthogonal, so vector magnitude, \\(\\lvert \\mathbf{x} \\rvert\\), is given by \\(\\sqrt{\\mathbf{x}_1^2 + \\mathbf{x}_2^2 + ... + \\mathbf{x}_n^2}\\) (and due to L2 norm this is equal to 1). Therefore, in the above formula we divide by \\(\\lvert \\mathbf{a} \\rvert \\lvert \\mathbf{b} \\rvert = 1 \\times 1 = 1\\), and we have that",
        "\\[\\cos(\\theta) = \\mathbf{X}_i \\cdot \\mathbf{X}_i^T,\\]",
        "where \\(\\mathbf{X}_i\\) is a vector in \\(\\mathbf{X}\\).",
        "Result: symmetric 900x900 matrix, or 900 vectors in \\(\\mathbb{R}^{900}\\), where values relate to angles between vectors (i.e., between research documents). And this is a representation of (dis)similarity in language."
      ]
    },
    {
      "id": "textH",
      "body": [
        "2. PCA"
      ]
    },
    {
      "id": "textB",
      "body": [
        "We reduce complexity with PCA: a linear mapping of the data to lower dimensions, while preserving properties of global structure. The first 50 principal components are included (where we assume that variance = signal).",
        "On the OII dataset, running PCA before t-SNE resulted in more defined clusters."
      ]
    },
    {
      "id": "textH",
      "body": [
        "3. t-SNE"
      ]
    },
    {
      "id": "textB",
      "body": [
        "Embed data into 2-D with t-SNE. Focus on neighbourhood structure, where we have various parameters to guide clustering behaviour."
      ]
    },
    {
      "id": "textH",
      "body": [
        "4. Visualised above using [1] (in-text maths typset using [2])"
      ],
      "links": [
        {
          "D3.js": "https://d3js.org"
        },
        {
          "MathJax": "https://www.mathjax.org"
        }
      ]
    }
  ]

}
