text;title;author;year;source page;label(s)
RT (formerly, Russia Today) is one of the most important organizations in the global political economy of disinformation. It is the most richly funded, well-staffed, formal organization in the world producing, disseminating, and marketing news in the service of the Kremlin. It is an agency accused of many things, but little is known about all the creative work involved in financing, governing, training, and motivating RT’s activities. To understand more about the production of political news and information by RT, we investigate its organizational behavior through in-depth interviews of current and former staff. Our data show that RT is an opportunist channel that is used as an instrument of state defense policy to meddle in the politics of other states. The channel has been established in the shadows of the Soviet media system and its organizational behavior is characterized by Soviet-style controls.;“Anything that Causes Chaos”: The Organizational Behavior of Russia Today;Elswah, M. & Howard, P.;2020;Howard, P.;Digital Politics and Government
Social media is an important source of news and information in the United States. But during the 2016 US presidential election, social media platforms emerged as a breeding ground for influence campaigns, conspiracy, and alternative media. Anecdotally, the nature of political news and information evolved over time, but political communication researchers have yet to develop a comprehensive, grounded, internally consistent typology of the types of sources shared. Rather than chasing a definition of what is popularly known as “fake news,” we produce a grounded typology of what users actually shared and apply rigorous coding and content analysis to define the phenomenon. To understand what social media users are sharing, we analyzed large volumes of political conversations that took place on Twitter during the 2016 presidential campaign and the 2018 State of the Union address in the United States. We developed the concept of “junk news,” which refers to sources that deliberately publish misleading, deceptive, or incorrect information packaged as real news. First, we found a 1:1 ratio of junk news to professionally produced news and information shared by users during the US election in 2016, a ratio that had improved by the State of the Union address in 2018. Second, we discovered that amplifier accounts drove a consistently higher proportion of political communication during the presidential election but accounted for only marginal quantities of traffic during the State of the Union address. Finally, we found that some of the most important units of analysis for general political theory—parties, the state, and policy experts—generated only a fraction of the political communication.;Sourcing and Automation of Political News and Information over Social Media in the United States, 2016-2018;Bradshaw, S. & Howard, P. ;2019;Howard, P.;Digital Politics and Government
A 2016 review of literature about automation, algorithms and politics identified China as the foremost area in which further research was needed because of the size of its population, the potential for Chinese algorithmic manipulation in the politics of other countries, and the frequency of exportation of Chinese software and hardware. This paper contributes to the small body of knowledge on the first point (domestic automation and opinion manipulation) and presents the first piece of research into the second (international automation and opinion manipulation). Findings are based on an analysis of 1.5 million comments on official political information posts on Weibo and 1.1 million posts using hashtags associated with China and Chinese politics on Twitter. In line with previous research, little evidence of automation was found on Weibo. In contrast, a large amount of automation was found on Twitter. However, contrary to expectations and previous news reports, no evidence was found of pro-Chinese-state automation on Twitter. Automation on Twitter was associated with anti-Chinese-state perspectives and published in simplified Mandarin, presumably aimed at diasporic Chinese and mainland users who ‘jump the wall’ to access blocked platforms. These users come to Twitter seeking more diverse information and an online public sphere but instead they find an information environment in which a small number of anti-Chinese-state voices are attempting to use automation to dominate discourse. Our understanding of public conversation on Twitter in Mandarin is extremely limited and, thus, this paper advances the understanding of political communication on social media.;Chinese computational propaganda: automation, algorithms and the manipulation of information about Chinese politics on Twitter and Weibo;Bolsover, G. & Howard, P.;2018;Howard, P.;Digital Politics and Government
Voters increasingly rely on social media for news and information about politics. But increasingly, social media has emerged as a fertile soil for deliberately produced misinformation campaigns, conspiracy, and extremist alternative media. How does the sourcing of political news and information define contemporary political communication in different countries in Europe? To understand what users are sharing in their political communication, we analyzed large volumes of political conversation over a major social media platform—in real-time and native languages during campaign periods—for three major European elections. Rather than chasing a definition of what has come to be known as “fake news,” we produce a grounded typology of what users actually shared and apply rigorous coding and content analysis to define the types of sources, compare them in context with known forms of political news and information, and contrast their circulation patterns in France, the United Kingdom, and Germany. Based on this analysis, we offer a definition of “junk news” that refers to deliberately produced misleading, deceptive, and incorrect propaganda purporting to be real news. In the first multilingual, cross-national comparison of junk news sourcing and consumption over social media, we analyze over 4 million tweets from three elections and find that (1) users across Europe shared substantial amounts of junk news in varying qualities and quantities, (2) amplifier accounts drive low to medium levels of traffic and news sharing, and (3) Europeans still share large amounts of professionally produced information from media outlets, but other traditional sources of political information including political parties and government agencies are in decline.;Sourcing and Automation of Political News and Information During Three European Elections;Neudert, L-M., Howard, P. & Kollanyi, B.;2019;Howard, P.;Digital Politics and Government
Social media has emerged as a powerful tool for political engagement and expression. However, state actors are increasingly leveraging these platforms to spread computational propaganda and disinformation during critical moments of public life. These actions serve to nudge public opinion, set political or media agendas, censor freedom of speech, or control the flow of information online. Drawing on data collected from the Computational Propaganda Project’s 2017 investigation into the global organization of social-media manipulation, we examine how governments and political parties around the world are using social media to shape public attitudes, opinions, and discourses at home and abroad. We demonstrate the global nature of this phenomenon, comparatively assessing the organizational capacity and form these actors assume, and discuss the consequences for the future of power and democracy.;The Global Organization of Social Media Disinformation Campaigns;Howard, P. & Bradshaw, S.;2018;Howard, P.;Digital Politics and Government
Political communication is the process of putting information, technology, and media in the service of power. Increasingly, political actors are automating such processes, through algorithms that obscure motives and authors yet reach immense networks of people through personal ties among friends and family. Not all political algorithms are used for manipulation and social control however. So what are the primary ways in which algorithmic political communication—organized by automated scripts on social media—may undermine elections in democracies? In the US context, what specific elements of communication policy or election law might regulate the behavior of such “bots,” or the political actors who employ them? First, we describe computational propaganda and define political bots as automated scripts designed to manipulate public opinion. Second, we illustrate how political bots have been used to manipulate public opinion and explain how algorithms are an important new domain of analysis for scholars of political communication. Finally, we demonstrate how political bots are likely to interfere with political communication in the United States by allowing surreptitious campaign coordination, illegally soliciting either contributions or votes, or violating rules on disclosure.;Algorithms, bots, and political communication in the US 2016 election: The challenge of automated political communication for election law and administration;Howard, P., Woolley, S. & Calo, R.;2018;Howard, P.;Digital Politics and Government
"Computational propaganda has recently exploded into public consciousness. The U.S. presidential campaign of 2016 was marred by evidence, which continues to emerge, of targeted political propaganda and the use of bots to distribute political messages on social media. This computational propaganda is both a social and technical phenomenon. Technical knowledge is necessary to work with the massive databases used for audience targeting; it is necessary to create the bots and algorithms that distribute propaganda; it is necessary to monitor and evaluate the results of these efforts in agile campaigning. Thus, a technical knowledge comparable to those who create and distribute this propaganda is necessary to investigate the phenomenon.";Computational Propaganda and Political Big Data: Moving Toward a More Critical Research Agenda;Bolsover, G. & Howard, P.;2017;Howard, P.;Digital Politics and Government
Does social media use have a positive or negative impact on civic engagement? The cynical “slacktivism hypothesis” holds that if citizens use social media for political conversation, those conversations will be fleeting and vapid. Most attempts to answer this question involve public opinion data from the United States, so we offer an examination of an important case from Mexico, where an independent candidate used social media to communicate with the public and eschewed traditional media outlets. He won the race for state governor, defeating candidates from traditional parties and triggering sustained public engagement well beyond election day. In our investigation, we analyze over 750,000 posts, comments, and replies over three years of conversations on the public Facebook page of “El Bronco.” We analyze how rhythms of political communication between the candidate and users evolved over time and demonstrate that social media can be used to sustain a large quantity of civic exchanges about public life well beyond a particular political event.;Social Media, Civic Engagement, and the Slacktivism Hypothesis: Lessons From Mexico’s “El Bronco”;Howard, P., Savage, S., Saviaga, C. F., Toxtli, C. & Monroy-Hernandez, A.;2016;Howard, P.;Digital Politics and Government
We review the great variety of critical scholarship on algorithms, automation, and big data in areas of contemporary life both to document where there has been robust scholarship and to contribute to existing scholarship by identifying gaps in our research agenda. We identify five domains with opportunities for further scholarship: (a) China, (b) international interference in democratic politics, (c) civic engagement in Latin American, (d) public services, and (e) national security and foreign affairs. We argue that the time is right to match dedication to critical theory of algorithmic communication with a dedication to empirical research through audit studies, network ethnography, and investigation of the political economy of algorithmic production.;Automation, Algorithms, and Politics | Automation, Big Data and Politics: A Research Review;Shorey, S. & Howard, P.;2016;Howard, P.;Digital Politics and Government
The Internet certainly disrupted our understanding of what communication can be, who does it, how, and to what effect. What constitutes the Internet has always been an evolving suite of technologies and a dynamic set of social norms, rules, and patterns of use. But the shape and character of digital communications are shifting again—the browser is no longer the primary means by which most people encounter information infrastructure. The bulk of digital communications are no longer between people but between devices, about people, over the Internet of things. Political actors make use of technological proxies in the form of proprietary algorithms and semiautomated social actors—political bots—in subtle attempts to manipulate public opinion. These tools are scaffolding for human control, but the way they work to afford such control over interaction and organization can be unpredictable, even to those who build them. So to understand contemporary political communication—and modern communication broadly—we must now investigate the politics of algorithms and automation. ;Political Communication, Computational Propaganda, and Autonomous Agents ;Woolley, S. & Howard, P.;2016;Howard, P.;Digital Politics and Government
This is a response to the article by Ethan Zuckerman “New Media, New Civics?” published in this issue of Policy & Internet (2014: vol. 6, issue 2). Dissatisfaction with existing governments, a broad shift to “post‐representative democracy” and the rise of participatory media are leading toward the visibility of different forms of civic participation. Zuckerman's article offers a framework to describe participatory civics in terms of theories of change used and demands places on the participant, and examines some of the implications of the rise of participatory civics, including the challenges of deliberation in a diverse and competitive digital public sphere. Philip Howard responds.;Participation, Civics and Your Next Coffee Maker;Howard, P.;2014;Howard, P.;Digital Politics and Government
"What is the recipe for good information policy? Hosman and Howard address this in an emerging economy context through case studies of six states that arose following the dissolution of the former Yugoslavia. These new nations pursued differing information policy paths that led to diverse outcomes. The authors find, in general, conventional positive outcomes supporting policies for privatization, liberalization, and competition; but at the same time discover many counterintuitive outcomes based on each country's unique circumstances. General rules are good, but in specific cases alternative paths can also lead to success.";Telecom Policy Across the Former Yugoslavia: Incentives, Challenges, and Lessons Learned;Hosman, L. & Howard, P.;2014;Howard, P.;Digital Politics and Government
It has been 15 years since the last wave of democratization. But as a region, North Africa and the Middle East were noticeably devoid of popular democracy movements—until the early months of 2011. Democratization movements had existed long before technologies like mobile phones and the Internet came to these countries. But with these technologies, people sharing an interest in democracy built extensive networks and activated collective action movements for political change. What might have made regimes more susceptible than others to these uprisings, and what might explain the relative successes of some movements over others? What role does information technology have in the modern recipe for democratization? Weighing multiple political, economic, demographic, and cultural conditions, we find that information infrastructure—especially mobile phone use—consistently appears as one of the key ingredients in parsimonious models for the conjoined combinations of causes behind regime fragility and social movement success. To understand the successes and failures of contemporary political protests, we must also assess how civil society leaders and authoritarian security forces treat communication technologies as democratically consequential.;What Best Explains Successful Protest Cascades? ICTs and the Fuzzy Causes of the Arab Spring;Hussain, M. & Howard, P.;2013;Howard, P.;Digital Politics and Government
"This introductory essay highlights the key findings, methodological tool kit, and production process of this Special Issue. We argue that communication researchers are uniquely positioned to analyze the relationships between social media and political change in careful and nuanced ways, in terms of both causes and consequences. Finally, we offer a working definition of social media, based on the diverse and considered uses of the term by the contributors to the collection. Social media consists of (a) the information infrastructure and tools used to produce and distribute content that has individual value but reflects shared values; (b) the content that takes the digital form of personal messages, news, ideas, that becomes cultural products; and (c) the people, organizations, and industries that produce and consume both the tools and the content.";Social Media and Political Change: Capacity, Constraint, and Consequence;Howard, P. & Parks, M.;2012;Howard, P.;Digital Politics and Government
"During the ""Arab Spring,"" young tech savvy activists led uprisings in a dozen countries across North Africa and the Middle East. At first, digital media allowed democratization movements to develop new tactics for catching dictators off guard. Eventually, authoritarian governments worked social media into their own counter-insurgency strategies. What have we learned about the role of digital media in modern protest? Digital media helped to turn individualized, localized, and community-specific dissent into structured movements with a collective consciousness about both shared grievances and opportunities for action.";The Upheavals in Egypt and Tunisia: The Role of Digital Media;Howard, P. & Hussain, M.;2011;Howard, P.;Digital Politics and Government
Although there have been many studies of the different ways regimes censor the use of social media by their citizens, shutting off social media altogether is something that rarely happens. However, it happens at the most politically sensitive times and has widespread—if not global—consequences for political, economic and cultural life. When do states disconnect their digital networks, and why? To answer this question, the authors build an event history database of incidents in which a regime went beyond mere censorship of particular websites or users. The authors draw from multiple sources, including major news media, specialized news services, and international experts, to construct an event log database of 566 incidents. This rich, original dataset allows for a nuanced analysis of the conditions for state action, and the authors offer some assessment of the effect of such desperate action. Comparative analysis indicates that both democratic and authoritarian regimes disable social media networks for citing concerns about national security, protecting authority figures, and preserving cultural and religious morals. Whereas democracies disable social media with the goal of protecting children, authoritarian regimes also attempt to eliminate what they perceive as propaganda on social media. The authors cover the period 1995–2011 and build a grounded typology on the basis of regime type, what states actually did to interfere with digital networks, why they did it, and who was affected.;When Do States Disconnect Their Digital Networks? Regime Responses to the Political Uses of Social Media;Howard, P., Agarwal, S. & Hussain, M.;2011;Howard, P.;Digital Politics and Government
Technology convergence and rising expectations for interactivity have had a significant impact on the news diets of U.S. voters. While television may appear to be the most important single media in this system of political communication, for a growing portion of the population, news diets are defined by combinations and permutations of secondary media. What explains the changing distribution of primary media choice and the dramatic rise in secondary media? We offer a theory of omnivorous information habits to help explain the rising number of people who make active choices to get political news and information from several media technologies, sourced from multiple news organizations, and then engage with news and information through varied interactive tools. Data from 2000, 2004, and 2008 demonstrate not just the growing importance of secondary media, but the importance of the Internet in particular. Indeed, elections have become occasions in which people make significant changes in their information diets.;Information Technologies and Omnivorous News Diets over Three U.S. Presidential Elections;Massanari, A. & Howard, P.;2011;Howard, P.;Digital Politics and Government
"This study analyzes the privacy practices of political parties in Australia, Canada, the United Kingdom, and the United States. Comparative analysis — across countries and between political parties — reveals much about the strengths and weaknesses of current practices, and the role of political parties and policy makers in protecting voter privacy. Candidates, parties, lobby groups and data‐mining firms have violated the privacy norms of many citizens in all four countries. Political parties themselves face relatively few restrictions, and have developed a wide variety of largely voluntary website privacy policies that rarely extend to the use of voter datasets. There is great variety in the forms of public policy oversight: in Australia the parties are exempt from oversight; in Canada and the United Kingdom policy oversight is developing and preventative; in the United States party behavior is regulated by decentralized government agencies in an ad hoc manner. Moreover, country-specific voter data, analytical tools, and consulting expertise increasingly moves across borders. The study assesses the current state of the art of voter data mining, offers a history of the political data‐mining industry, and compare the capacity of regulatory bodies in these countries to protect the privacy of voters.";Political Parties & Voter Privacy: Australia, Canada, the United Kingdom, and United States in Comparative Perspective;Howard, P. & Kreiss, D.;2009;Howard, P.;Digital Politics and Government
Pundits and scholars laud online campaigning for its potential to democratize politics and praise the 2008 Barack Obama campaign for using new information technologies to mobilize voters. Underneath these extraordinary forms of technologically-enabled political participation, however, is an infrastructure and industry for political data that has received far less attention. To help fill this gap in scholarly understanding, we provide an overview of the data practices of political campaigns over the last decade and take a particularly close look at many of the new tools used by the Obama campaign. As a call for further research, we then outline a set of potential normative concerns about this use of data. We suggest that the data practices of campaigns and other political organizations may undermine important democratic norms. Campaigns erode privacy and narrow political debate by using data on citizens and social networks to tailor messages and communicate with narrowly-defined segments of voters. The lack of policy oversight erodes institutional transparency and leaves citizens vulnerable to breeches in personal data. ;New Challenges to Political Privacy: Lessons from the First U.S. Presidential Race in the Web 2.0 Era;Kreiss, D. & Howard, P.;2010;Howard, P.;Digital Politics and Government
What is the best way to measure and track the digital divide, in a comparative manner, over time? What impact have differing policy interventions had on the digital divide in Canada and the United States? We offer a way of benchmarking equality in Internet access using Gini coefficients and demonstrate that overall the digital divide has been closing in both countries. We find that in terms of income, the digital divide in Canada has closed most dramatically, and that in terms of education, the digital divide remains most pronounced in the United States. We suggest that Canada has been more successful in reducing the concentration of Internet access among wealthy educated populations, in part due to the active role of the state in supporting the production of culturally relevant digital content.;Comparing Digital Divides: Internet Access and Social Inequality in Canada and the United States;Howard, P., Busch, L. & Sheets, P.;2010;Howard, P.;Digital Politics and Government
How do telecommunications policies influence technology adoption? Has regulatory reform helped mitigate or exacerbate the digital divide? We examine the effects of four policy reform strategies on a country’s share of internet bandwidth, internet hosts, internet users, personal computers, and mobile phones. We argue that the best policy environment for the telecommunications sector is one maintained by an independent regulator that is not above representing the public interest or entering into public private partnerships to develop national information infrastructure. Holding other factors constant, privatizing the national telecommunications operator only has a few demonstrable effects, and the effects are mixed. Liberalizing the market for consumer communications services and separating the telecommunications regulator from direct control by the executive branch of government are, for the most part, constructive policies for encouraging technology adoption. Yet over time, too little public policy oversight usually has a negative impact on technology adoption. Regulatory independence mitigates against the digital divide, but regulatory withdrawal exacerbates it. Our findings offer greater coverage than prior research, and emerge from a time-series cross-sectional study of multiple technology indicators in 154 countries during the period 1990–2007.;Telecommunications Reform, Internet Use and Mobile Phone Adoption in the Developing World;Howard, P. & Mazaheri, N.;2009;Howard, P.;Digital Politics and Government
Every year millions of digital records containing personally identifiable information are exposed. When are malicious hackers to blame, and when is it organizational malfeasance? Which kinds of organizations—private firms, government agencies, or educational institutions—lose the most data? With over 1.9 billion records lost (on average that's 9 records per U.S. adult), a surprising number of breaches can be attributed to organizational practices.;Data Collection and Leakage;Howard, P. & Erickson, K.;2010;Howard, P.;Digital Politics and Government
When researchers study technology diffusion in a global and comparative manner, they often find that economic productivity explains differences in the diffusion of information and communication technologies (ICTs). But when researchers study technology diffusion in a regional, national, or subnational context, they often find that politics and culture explains different diffusion rates. How do we make use of different kinds of conclusions drawn from different levels of analysis? Just knowing the ways in which wealth explains technology diffusion can obscure the ways in which politics and culture also explain patterns in technology diffusion. In this article, we offer a new perspective on weighting technology diffusion data by economic wealth to set into sharp relief the ways in which other factors—such as politics and culture—influence how well a country metabolizes new technologies. A simple but useful computation is offered, examples are assessed, and implications for public policy, industry, and research are discussed.;Sizing Up Information Societies: Toward a Better Metric for the Cultures of ICT Adoption;Howard, P., Anderson, K., Busch, L. & Dawn, N.;2009;Howard, P.;Digital Politics and Government
The computer hacker is one of the most vilified figures in the digital era, but to what degree are organizations actually responsible for compromised personal records? To examine the role of organizational behavior in privacy violations, we analyze 589 incidents of compromised data between 1980 and 2006. There were more reported incidents in 2005 and 2006 than in the previous 25 years combined. Excluding a particularly large security breach at Acxiom, hackers account for the largest volume of compromised records, some 45%, while 27% of the volume is attributed to organizational mismanagement and 28% remains unattributed. In terms of incidents, 9% were an unspecified type of breach, 31% of the incidents involved hackers, and 60% of the incidents involved organizational mismanagement: personally identifiable information accidentally placed online, missing equipment, lost backup tapes, or other administrative errors. Options for public policy oversight are discussed.;A Case of Mistaken Identity? News Accounts of Hacker, Consumer, and Organizational Responsibility for Compromised Digital Records;Erickson, K. & Howard, P.;2007;Howard, P.;Digital Politics and Government
Over the last decade, the FCC has licensed the use of the public spectrum with an online auction system. Has this auction system, which requires potential licensees to first apply to qualify as bidders, increased the diversity of ownership, and, if not, why? Analyzing data on 28,354 applicants for 16,383 licenses, we find that while one in ten applicant businesses were minority-owned, only one in thirteen qualified bidders and fewer than one in fourteen bid winners were minority-owned. Controlling for other factors, minority-owned businesses and first-time applicants are much more likely to be interested in a media broadcast license than a license to operate other communications services on the public spectrum. Businesses that are minority-owned or woman-owned, or first-time applicants, are less likely than other types of businesses to successfully qualify to bid.;Channeling Diversity in the Public Spectrum: Who Qualifies to Bid for Which FCC Licenses?;Howard, P. & Smith, S.;2007;Howard, P.;Digital Politics and Government
Using data from the Pew Internet and American Life Project surveys, this article explores changing trends in reported sophistication and satisfaction with search skills and with search engines. We find that the proportion of Internet users searching online for answers to specific questions—as opposed to casual browsing—has grown significantly. Moreover, as users get more experience online, they increasingly become dependent on search engines, confident in their findings, and savvy about how search engines structure information, privilege paid results, and track users. When other factors are controlled, years of online experience is a strong predictor of the likelihood of a person doing specific searches on a daily basis, and experience can have an even stronger positive effect than education and income. We also find that years of online experience, frequency of use, and sophistication with multiple search engines can overcome socio-economic status in predicting how active a person is in searching across different topics.;Learning to Search and Searching to Learn: Income, Education, and Experience Online;Howard, P. & Massanari, A.;2007;Howard, P.;Digital Politics and Government
This paper tests the ‘leap-frog’ hypothesis by modeling the impact of existing telecommunications infrastructure, controlling for economic, political and demographic factors, on changes in information communication technology (ICT) access for over 200 countries between 1995 and 2005. This study has significantly greater coverage than previous research, in terms of both time frame and country cases. First, the analysis demonstrates that in the first decade of the information society successful leap-frog countries are few and far between. Second, the relative distribution of personal computers, internet hosts and secure servers among the nations of the world has barely improved over the last decade. Third, contrary to received wisdom, most of the countries that might qualify as successful leap-frog countries are actually among the wealthiest in the world. Finally, while policy reform in the telecommunications sector can sometimes speed the diffusion of digital communication tools, the record of market reforms is mixed, and the overall effect of economic wealth is still paramount. In sum, a few poor countries have leapt ahead in the development of a few aspects of ICT infrastructure and use, but these relatively rare successes are more likely to be due to economic productivity than to privatization, regulatory separation and depoliticization, or market liberalization in the telecommunications sector.;TESTING THE LEAP-FROG HYPOTHESIS: The impact of existing infrastructure and telecommunications policy on the global digital divide;Howard, P.;2007;Howard, P.;Digital Politics and Government
Digital media strategies are a crucial component of contemporary political campaigns. Established political elites use database and Internet technologies to raise money, organize volunteers, gather intelligence on voters, and do opposition research. However, they use data-mining techniques that outrage privacy advocates and surreptitious technologies that few Internet users understand. Grassroots political actors and average votersbuild their own digital campaigns, researching public policy options, candidate histories, lobbyist maneuvering, and the finances of big campaigns. I examine the role of digital technologies in the production of contemporary political culture with ethnographic and survey evidence from four election seasons between 1996 and 2002. Democracy is deeper in terms of the diffusion of rich data about political actors, policy options, and the diversity of actors and opinion in the public sphere. Citizenship is thinner in terms of the ease in which people can become politically expressive without being substantively engaged.;Deep Democracy, Thin Citizenship: The Impact of Digital Media in Political Campaign Strategy;Howard, P.;2005;Howard, P.;Digital Politics and Government
Many new media technologies, such as the internet, serve both as a tool for organizing public commo ns and as a tool for surveilling private lives. This paper addresses the manner in which such technological innovations have enabled a dramatically expanded market for public policy opinion data, and explores the potential role of that market in facilitating panoptic regimes of both private and state surveillance. Whereas information about public policy opinion used to be highly reductive, expensive to collect, and restricted to a limited number of powerful political actors, today it is much less expensive, highly nuanced, and widely available. Pollsters now also have the ability to extrapolate political information from our commercial and noncommercial activities. We investigate the work of two organizations, a public policy polling firm named Grapevine Polling, and an advocacy consulting firm named United Campaigns. We find that both the increased sophistication of these firms' methods and the reduced cost of increasingly personalized data together have the potential to undermine the very public sphere that digital media were hoped to reinvigorate. Moreover, overlapping state and private demand for the products of such pollsters reflects the extent to which politics and the marketplace are increasingly intertwined and inseparable under the current articulation of democracy in the US.;Digital Technology and the Market for Political Surveillance;Howard, P., Carr, J. & Milstein, T.;2002;Howard, P.;Digital Politics and Government
Campaigns are complex exercises in the creation, transmission, and mutation of significant political symbols. However, there are important differences between political communication through new media and political communication through traditional media. I argue that the most interesting change in patterns of political communication is in the way political culture is produced, not in the way it is consumed. These changes are presented through the findings from systematic ethnographies of two organizations devoted to digitizing the social contract. DataBank.com is a private data mining company that used to offer its services to wealthier campaigns, but can now sell data to the smallest nascent grassroots movements and individuals. Astroturf-Lobby.org is a political action committee that helps lobbyists seek legislative relief to grievances by helping these groups find and mobilize their sympathetic publics. I analyze the range of new media tools for producing political culture, and with this ethnographic evidence build two theories about the role of new media in advanced democracies-a theory of thin citizenship and a theory about data shadows as a means of political representation.;Digitizing the Social Contract: Producing American Political Culture in the Age of New Media;Howard, P.;2010;Howard, P.;Digital Politics and Government
Social scientists are increasingly interested in innovative organizational forms made possible with new media, known as epistemic communities, knowledge networks, or communities of practice, depending on the discipline. Some organizational forms can be difficult to study qualitatively because human, social, cultural, or symbolic capital is transmitted over significant distances with technologies that do not carry the full range of human expression that an ethnographer or participant observer hopes to experience. Whereas qualitative methods render rich description of human interaction, they can be unwieldy for studying complex formal and informal organizations that operate over great distances and through new media. Whereas social network analysis renders an overarching sketch of interaction, it will fail to capture detail on incommensurate yet meaningful relationships. Using social network analysis to justify case selection for ethnography, I propose `network ethnography' as a synergistic research design for the study of the organizational forms built around new media.;Network Ethnography and the Hypermedia Organization: New Media, New Organizations, New Methods;Howard, P.;2002;Howard, P.;Digital Politics and Government
For a growing cohort of Americans, Internet tools have become a significant conduit of social life and work life. The surveys of the Pew Internet & American Life Project in 2000 show that more than 52 million Americans went online each day, and there are significant differences in use between men and women, young and old, those of different races and ethnic groups, and those of different socioeconomic status. A user typology can be built around two variables: the length of time a person has used the Internet and the frequency with which he or she logs on from home. The authors contend that use of e-mail helps people build their social networks by extending and maintaining friend and family relationships.;Days and Nights on the Internet: The Impact of a Diffusing Technology;Howard, P., Rainie, L. & Jones, S.;2001;Howard, P.;Digital Politics and Government
"As the 2019 Online Harms White paper (OHWP) notes, the Internet is an increasingly integral part of our lives, and can offer 'significant benefits'. In order to ensure these benefits are not undermined, the OHWP argues that new regulation is needed to reduce a wide array of 'online harms' such as those described in the joint ministerial foreword: 'In the wrong hands the Internet can be used to spread terrorist and other illegal or harmful content, undermine civil discourse, and abuse or bully other people'. ""Ihis opening statement (and, indeed, its joint authorships) is key to understanding many of the challenges of this policy proposal, insofar as at its heart lies an unhelpful elision between illegal and legal-but-harmful content. By conjoining these two issues the OHWP weakens its own case for new regulation, but this is by no means the only flaw Of the proposed approach. Drawing on scholar- ship from Internet (social science) research and policy rather than media law, I highlight below the main limitations and outline suggestions for a more coherent approach.";Revise and resubmit? Reviewing the 2019 Online Harms White Paper;Nash, V.;2019;Nash, V.;"Digital Politics and Government; Education, Digital Life and Wellbeing; Information Governance and Security"
Early adolescents are spending an increasing amount of time online, and a significant share of caregivers now use Internet filtering tools to shield this population from online sexual material. Despite wide use, the efficacy of filters is poorly understood. In this article, we present two studies: one exploratory analysis of secondary data collected in the European Union (n = 13,176), and one preregistered study focused on British adolescents and caregivers (n = 1,004) to rigorously evaluate their utility. In both studies, caregivers were asked about their use of Internet filtering, and adolescent participants were interviewed about their recent online experiences. Analyses focused on the absolute and relative risks of young people encountering online sexual material and the effectiveness of Internet filters. Results suggested that caregiver's use of Internet filtering had inconsistent and practically insignificant links with young people reports of encountering online sexual material. Our findings underscore the need for randomized controlled trials to determine the extent to which Internet filtering and related technologies support versus thwarts young people online, and if their perceived utility justifies their financial and informational costs.;Internet Filtering and Adolescent Exposure to Online Sexual Material;Przybylski, A. & Nash, V.;2018;Nash, V.;"Digital Politics and Government; Education, Digital Life and Wellbeing; Information Governance and Security"
"This editorial introduces the articles in our Special Issue (9:3–4) of Policy & Internet on “The Platform Society,” arising from the journal's IPP2016 conference held at the University of Oxford on September 22–23, 2016. The editorial provides an outline of existing academic research on online platforms; discusses platform labor, platform governance, and platform politics as three key research themes; and discusses the implications for public policy and future research directions.";Public Policy in the Platform Society;Nash, V., Bright, J., Margetts, H. & Lehdonvirta, V.;2017;Nash, V.;"Digital Politics and Government; Education, Digital Life and Wellbeing; Information Governance and Security; Digital Economies; Information Geography and Inequality"
To evaluate the effectiveness of Internet filtering tools designed to shield adolescents from aversive experiences online. A total of 1030 in-home interviews were conducted with early adolescents aged from 12 to 15 years (M = 13.50, SD = 1.18) and their caregivers. Caregivers were asked about their use of Internet filtering and adolescent participants were interviewed about their recent online experiences. Contrary to our hypotheses, policy, and industry advice regarding the assumed benefits of filtering we found convincing evidence that Internet filters were not effective at shielding early adolescents from aversive online experiences. Preregistered prospective and randomised controlled trials are needed to determine the extent to which Internet filtering technology supports vs thwarts young people online and if their widespread use justifies their financial and informational costs.;Internet Filtering Technology and Aversive Online Experiences in Adolescents;Przybylski, A. & Nash, V.;2017;Nash, V.;"Digital Politics and Government; Education, Digital Life and Wellbeing; Information Governance and Security"
"Intense media and policy focus on issues of online child protection have prompted a resurgence of moral panics about children and adolescents' Internet use, with frequent confounding of different types of risk and harm and little reference to empirical evidence of actual harm. Meanwhile, within the academic literature, the quantity and quality of studies detailing the risks and opportunities of online activity for children and young people has risen substantially in the past 10 years, but this is also largely focused on risk rather than evidence of harm. Whilst this is understandable given the methodological and ethical challenges of studying Internet-related harms to minors, the very concept of risk is dependent on some prior understanding of harm, meaning that without efforts to study what harms are connected with children's online experiences, discussions of risk lack a strong foundation. This article makes a key contribution to the field by reviewing available evidence about the scale and scope of online harms from across a range of disciplines and identifying key obstacles in this research area as well as the major policy implications. The findings are based on a review of 148 empirical studies. Results were found in relation to main types of harms: health-related harms as a result of using pro-eating disorder, self-harm or pro-suicide websites; sex-related harms such as Internet-initiated sexual abuse of minors and cyber-bullying.";Evidence on the extent of harms experienced by children as a result of online risks: implications for policy and research;Slavtcheva-Petkova, Nash, V. & Bulger, M.;2014;Nash, V.;"Digital Politics and Government; Education, Digital Life and Wellbeing; Information Governance and Security"
The same characteristics that make the Internet so unique as a tool for also create concerns that dangerous and illegal content and interactions are more easily available, particularly to children. This article explores these issues by examining the debate between two long-established strands of digital advocacy: child protection and freedom of expression. It suggests the value of a new analytic framework and model of intervention, arguing that a negotiation of values characterizes a policy development ecology. This article describes an “ecology of values” based on the phronetic, rather than epistemic, aspects of the discursive relationships created between members of these two advocacy groups, where core values are negotiated and redefined as part of the policymaking process.;Beyond Rational Games: An Analysis of the “Ecology of Values” in Internet Governance Debates;Powell, A. & Nash, V.;2013;Nash, V.;"Digital Politics and Government; Education, Digital Life and Wellbeing; Information Governance and Security"
"This article examines the politics of higher education expansion in Britain between the 1960s and 1990s, bookmarked by the Robbins and Deanng reports respectively. We argue that throughout this period higher education expansion has been employed, by all political parties, as an instrument of economic growth though this justification has been dressed up to satisfy transient ideological preferences. We first present a detailed account, based on archival and primary sources, of the three major reforms to higher education since the 1950s—Robbins, Thatcherism, and Dearing; and second, develop an argument about how the dominant ideas about the rationale for higher education constitute a form of path dependency for policy choices once these institutions are publicly funded and explicitly linked to economic needs.";Continuity of Ideas and the Politics of Higher Education Expansion in Britain from Robbins to Dearing;King, D. & Nash, V.;2001;Nash, V.;"Digital Politics and Government; Education, Digital Life and Wellbeing; Information Governance and Security"
This report provides a new perspective on the social and political dynamics behind the threats to expression. It develops a conceptual framework on the ‘ecology of freedom of expression’ for discussing the broad context of policy and practice that should be taken into consideration in discussions of this issue. Over the first decade of the 21st century, the Internet and its convergence with mobile communications has enabled greater access to information and communication resources. In 2010, nearly 2 billion people worldwide – over one quarter of the world’s population – use the Internet. However, during the same period, defenders of digital rights have raised growing concerns over how legal and regulatory trends might be constraining online freedom of expression. Anecdotal accounts of the arrests of bloggers, the filtering of content and the disconnection of users have sparked these concerns. However, they are reinforced by more systematic studies that provide empirical evidence of encroachments on freedom of expression, such as through the increased use of content filtering.;Freedom of connection, freedom of expression: the changing legal and regulatory ecology shaping the Internet;Dutton, W., Dopatka, A. & Ginette, L. et al.;2011;Nash, V.;"Digital Politics and Government; Education, Digital Life and Wellbeing; Information Governance and Security"
"This one-year Oxford Internet Institute research project set out to explore the lessons learnt by the online gambling industry with respect to the successful application of age verification processes. These experiences and lessons were compared with age verification techniques applied in other industries including parts of the online retail and social gaming sectors. Specifically we sought to: Understand the rationale for the use or non-use of age verification in three case studies, (online gambling, online sale of age-restricted goods and social gaming); Explore lessons learnt from the development and deployment of existing age verification practices in these sectors ; Identify examples of good practice to inform future practice across online industry sectors. Our study investigated these objectives in three overlapping steps: a literature review of academic research, policy documents and publicly available market analysis; 38 in-depth interviews with experts on age verification drawn from across three online industry sectors as well as regulatory bodies, identity and payment providers and children’s charities; a workshop with experts in the field to gain feedback on draft findings. ";Effective age verification techniques: Lessons to be learnt from the online gambling industry ;Nash, V., O'Connell, R. & Zevenbergen, B. et al.;2013;Nash, V.;"Digital Politics and Government; Education, Digital Life and Wellbeing; Information Governance and Security"
Social and behavioural scientists have attempted to speak to the COVID-19 crisis. But is behavioural research on COVID-19 suitable for making policy decisions? We offer a taxonomy that lets our science advance in ‘evidence readiness levels’ to be suitable for policy. We caution practitioners to take extreme care translating our findings to applications.;Use caution when applying behavioural science to policy;Ijzerman, H., Lewis, N. Jr., Przybylski, A. et al.;2020;Przybylski, A.;Education, Digital Life and Wellbeing
Previous studies have offered mixed results regarding the link between digital screen engagement and the psychosocial functioning of young people. In this study, we aimed to determine the magnitude of this relation, to inform the discussion regarding whether amount of digital screen time has a subjectively significant impact on the psychosocial functioning of children and adolescents. We analyzed data from primary caregivers participating in the National Survey of Children’s Health (NSCH), an annual nationally representative survey fielded by the US Census Bureau between June 2016 and February 2017. NSCH uses an address-based sampling frame and both Web- and paper-based data collection instruments to measure psychosocial functioning and digital engagement, including a modified version of the Strengths and Difficulties questionnaire and caregiver estimates of daily television- and device-based engagement, respectively. The expected parabolic inverted-U-shaped relationship linking digital screen engagement to psychosocial functioning was found. These results replicated past findings suggesting that moderate levels of screen time (1-2 hours a day) were associated with slightly higher levels of psychosocial functioning compared to lower or higher levels of engagement. Furthermore, it indicated that children and adolescents would require 4 hours 40 minutes of television-based engagement and 5 hours 8 minutes of daily device-based engagement before caregivers would be able to notice subjectively significant variations in psychosocial functioning. The possible influence of digital screen engagement is likely smaller and more nuanced than we might expect. These findings do not rule out the possibility that parents might only notice very high levels of screen time when their child manifests pronounced psychosocial difficulties. Future work should be guided by transparent and confirmatory programs of research.;How Much Is Too Much? Examining the Relationship Between Digital Screen Engagement and Psychosocial Functioning in a Confirmatory Cohort Study;Przybylski, A., Orben, A. & Weinstein, N.;2019;Przybylski, A.;Education, Digital Life and Wellbeing
In their critique, Twenge et al. incorrectly concluded that the small associations between adolescent well-being and use of digital technology presented in Orben and Przybylski are the product of “analytical decisions that resulted in lower effect sizes”. Although some of the concerns raised were already addressed in the Supplementary Information of the original paper, we refute the other concerns by providing additional analyses based on the study data.;Reply to: Underestimating digital media harm;Orben, A. & Przybylski, A.;2020;Przybylski, A.;Education, Digital Life and Wellbeing
The coronavirus disease 2019 (COVID-19) pandemic is having a profound effect on all aspects of society, including mental health and physical health. We explore the psychological, social, and neuroscientific effects of COVID-19 and set out the immediate priorities and longer-term strategies for mental health science research. These priorities were informed by surveys of the public and an expert panel convened by the UK Academy of Medical Sciences and the mental health research charity, MQ: Transforming Mental Health, in the first weeks of the pandemic in the UK in March, 2020. We urge UK research funding agencies to work with researchers, people with lived experience, and others to establish a high level coordination group to ensure that these research priorities are addressed, and to allow new ones to be identified over time. The need to maintain high-quality research standards is imperative. International collaboration and a global perspective will be beneficial. An immediate priority is collecting high-quality data on the mental health effects of the COVID-19 pandemic across the whole population and vulnerable groups, and on brain function, cognition, and mental health of patients with COVID-19. There is an urgent need for research to address how mental health consequences for vulnerable groups can be mitigated under pandemic conditions, and on the impact of repeated media consumption and health messaging around COVID-19. Discovery, evaluation, and refinement of mechanistically driven interventions to address the psychological, social, and neuroscientific aspects of the pandemic are required. Rising to this challenge will require integration across disciplines and sectors, and should be done together with people with lived experience. New funding will be required to meet these priorities, and it can be efficiently leveraged by the UK's world-leading infrastructure. This Position Paper provides a strategy that may be both adapted for, and integrated with, research efforts in other countries.;Multidisciplinary research priorities for the COVID-19 pandemic: a call for action for mental health science;Holmes, E. & O'Connor, R. et al.;2020;Przybylski, A.;Education, Digital Life and Wellbeing
Throughout the developed world, adolescents are growing up with increased access to and engagement with a range of screen-based technologies, allowing them to encounter ideas and people on a global scale from the intimacy of their bedroom. The concerns about digital technologies negatively influencing sleep are therefore especially noteworthy, as sleep has been proven to greatly affect both cognitive and emotional well-being. The associations between digital engagement and adolescent sleep should therefore be carefully investigated in research adhering to the highest methodological standards. This understood, studies published to date have not often done so and have instead focused mainly on data derived from general retrospective self-report questionnaires. The value of this work has been called into question by recent research showing that retrospective questionnaires might fail to accurately measure these variables of interest. Novel and diverse approaches to measurement are therefore necessary for academic study to progress.;Teenage sleep and technology engagement across the week;Orben, A. & Przybylski, A.;2020;Przybylski, A.;Education, Digital Life and Wellbeing
The American Psychiatric Association (APA) and World Health Organization (WHO) have called for research investigating the clinical relevance of dysregulated video-game play. A growing number of exploratory studies have applied self-determination theory to probe the psychological dynamics of problematic gaming, but little is known about these dynamics in adolescents—the targets of most concerns—or the extent to which dysregulated gaming, in turn, affects functioning. In our study of British adolescents and their caregivers (n = 2,008), we adopted a confirmatory lens to test the extent to which basic psychological need satisfactions and frustrations underlie dysfunctional gaming behavior. The results, in line with preregistered sampling and data-analysis plans, indicated the frustrations, but not the absence of satisfactions, of psychological needs predicted adolescents’ dysregulated gaming and psychosocial functioning. Our discussion focuses on the clinical significance of gaming dysregulation and the advantages of transparent scientific practices for research informed by, and meant to inform, APA and WHO guidance.;Investigating the Motivational and Psychosocial Dynamics of Dysregulated Gaming: Evidence From a Preregistered Cohort Study;Przybylski, A. & Weinstein, N.;2019;Przybylski, A.;Education, Digital Life and Wellbeing
Virtual reality (VR) is a popular subject of scientific study across a variety of academic fields. In the present study we evaluate methodological trends in behavioral research on VR with respect to data collection practices, statistical reporting, and data availability. In line with this goal, we conducted a meta-scientific analysis of 61 articles encompassing a total of 1122 statistical tests and highlight three emergent trends that inform our understanding of past and future studies focused on VR. Conclusions from analysis of the data include a high incidence of errors in statistical reporting, and a general lack of transparency with respect to the availability of study data. Transparency in data analysis, increased statistical power, and more careful reporting of statistical outcomes are suggested to heighten methodological rigor and improve reproducibility in the field of VR research.;Virtual reality check: Statistical power, reported results, and the validity of research on the psychology of virtual reality and immersive environments;Lanier, M., Waddell, F., Elson, M. et al.;2019;Przybylski, A.;Education, Digital Life and Wellbeing
Secondary data analysis, or the analysis of preexisting data, provides a powerful tool for the resourceful psychological scientist. Never has this been more true than now, when technological advances enable both sharing data across labs and continents and mining large sources of preexisting data. However, secondary data analysis is easily overlooked as a key domain for developing new open-science practices or improving analytic methods for robust data analysis. In this article, we provide researchers with the knowledge necessary to incorporate secondary data analysis into their methodological toolbox. We explain that secondary data analysis can be used for either exploratory or confirmatory work, and can be either correlational or experimental, and we highlight the advantages and disadvantages of this type of research. We describe how transparency-enhancing practices can improve and alter interpretations of results from secondary data analysis and discuss approaches that can be used to improve the robustness of reported results. We close by suggesting ways in which scientific subfields and institutions could address and improve the use of secondary data analysis.;Recommendations for Increasing the Transparency of Analysis of Preexisting Data Sets;Weston, S., Ritchie, S. & Rohrer, J. et al.;2019;Przybylski, A.;Education, Digital Life and Wellbeing
The notion that digital-screen engagement decreases adolescent well-being has become a recurring feature in public, political, and scientific conversation. The current level of psychological evidence, however, is far removed from the certainty voiced by many commentators. There is little clear-cut evidence that screen time decreases adolescent well-being, and most psychological results are based on single-country, exploratory studies that rely on inaccurate but popular self-report measures of digital-screen engagement. In this study, which encompassed three nationally representative large-scale data sets from Ireland, the United States, and the United Kingdom (N = 17,247 after data exclusions) and included time-use-diary measures of digital-screen engagement, we used both exploratory and confirmatory study designs to introduce methodological and analytical improvements to a growing psychological research area. We found little evidence for substantial negative associations between digital-screen engagement—measured throughout the day or particularly before bedtime—and adolescent well-being.;Screens, Teens, and Psychological Well-Being: Evidence From Three Time-Use-Diary Studies;Orben, A. & Przybylski, A.;2019;Przybylski, A.;Education, Digital Life and Wellbeing
Research investigating the effect of new technologies on adolescents is more often characterized by media hype than sound science. We therefore welcome Foster and Jackson’s consideration of this research area’s measurement practices because we believe a critical mindset benefits academic, civic, and industry stakeholders. While the authors raised important questions, however, the conclusions drawn by their letter are left unsupported by the available scientific evidence. First, Foster and Jackson posit that our findings are confounded because adolescents reported only weekday social media use, observing the engagement was “surprisingly low”. The amount of usage reported in our data were, however, in line with the United Kingdom’s telecommunications regulator’s annual report. Measurements of weekday and weekend engagement with technologies are also highly correlated (e.g., r = 0.72), especially when viewed in light of correlations between estimates acquired using different measurement methods (values of r ≤ 0.18). Furthermore, evidence suggests that associations between weekday use and well-being are equivalent, or more negative, than those associations examining weekend use. There is therefore no evidence that weekday assessments substantially challenge our conclusions. Second, Foster and Jackson raise concerns about the self-report social media engagement measure’s framing. This is a formattable measurement challenge, and we noted our own dissatisfaction with relying on questionnaires in our paper as they “only partially reflect the objective time adolescents spend engaging with social media.” With that understood, it is incorrect to conclude that this measurement introduces confounds. Like all large-scale cohort studies, the measures used in the Understanding Society dataset are extensively tested, revised, and harmonized, using innovation panels, interviews, and surveys, to adapt them to ever-changing social environments. No self-report measurement is perfect compared with the ground truth, but there exists no evidence that this particular instrument is any more or less reliable than those used in other large-scale datasets. In conclusion, we report well-evidenced inferences about the enduring effects of social media engagement by pairing a robust analytic approach with one of the “best-quality datasets informing vital research in this area today”. We agree with Foster and Jackson that measurement practices deserve scrutiny but strongly believe that any given concern is only valid insofar it is supported by data. Because this was not provided, the reply is best understood as an opportunity to reflect on the value of scientists sharing data, materials, and code underlying their research inferences. This is not standard practice for those who routinely make extreme claims about technology effects but is necessary for “independent scientists, policymakers, and industry researchers [to] cooperate more closely”. Longitudinal, transparent, and reproducible work is the only way forward for scientists seeking to promote meaningful and actionable technology effects research.;Reply to Foster and Jackson: Open scientific practices are the way forward for social media effects research;Orben, A., Dienlin, T. & Przybylski, A.;2019;Przybylski, A.;Education, Digital Life and Wellbeing
In this study, we used large-scale representative panel data to disentangle the between-person and within-person relations linking adolescent social media use and well-being. We found that social media use is not, in and of itself, a strong predictor of life satisfaction across the adolescent population. Instead, social media effects are nuanced, small at best, reciprocal over time, gender specific, and contingent on analytic methods.;Social media’s enduring effect on adolescent life satisfaction;Orben, A., Dienlin, T. & Przybylski, A.;2019;Przybylski, A.;Education, Digital Life and Wellbeing
Mobile games, those played on smartphones and tablets, are rapidly becoming a dominant form of entertainment for young people. Given most of these games provide opportunities for competitive and cooperative play, they have the potential to build and enhance existing social relationships and might also provide an avenue for negative social experiences such as bullying or trolling behaviors. Unfortunately, little is understood about the prevalence or effects of cyberbullying for young people in this new form of electronic play. The present research, conducted in March 2018, surveyed a large and representative cohort of British adolescents and their caregivers (n = 2,008) with the aim of building an empirical understanding of this phenomenon. Adolescents were asked about their play habits and experiences, and caregivers provided data on their children's socioeconomic and demographic background and psychosocial functioning. Of key interest was estimating how frequently adolescents encounter bullying in mobile games, the extent to which bullying impacts them, and the sources of support adolescents seek following these events. Results indicated that bullying in mobile games is relatively common (33.5%), although less than 1 in 10 experiences serious repeated bullying (9.3%). Analysis showed males, players from a minority ethnicity, and those whose caregivers identified as having conduct problems, were the most likely to report significant victimization. Furthermore, nearly 4 in 10 (39.4%) reported feeling fairly or very upset by the experience and parents (49.3%), not gaming platforms (4.2%), were most likely to be sought out for support after bullying was experienced. Results are discussed with respect to this soon-to-be expanding research area and recommendations for the use of open scientific methodologies are provided.;Exploring Adolescent Cyber Victimization in Mobile Games: Preliminary Evidence from a British Cohort;Przybylski, A.;2019;Przybylski, A.;Education, Digital Life and Wellbeing
In this study, we investigated the extent to which adolescents who spend time playing violent video games exhibit higher levels of aggressive behaviour when compared with those who do not. A large sample of British adolescent participants (n = 1004) aged 14 and 15 years and an equal number of their carers were interviewed. Young people provided reports of their recent gaming experiences. Further, the violent contents of these games were coded using official EU and US ratings, and carers provided evaluations of their adolescents' aggressive behaviours in the past month. Following a preregistered analysis plan, multiple regression analyses tested the hypothesis that recent violent game play is linearly and positively related to carer assessments of aggressive behaviour. Results did not support this prediction, nor did they support the idea that the relationship between these factors follows a nonlinear parabolic function. There was no evidence for a critical tipping point relating violent game engagement to aggressive behaviour. Sensitivity and exploratory analyses indicated these null effects extended across multiple operationalizations of violent game engagement and when the focus was on another behavioural outcome, namely, prosocial behaviour. The discussion presents an interpretation of this pattern of effects in terms of both the ongoing scientific and policy debates around violent video games, and emerging standards for robust evidence-based policy concerning young people's technology use.;Violent video game engagement is not associated with adolescents' aggressive behaviour: evidence from a registered report;Przybylski, A. & Weinstein, N.;2019;Przybylski, A.;Education, Digital Life and Wellbeing
To determine the extent to which time spent with digital devices predicts meaningful variability in pediatric sleep. Following a preregistered analysis plan, data from a sample of American children (n = 50 212) derived from the 2016 National Survey of Children's Health were analyzed. Models adjusted for child-, caregiver-, household-, and community-level covariates to estimate the potential effects of digital screen use. Each hour devoted to digital screens was associated with 3-8 fewer minutes of nightly sleep and significantly lower levels of sleep consistency. Furthermore, those children who complied with 2010 and 2016 American Academy of Pediatrics guidance on screen time limits reported between 20 and 26 more minutes, respectively, of nightly sleep. However, links between digital screen time and pediatric sleep outcomes were modest, accounting for less than 1.9% of observed variability in sleep outcomes. Digital screen time, on its own, has little practical effect on pediatric sleep. Contextual factors surrounding screen time exert a more pronounced influence on pediatric sleep compared to screen time itself. These findings provide an empirically robust template for those investigating the digital displacement hypothesis as well as informing policy-making.;Digital Screen Time and Pediatric Sleep: Evidence from a Preregistered Cohort Study;Przybylski, A.;2018;Przybylski, A.;Education, Digital Life and Wellbeing
The widespread use of digital technologies by young people has spurred speculation that their regular use negatively impacts psychological well-being. Current empirical evidence supporting this idea is largely based on secondary analyses of large-scale social datasets. Though these datasets provide a valuable resource for highly powered investigations, their many variables and observations are often explored with an analytical flexibility that marks small effects as statistically significant, thereby leading to potential false positives and conflicting results. Here we address these methodological challenges by applying specification curve analysis (SCA) across three large-scale social datasets (total n = 355,358) to rigorously examine correlational evidence for the effects of digital technology on adolescents. The association we find between digital technology use and adolescent well-being is negative but small, explaining at most 0.4% of the variation in well-being. Taking the broader context of the data into account suggests that these effects are too small to warrant policy change.;The association between adolescent well-being and digital technology use;Orben, A. & Przybylski, A.;2019;Przybylski, A.;Education, Digital Life and Wellbeing
Caregivers employ a range of motivational strategies to help regulate and protect adolescents using connective technologies. The present study explored a new conceptual model informed by self-determination theory (Deci and Ryan, 2000, Ryan and Deci, 2000) with a representative sample of 1000 adolescents recruited nationwide within Britain, and using a confirmatory, pre-registered and open science methodology. In this experimental study we compared controlling (pressuring, coercive, or punitive) styles of restricting technology with neutral, and autonomy-supportive (empathic, choice-promoting) styles of restricting to predict adolescents' concealing their technology use from caregivers. We further tested two mechanisms which might explain the links of condition and concealment: perceiving caregivers to be trusting, and experiencing reactance or the desire to do the opposite of what was instructed. Findings are discussed in terms of the role of regulation styles on interpersonal outcomes and adolescent development, and implications for technology use policy and recommendations to caregivers and teachers.;The impacts of motivational framing of technology restrictions on adolescent concealment: Evidence from a preregistered experimental study;Weinstein, N. & Przybylski, A.;2019;Przybylski, A.;Education, Digital Life and Wellbeing
There is little empirical understanding of how young children's screen engagement links to their well‐being. Data from 19,957 telephone interviews with parents of 2‐ to 5‐year‐olds assessed their children's digital screen use and psychological well‐being in terms of caregiver attachment, resilience, curiosity, and positive affect in the past month. Evidence did not support implementing limits (< 1 or < 2 hr/day) as recommended by the American Academy of Pediatrics, once variability in child ethnicity, age, gender, household income, and caregiver educational attainment were considered. Yet, small parabolic functions linked screen time to attachment and positive affect. Results suggest a critical cost–benefit analysis is needed to determine whether setting firm limits constitutes a judicious use of caregiver and professional resources.;Digital Screen Time Limits and Young Children's Psychological Well‐Being: Evidence From a Population‐Based Study;Przybylski, A. & Weinstein, N.;2017;Przybylski, A.;Education, Digital Life and Wellbeing
"Little is known about how parents may protect against cyberbullying, a growing problem-behavior among youth. Guided by self-determination theory, a theory concerned with effectively motivating and regulating behavior, six preregistered hypotheses concerning parenting strategies of regulating cyberbullying behavior were tested in 1004 parent–child dyads (45.9% female adolescents; adolescents were either 14 (49.5%) or 15 (50.5%) years old). The results largely supported hypotheses: Parents who used more autonomy-supportive strategies—understanding the adolescent’s perspective, offering choice, and giving rationales for prohibitions—had adolescents who reported engaging in less cyberbullying than parents who used controlling strategies (especially using guilt, shame, and conditional regard). Further, this was mediated by lower feelings of reactance to, or a desire to do the opposite of, parents’ requests. The discussion focuses on the limits of this study to investigate reciprocal effects of adolescent behavior shaping parenting strategies—a critical agenda for future research—as well as the potential benefits of interventions aimed at increasing parental autonomy support for reducing cyberbullying and other problem behaviors in adolescents.";Parenting Strategies and Adolescents’ Cyberbullying Behaviors: Evidence from a Preregistered Study of Parent–Child Dyads;Legate, N., Weinstein, N. & Przybylski, A.;2019;Przybylski, A.;Education, Digital Life and Wellbeing
We greatly appreciate the care and thought that is evident in the 10 commentaries that discuss our debate paper, the majority of which argued in favor of a formalized ICD-11 gaming disorder. We agree that there are some people whose play of video games is related to life problems. We believe that understanding this population and the nature and severity of the problems they experience should be a focus area for future research. However, moving from research construct to formal disorder requires a much stronger evidence base than we currently have. The burden of evidence and the clinical utility should be extremely high, because there is a genuine risk of abuse of diagnoses. We provide suggestions about the level of evidence that might be required: transparent and preregistered studies, a better demarcation of the subject area that includes a rationale for focusing on gaming particularly versus a more general behavioral addictions concept, the exploration of non-addiction approaches, and the unbiased exploration of clinical approaches that treat potentially underlying issues, such as depressive mood or social anxiety first. We acknowledge there could be benefits to formalizing gaming disorder, many of which were highlighted by colleagues in their commentaries, but we think they do not yet outweigh the wider societal and public health risks involved. Given the gravity of diagnostic classification and its wider societal impact, we urge our colleagues at the WHO to err on the side of caution for now and postpone the formalization.;A weak scientific basis for gaming disorder: Let us err on the side of caution;van Rooij, A., Ferguson, C., Colder Carras, M. et al.;2018;Przybylski, A.;Education, Digital Life and Wellbeing
"Bullying is a major public health problem. We aimed to estimate the prevalence of cyberbullying and traditional bullying among adolescents in England, and assess its relative effects on mental well-being. In this population-based study, we analysed data from a nationally representative cross-sectional study, What About Youth, which enrolled a random sample of 298 080 school pupils drawn from 564 886 National Pupil Database records of adolescents aged 15 years, living in England, with matching postcode and local authority data, to complete self-report surveys between Sept 22, 2014, and Jan 9, 2015. Mental well-being, defined as life satisfaction, fulfilling social relationships, purpose in life, and a subjective sense of flourishing, was assessed using the Warwick-Edinburgh Mental Well-being Scale and was compared between those adolescents who reported traditional bullying (including physical, verbal, and relational bullying) or cyberbullying 2–3 times a month or more compared with those adolescents who reported traditional bullying and cyberbullying once or twice in the past couple of months or less. Traditional bullying was defined as repeated, intentional aggression that is targeted at a person who cannot easily defend himself or herself; cyberbullying was additionally defined as taking place in an electronic context (eg, e-mail, blogs, instant messages, text messages). Traditional bullying is considerably more common among adolescents in England than cyberbullying. While both forms of bullying were associated with poorer mental well-being, cyberbullying accounted for a very small share of variance after adjustment for offline bullying and other covariates.";Cyberbullying and adolescent well-being in England: a population-based cross-sectional study;Przybylski, A. & Bowes, L.;2017;Przybylski, A.;Education, Digital Life and Wellbeing
Concerns about problematic gaming behaviors deserve our full attention. However, we claim that it is far from clear that these problems can or should be attributed to a new disorder. The empirical basis for a Gaming Disorder proposal, such as in the new ICD-11, suffers from fundamental issues. Our main concerns are the low quality of the research base, the fact that the current operationalization leans too heavily on substance use and gambling criteria, and the lack of consensus on symptomatology and assessment of problematic gaming. The act of formalizing this disorder, even as a proposal, has negative medical, scientific, public-health, societal, and human rights fallout that should be considered. Of particular concern are moral panics around the harm of video gaming. They might result in premature application of diagnosis in the medical community and the treatment of abundant false-positive cases, especially for children and adolescents. Second, research will be locked into a confirmatory approach, rather than an exploration of the boundaries of normal versus pathological. Third, the healthy majority of gamers will be affected negatively. We expect that the premature inclusion of Gaming Disorder as a diagnosis in ICD-11 will cause significant stigma to the millions of children who play video games as a part of a normal, healthy life. At this point, suggesting formal diagnoses and categories is premature: the ICD-11 proposal for Gaming Disorder should be removed to avoid a waste of public health resources as well as to avoid causing harm to healthy video gamers around the world.;Scholars’ open debate paper on the World Health Organization ICD-11 Gaming Disorder proposal;Aarseth, E., Bean, A. & Boonen, H. et al.;2017;Przybylski, A.;Education, Digital Life and Wellbeing
We appreciate the comments by Yao and colleagues regarding our recent study on Internet gaming disorder, an important yet little-understood phenomenon. In these comments, the authors highlight three aspects of Internet gaming disorder research that we believe merit careful consideration as this area of research develops. First, the letter highlighted that sampling strategies are important, and we agree. Current estimates of Internet gaming disorder prevalence vary widely as a function of the methodology employed and range from 0.2% of the general population to nearly half of self-selecting gamers. Such tremendous variation can be addressed with approaches following best practices in health research, as was done in the present work. The issue of sampling methodology should be considered carefully because nonrepresentative samples likely yield biased estimates and lead readers to draw faulty conclusions about the effects of gaming. Survey design, in addition to sampling, is essential. For example, Yao et al. cite an estimate based on a study sample using a Likert-based assessment that deviates both from the proposed DSM-5 criteria and from alternative theoretically rigorous approaches. As a result, the 12-month period prevalence estimates range from a modest 1.2% (95% CI=1.0–1.4), in line with our findings, to an implausibly high 6.3% (95% CI=5.8–6.7). Such discrepancies within single studies underline the need to minimize researcher degrees of freedom by distinguishing between exploratory and confirmatory research reporting.;Open Scientific Practices Are the Way Forward for Internet Gaming Disorder Research: Response to Yao et al.;Przybylski, A., Weinstein, N. & Murayama, K.;2017;Przybylski, A.;Education, Digital Life and Wellbeing
"Technology and Human Behavior – a title that could not be more generic for a Special Issue. On its face, the phenomena examined in this issue are not distinct from others published in the Journal of Media Psychology(JMP): Can immersive technology be used to promote pro-environmental behaviors? (Soliman, Peetz, & Davydenko, 2017); How do subtitles and complexity of MOOC-type videos impact learning outcomes? (van der Zee, Admiraal, Paas, Saab, & Gisbers, 2017); Does cooperative video game play foster prosocial behavior? (Breuer, Velez, Bowman, Wulf, & Bente, 2017); What is the role of video game use in the unique risk environment of college students? (Holz Ivory, Ivory, & Lanier, 2017); Do interactive narratives have the potential to advocate social change? (Steineman, Iten, Opwis, Forse, Frasseck, & Mekler, 2017). What makes this issue special is not a thematic focus, but the nature of the scientific approach to hypotheses testing: It is explicitly confirmatory. All five studies are registered reports which are reviewed in two phases: First, the theoretical background, hypotheses, methods, and analysis plans of a study are peer-reviewed before the data are collected. If they are evaluated as sound, the study receives an “in-principle” acceptance, and researchers proceed to conduct it (taking potential changes or additions suggested by the reviewers into consideration). Consequently, the data collected can be used as a true (dis-)confirmatory hypothesis test. In a second step, the soundness of the analyses and discussion section are reviewed, but the publication decision is not contingent on the outcome of the study (see our call for papers; Elson, Przybylski, & Krämer, 2015). All additional, nonpreregistered analyses conducted are clearly labelled as exploratory and serve to discover alternative explanations or generate new hypotheses. Further, the authors were required to provide a sampling plan designed to achieve at least 80% statistical power (or comparable criterion for Bayesian analysis strategies) for all of their confirmatory hypothesis tests, and to make all materials, data, and analysis scripts freely available on the Open Science Framework (OSF) at https://osf.io/5cvkr/. We believe that making these materials available to anyone increases the value of the research as it allows others to reproduce analyses, replicate the studies, or build on and extend the empirical foundation. As such, the five studies represent the first in JMP that employ these new practices. It is our hope that these contributions will serve as an inspiration and model for other media researchers, and encourage scientists studying media to preregister designs and share their data and materials openly. All research proposals were reviewed by content experts from within the field and additional outside experts in methodology and statistics. Their reviews, too, are available on the OSF, and we deeply appreciate their contributions to meliorate each individual research report and their commitment to open and reproducible science: Marko Bachl, Chris Chambers, Julia Erdmann, Pete Etchells, Alexander Etz, Karin Fikkers, Jesse Fox, Chris Hartgerink, Moritz Heene, Joe Hilgard, Markus Huff, Rey Junco, Daniël Lakens, Benny Liebold, Patrick Markey, Jörg Matthes, Candice Morey, Richard Morey, Michèle Nuijten, Elizabeth Page-Gould, Daniel Pietschmann, Michael Scharkow, Felix Schönbrodt, Cary Stothart, Morgan Tear, Netta Weinstein, and additional reviewers who would like to remain anonymous. Finally, we would like to extend our sincerest gratitude to JMP’s Editor-in-Chief Nicole Krämer and editorial assistant German Neubaum for their support and guidance from the conception to the publication of this issue.";The Science of Technology and Human Behavior: Standards, Old and New;Elson, M. & Przybylski, A.;2017;Przybylski, A.;Education, Digital Life and Wellbeing
The American Psychiatric Association (APA) identified Internet gaming disorder as a new potential psychiatric disorder and has recognized that little is known about the prevalence, validity, or cross-cultural robustness of proposed Internet gaming disorder criteria. In response to this gap in our understanding, the present study, a first for this research topic, estimated the period prevalence of this new potential psychiatric disorder using APA guidance, examined the validity of its proposed indicators, evaluated reliability cross-culturally and across genders, compared it to gold-standard research on gambling addiction and problem gaming, and estimated its impact on physical, social, and mental health.;Internet Gaming Disorder: Investigating the Clinical Relevance of a New Phenomenon;Przybylski, A., Weinstein, N. & Murayama, K.;2016;Przybylski, A.;Education, Digital Life and Wellbeing
Although the time adolescents spend with digital technologies has sparked widespread concerns that their use might be negatively associated with mental well-being, these potential deleterious influences have not been rigorously studied. Using a preregistered plan for analyzing data collected from a representative sample of English adolescents (n = 120,115), we obtained evidence that the links between digital-screen time and mental well-being are described by quadratic functions. Further, our results showed that these links vary as a function of when digital technologies are used (i.e., weekday vs. weekend), suggesting that a full understanding of the impact of these recreational activities will require examining their functionality among other daily pursuits. Overall, the evidence indicated that moderate use of digital technology is not intrinsically harmful and may be advantageous in a connected world. The findings inform recommendations for limiting adolescents’ technology use and provide a template for conducting rigorous investigations into the relations between digital technology and children’s and adolescents’ health.;A Large-Scale Test of the Goldilocks Hypothesis: Quantifying the Relations Between Digital-Screen Use and the Mental Well-Being of Adolescents;Przybylski, A. & Weinstein, N.;2017;Przybylski, A.;Education, Digital Life and Wellbeing
The American Psychiatric Association has identified Internet Gaming Disorder (IGD) as a potential psychiatric condition and called for research to investigate its etiology, stability, and impacts on health and behavior. The present study recruited 5,777 American adults and applied self-determination theory to examine how motivational factors influence, and are influenced by, IGD and health across a six month period. Following a preregistered analysis plan, results confirmed our hypotheses that IGD criteria are moderately stable and that they and basic psychological need satisfaction have a reciprocal relationship over time. Results also showed need satisfaction promoted health and served as a protective factor against IGD. Contrary to what was hypothesized, results provided no evidence directly linking IGD to health over time. Exploratory analyses suggested that IGD may have indirect effects on health by way of its impact on basic needs. Implications are discussed in terms of existing gaming addiction and motivational frameworks.;A prospective study of the motivational and health dynamics of Internet Gaming Disorder;Weinstein, N., Przybylski, A. & Murayama, K.;2017;Przybylski, A.;Education, Digital Life and Wellbeing
A growing research literature suggests that regular electronic game play and game-based training programs may confer practically significant benefits to cognitive functioning. Most evidence supporting this idea, the gaming-enhancement hypothesis, has been collected in small-scale studies of university students and older adults. This research investigated the hypothesis in a general way with a large sample of 1,847 school-aged children. Our aim was to examine the relations between young people’s gaming experiences and an objective test of reasoning performance. Using a Bayesian hypothesis testing approach, evidence for the gaming-enhancement and null hypotheses were compared. Results provided no substantive evidence supporting the idea that having preference for or regularly playing commercially available games was positively associated with reasoning ability. Evidence ranged from equivocal to very strong in support for the null hypothesis over what was predicted. The discussion focuses on the value of Bayesian hypothesis testing for investigating electronic gaming effects, the importance of open science practices, and pre-registered designs to improve the quality of future work.;A large scale test of the gaming-enhancement hypothesis;Przybylski, A. & Wang, J.;2016;Przybylski, A.;Education, Digital Life and Wellbeing
Electronic gaming contexts are now a dominant entertainment medium for young people in the developed and developing world (Lenhart et al., 2008), yet little is known about how distinct doses of gaming exposure may influence adolescents. This research focused on the effects of quantity of play, the amount of time devoted to gaming on a typical day, and quality of play, the kinds of games regularly played, as predictors on teachers’ evaluations of young peoples’ academic engagement and psychosocial functioning. Results derived from a school-based sample of 217 young people indicated that, compared with those who did not play, adolescents who engaged in low levels of gaming,<1 hr a day, evidenced lower levels of hyperactivity and conduct issues whereas the opposite was found for those who gamed for >3 hr a day. Further, the teachers of young people who tended toward playing mainly single-player games reported that these students showed lower levels of hyperactivity and conduct problems, fewer peer and emotional difficulties, as well as higher levels of active academic engagement. Teachers of young people who played cooperative and competitive online games rated these students as more emotionally stable and had better relationships with classmates, a pattern of results that remained in evidence controlling for variance linked to participant sex. Results are discussed in light of a developing and increasingly nuanced literature focused on determining the ways and the extent to which electronic gaming may influence young people. (PsycInfo Database Record (c) 2020 APA, all rights reserved);How the quantity and quality of electronic gaming relates to adolescents’ academic engagement and psychosocial adjustment.;Przybylski, A. & Mishkin, A.;2016;Przybylski, A.;Education, Digital Life and Wellbeing
The most recent update to the American Psychiatric Association’s (APA) Diagnostic and Statistical Manual of Mental Disorders (DSM-5) included Internet Gaming Disorder as a new potential psychiatric condition that merited further scientific study. The present research was conducted in response to the APA Substance-Related Disorders Working Group’s research call to estimate the extent to which mischievous responding—a known problematic pattern of participant self-report responding in questionnaires—is relevant to Internet Gaming Disorder research. In line with a registered sampling and analysis plan, findings from two studies (ntot= 11,908) provide clear evidence that mischievous responding is positively associated with the number of Internet Gaming Disorder indicators participants report. Results are discussed in the context of ongoing problem gaming research and the discussion provides recommendations for improving the quality of scientific practice in this area.;Mischievous responding in Internet Gaming Disorder research;Przybylski, A.;2016;Przybylski, A.;Education, Digital Life and Wellbeing
Theories regarding the influences of electronic games drive scientific study, popular debate, and public policy. The fractious interchanges among parents, pundits, and scholars hint at the rich phenomenological and psychological dynamics that underlie how people view digital technologies such as games. The current research applied Martin Heidegger’s concept of interpretive frameworks (Heidegger, 1987) and Robert Zajonc’s exposure-attitude hypothesis (Zajonc, 1968) to explore how attitudes towards technologies such as electronic games arise. Three studies drew on representative cohorts of American and British adults and evaluated how direct and indirect experiences with games shape how they are seen. Results indicated this approach was fruitful: negative attitudes and beliefs linking games to real-world violence were prominent among those with little direct exposure to electronic gaming contexts, whereas those who played games and reported doing so with their children tended to evaluate gaming more positively. Further findings indicated direct experience tended to inform the accuracy of beliefs about the effects of digital technology, as those who had played were more likely to believe that which is empirically known about game effects. Results are discussed with respect to ongoing debates regarding gaming and broader applications of this approach to understand the psychological dynamics of adapting to technological advances.;How we see electronic games;Przybylski, A. & Weinstein, N.;2016;Przybylski, A.;Education, Digital Life and Wellbeing
Through appearances, interviews, and a recent book Susan Greenfield, a senior research fellow at Lincoln College, Oxford, has promoted the idea that internet use and computer games can have harmful effects on the brain, emotions, and behaviour, and she draws a parallel between the effects of digital technology and climate change. Despite repeated calls for her to publish these claims in the peer reviewed scientific literature, where clinical researchers can check how well they are supported by evidence, this has not happened, and the claims have largely been aired in the media. As scientists working in mental health, developmental neuropsychology, and the psychological impact of digital technology, we are concerned that Greenfield’s claims are not based on a fair scientific appraisal of the evidence, often confuse correlation for causation, give undue weight to anecdote and poor quality studies, and are misleading to parents and the public at large.;The debate over digital technology and young people;Bell, V., Bishop, D. & Przybylski, A.;2015;Przybylski, A.;Education, Digital Life and Wellbeing
The rise of electronic games has driven both concerns and hopes regarding their potential to influence young people. Existing research identifies a series of isolated positive and negative effects, yet no research to date has examined the balance of these potential effects in a representative sample of children and adolescents. The objective of this study was to explore how time spent playing electronic games accounts for significant variation in positive and negative psychosocial adjustment using a representative cohort of children aged 10 to 15 years. A large sample of children and adolescents aged 10 to 15 years completed assessments of psychosocial adjustment and reported typical daily hours spent playing electronic games. Relations between different levels of engagement and indicators of positive and negative psychosocial adjustment were examined, controlling for participant age and gender and weighted for population representativeness. Low levels (<1 hour daily) as well as high levels (>3 hours daily) of game engagement was linked to key indicators of psychosocial adjustment. Low engagement was associated with higher life satisfaction and prosocial behavior and lower externalizing and internalizing problems, whereas the opposite was found for high levels of play. No effects were observed for moderate play levels when compared with non-players. The links between different levels of electronic game engagement and psychosocial adjustment were small (<1.6% of variance) yet statistically significant. Games consistently but not robustly associated with children’s adjustment in both positive and negative ways, findings that inform policy-making as well as future avenues for research in the area.;Electronic Gaming and Psychosocial Adjustment;Przybylski, A.;2014;Przybylski, A.;Education, Digital Life and Wellbeing
Electronic games have rapidly become a popular form of human recreation, and the immersive experiences they provide millions have led many to voice concerns that some games, and violent ones in particular, may negatively impact society. Increasingly heated debates make it clear that gaming-related aggression is a topic that elicits strong opinions. Despite a complex and growing literature concerned with violent games, little is known empirically about why some ardently believe, whereas others dismiss, notions that this form of leisure is a source of aggression. The present research recruited three nationally representative samples to investigate this understudied topic. Results showed that belief was normally distributed across the population, prominent among demographic cohorts who did not grow up with games and those who lack concrete gaming experience. Results are discussed in the context of this developing research area, wider social science perspectives, and the place of electronic games in society.;Who Believes Electronic Games Cause Real World Aggression?;;2014;Przybylski, A.;Education, Digital Life and Wellbeing
Recent studies have examined whether electronic games foster aggression. At present, the extent to which games contribute to aggression and the mechanisms through which such links may exist are hotly debated points. In current research we tested a motivational hypothesis derived from self-determination theory—that gaming would be associated with indicators of human aggression to the degree that the interactive elements of games serve to impede players’ fundamental psychological need for competence. Seven studies, using multiple methods to manipulate player competence and a range of approaches for evaluating aggression, indicated that competence-impeding play led to higher levels of aggressive feelings, easier access to aggressive thoughts, and a greater likelihood of enacting aggressive behavior. Results indicated that player perceived competence was positively related to gaming motivation, a factor that was, in turn, negatively associated with player aggression. Overall, this pattern of effects was found to be independent of the presence or absence of violent game contents. We discuss the results in respect to research focused on psychological need frustration and satisfaction and as they regard gaming-related aggression literature.;Competence-impeding electronic games and players’ aggressive feelings, thoughts, and behaviors.;Przybylski, A., Deci, E. & Rigby, C. et al.;2014;Przybylski, A.;Education, Digital Life and Wellbeing
"Social media utilities have made it easier than ever to know about the range of online or offline social activities one could be engaging. On the upside, these social resources provide a multitude of opportunities for interaction; on the downside, they often broadcast more options than can be pursued, given practical restrictions and limited time. This dual nature of social media has driven popular interest in the concept of Fear of Missing Out – popularly referred to as FoMO. Defined as a pervasive apprehension that others might be having rewarding experiences from which one is absent, FoMO is characterized by the desire to stay continually connected with what others are doing. The present research presents three studies conducted to advance an empirically based understanding of the fear of missing out phenomenon. The first study collected a diverse international sample of participants in order to create a robust individual differences measure of FoMO, the Fear of Missing Out scale (FoMOs); this study is the first to operationalize the construct. Study 2 recruited a nationally representative cohort to investigate how demographic, motivational and well-being factors relate to FoMO. Study 3 examined the behavioral and emotional correlates of fear of missing out in a sample of young adults. Implications of the FoMOs measure and for the future study of FoMO are discussed.";Motivational, emotional, and behavioral correlates of fear of missing out;Przybylski, A., Murayama, K. & DeHaan, C. et al.;2013;Przybylski, A.;Education, Digital Life and Wellbeing
Recent advancements in communication technology have enabled billions of people to connect over great distances using mobile phones, yet little is known about how the frequent presence of these devices in social settings influences face-to-face interactions. In two experiments, we evaluated the extent to which the mere presence of mobile communication devices shape relationship quality in dyadic settings. In both, we found evidence they can have negative effects on closeness, connection, and conversation quality. These results demonstrate that the presence of mobile phones can interfere with human relationships, an effect that is most clear when individuals are discussing personally meaningful topics.;Can you connect with me now? How the presence of mobile communication technology influences face-to-face conversation quality;Przybylski, A. & Weinstein, N.;2012;Przybylski, A.;Education, Digital Life and Wellbeing
Recent research has provided new insights into the integrative process, which allows for unified self-functioning. In this article, we review recent work that has used a variety of behavioral, physiological, dual-process, and survey techniques to examine personality integration. On the basis of theoretical considerations and the growing body of findings, we highlight three subprocesses—namely, awareness, ownership/autonomy, and nondefensiveness—and summarize evidence linking these facets of integration to energy, wellness, and relational benefits. Finally, we review contextual factors, such as autonomy support and unconditional regard, that have been shown to be conducive to integration, and we suggest tools that may be used in future research on integration.;The Integrative Process: New Research and Future Directions;Weinstein, N., Przybylski, A. & Ryan, R.;2013;Przybylski, A.;Education, Digital Life and Wellbeing
When individuals grow up with autonomy-thwarting parents, they may be prevented from exploring internally endorsed values and identities and as a result shut out aspects of the self perceived to be unacceptable. Given the stigmatization of homosexuality, individuals perceiving low autonomy support from parents may be especially motivated to conceal same-sex sexual attraction, leading to defensive processes such as reaction formation. Four studies tested a model wherein perceived parental autonomy support is associated with lower discrepancies between self-reported sexual orientation and implicit sexual orientation (assessed with a reaction time task). These indices interacted to predict anti-gay responding indicative of reaction formation. Studies 2–4 showed that an implicit/explicit discrepancy was particularly pronounced in participants who experienced their fathers as both low in autonomy support and homophobic, though results were inconsistent for mothers. Findings of Study 3 suggested contingent self-esteem as a link between parenting styles and discrepancies in sexual orientation measures.;Parental autonomy support and discrepancies between implicit and explicit sexual identities: Dynamics of self-acceptance and defense;Weinstein, N., Ryan, W. & DeHaan, C. et al.;2012;Przybylski, A.;Education, Digital Life and Wellbeing
Video games constitute a popular form of entertainment that allows millions of people to adopt virtual identities. In our research, we explored the idea that the appeal of games is due in part to their ability to provide players with novel experiences that let them “try on” ideal aspects of their selves that might not find expression in everyday life. We found that video games were most intrinsically motivating and had the greatest influence on emotions when players’ experiences of themselves during play were congruent with players’ conceptions of their ideal selves. Additionally, we found that high levels of immersion in gaming environments, as well as large discrepancies between players’ actual-self and ideal-self characteristics, magnified the link between intrinsic motivation and the experience of ideal-self characteristics during play.;The Ideal Self at Play: The Appeal of Video Games That Let You Be All You Can Be;Przybylski, A., Weinstein, N. & Murayama, K. et al.;2011;Przybylski, A.;Education, Digital Life and Wellbeing
"More Americans now play video games than go to the movies (NPD Group, 2009). The meteoric rise in popularity of video games highlights the need for research approaches that can deepen our scientific understanding of video game engagement. This article advances a theory-based motivational model for examining and evaluating the ways by which video game engagement shapes psychological processes and influences well-being. Rooted in self-determination theory (Deci & Ryan, 2000; Ryan & Deci, 2000a), our approach suggests that both the appeal and well-being effects of video games are based in their potential to satisfy basic psychological needs for competence, autonomy, and relatedness. We review recent empirical evidence applying this perspective to a number of topics including need satisfaction in games and short-term well-being, the motivational appeal of violent game content, motivational sources of postplay aggression, the antecedents and consequences of disordered patterns of game engagement, and the determinants and effects of immersion. Implications of this model for the future study of game motivation and the use of video games in interventions are discussed.";A Motivational Model of Video Game Engagement;Przybylski, A., Rigby, C. & Ryan, R.;2010;Przybylski, A.;Education, Digital Life and Wellbeing
The present research examined the background and consequences of different styles of engagement in video game play. Based on self-determination theory and the dualistic model of passion, the authors hypothesized that high levels of basic psychological need satisfaction would foster harmonious passion for video play, supporting the subjective sense that play is something one wants to do. It was also predicted that low levels of need satisfaction would promote obsessive passion for games and contribute to the feeling that game play is something one feels compelled to or has to do. It was expected, in turn, that passion for play would directly influence player outcomes closely tied to games, moderate links between play and well-being, and relate to overall levels of well-being as a function of basic need satisfaction. As expected, results showed that low levels of basic need satisfaction were associated with more obsessive passion, higher amounts of play, greater tension following play, and low game enjoyment, whereas high levels of need satisfaction did not predict hours of play but were associated with more harmonious passion, game enjoyment, and energy following play. Moderation analyses showed that high amounts of play related negatively to well-being only to the extent that players reported an obsessive passion and that the unique relations between passion and overall levels of player well-being were quite small once controlling for their basic need satisfaction in daily life. Discussion of the current findings focuses on their significance for understanding disordered play and the value of applying a theory-based approach to study motivation for virtual contexts.;Having to versus Wanting to Play: Background and Consequences of Harmonious versus Obsessive Engagement in Video Games;Przybylski, A., Weinstein, N. & Ryan, R. et al.;2009;Przybylski, A.;Education, Digital Life and Wellbeing
"Participation in expansive video games called 'virtual worlds' has become a mainstream leisure activity for tens of millions of people around the world. The growth of this industry and the strong motivational appeal of these digital worlds invite a closer examination as to how educators can learn from today's virtual worlds in the development of next generation learning environments. Self-determination theory (SDT; Ryan and Deci, 2000) has shown value in explaining both the motivational dynamics of learning (Deci et al., 1994), as well as the strong motivational pull of video games and virtual worlds (Ryan et al., 2006). As such, SDT provides a framework that can bridge the gap between education and consumer virtual worlds and be applied to new research and development in how to best build virtual worlds for learning. The concept of the `learner hero' is introduced as a potentially useful unifying concept in considering how to leverage the high motivational appeal of commercial virtual worlds in building digital learning environments.";Virtual worlds and the learner hero: How today's video games can inform tomorrow's digital learning environments;Rigby, C. & Przybylski, A.;2009;Przybylski, A.;Education, Digital Life and Wellbeing
Six studies, two survey based and four experimental, explored the relations between violent content and people's motivation and enjoyment of video game play. Based on self-determination theory, the authors hypothesized that violence adds little to enjoyment or motivation for typical players once autonomy and competence need satisfactions are considered. As predicted, results from all studies showed that enjoyment, value, and desire for future play were robustly associated with the experience of autonomy and competence in gameplay. Violent content added little unique variance in accounting for these outcomes and was also largely unrelated to need satisfactions. The studies also showed that players high in trait aggression were more likely to prefer or value games with violent contents, even though violent contents did not reliably enhance their game enjoyment or immersion. Discussion focuses on the significance of the current findings for individuals and the understanding of motivation in virtual environments.;The Motivating Role of Violence in Video Games;Przybylski, A., Ryan, R. & Rigby, C.;2009;Przybylski, A.;Education, Digital Life and Wellbeing
"Four studies apply self-determination theory (SDT; Ryan & Deci, 2000) in investigating motivation for computer game play, and the effects of game play on well-being. Studies 1–3 examine individuals playing 1, 2 and 4 games, respectively and show that perceived in-game autonomy and competence are associated with game enjoyment, preferences, and changes in well-being pre- to post-play. Competence and autonomy perceptions are also related to the intuitive nature of game controls, and the sense of presence or immersion in participants’ game play experiences. Study 4 surveys an on-line community with experience in multi-player games. Results show that SDT’s theorized needs for autonomy, competence, and relatedness independently predict enjoyment and future game play. The SDT model is also compared with Yee’s (2005) motivation taxonomy of game play motivations. Results are discussed in terms of the relatively unexplored landscape of human motivation within virtual worlds.";The Motivational Pull of Video Games: A Self-Determination Theory Approach;Ryan, R., Rigby, C. & Przybylski, A.;2006;Przybylski, A.;Education, Digital Life and Wellbeing
We often hear of measures that promote traditional security concepts such as ‘defence in depth’ or ‘compartmentalisation’. One aspect that has been largely ignored in computer security is that of ‘deterrence’. This may be due to difficulties in applying common notions of strategic deterrence, such as attribution — resulting in previous work focusing on the role that deterrence plays in large-scale cyberwar or other esoteric possibilities. In this paper, we focus on the operational and tactical roles of deterrence in providing everyday security for individuals. As such, the challenge changes: from one of attribution to one of understanding the role of attacker beliefs and the constraints on attackers and defenders. To this end, we demonstrate the role deterrence can play as part of the security of individuals against the low-focus, low-skill attacks that pervade the Internet. Using commonly encountered problems of spam email and the security of wireless networks as examples, we demonstrate how different notions of deterrence can complement well-developed models of defence, as well as provide insights into how individuals can overcome conflicting security advice. We use dynamic games of incomplete information, in the form of screening and signalling games, as models of users employing deterrence. We find multiple equilibria that demonstrate aspects of deterrence within specific bounds of utility, and show that there are scenarios where the employment of deterrence changes the game such that the attacker is led to conclude that the best move is not to play.;When the Winning Move is Not to Play: Games of Deterrence in Cyber Security;Heitzenrater, C., Taylor, G. & Simpson, A.;2015;Taylor, G.;Digital Economies
We study situations in which consumers rely on a biased intermediary's advice when choosing among sellers. We introduce the notion that sellers' and consumers' payoffs can be congruent or conflicting, and show that this has important implications for the effects of bias. Under congruence, the firm benefiting from bias has an incentive to offer a better deal than its rival and consumers can be better‐off than under no bias. Under conflict, the favored firm offers lower utility, and bias harms consumers. We study various policies for dealing with bias and show that their efficacy also depends on whether the payoffs exhibit congruence or conflict.;A model of biased intermediation;de Cornière, A. & Taylor, G.;2019;Taylor, G.;Digital Economies
Medicines are increasingly purchased online, yet little is known regarding the ocular information-seeking behavior with medicine queries in search engines. A share of pharmacies found via search engines operate unlicensed and sell prescription-only medicines without a prescription. This study aimed to investigate how search engine users distinguish between genuine and falsified sources of information in terms of unlicensed and licensed online pharmacies in the case of medicine queries. Eye-tracking of search tasks (transactional, navigational, informational and two limited results) in a Google search engine environment with retrospective gaze-cued think aloud protocol. Purposive sample of N = 50 across three hospitals and one general practitioner. Results: Discovery of a trichotomy of ocular search strategies based on the inclusion or exclusion of URLs in the information-seeking process. Finding of dissonance to existing studies related to fixation duration of search engine result page (SERP) elements. Discovery of an addition to information foraging theory (IFT): proximal cues are, in environments with non-credible information, used in both positive and negative ways.;Information-Seeking Strategies in Medicine Queries: A Clinical Eye-Tracking Study with Gaze-Cued Retrospective Think-Aloud Protocol;Muntinga, T. & Taylor, G.;2017;Taylor, G.;Digital Economies
Consumers tend to browse products they are interested in and firms often invest resources in selling to them. A consequence, I show, is that it is optimal for a firm to increase the cost of browsing (even though this drives away potential customers) because doing so allows it to target sales efforts at those consumers most likely to buy. Despite representing pure waste, this can increase welfare by facilitating efficient allocation of sales or marketing resources. For a similar reason, consumers often benefit from search costs in aggregate, and prefer them to other means of screening, such as price increases.;Raising search costs to deter window shopping can increase profits and welfare;Taylor, G.;2017;Taylor, G.;Digital Economies
We study the effects of integration between a search engine and a publisher. In a model in which the search engine (i) allocates users across publishers and (ii) competes with publishers to attract advertisers, we find that the search engine is biased against publishers that display many ads – even without integration. Integration can (but need not) lead to own‐content bias. It can also benefit consumers by reducing the nuisance costs due to excessive advertising. Advertisers are more likely to suffer from integration than consumers. On net, the welfare effects of integration are ambiguous.;Integration and search engine bias;de Cornière, A. & Taylor, G.;2014;Taylor, G.;Digital Economies
Consumers are attracted by high‐quality search results. Search engines, though, essentially compete against themselves because consumers are induced to substitute away from advertisement links when their organic counterparts are of high quality. I characterize the effect of such revenue cannibalization upon equilibrium quality when search engines compete for clicks. Cannibalization provides an incentive for quality degradation, engendering low‐quality equilibria—even when provision is costless. When consumers exhibit loyalty there is a ceiling above which result quality cannot rise, regardless of what the maximum feasible quality happens to be. Seemingly procompetitive developments may exert downward pressure on equilibrium quality.;Search Quality and Revenue Cannibalization by Competing Search Engines;Taylor, G.;2013;Taylor, G.;Digital Economies
Search has assumed a position of central importance in the way that people access and use online information and services. In this introduction we summarize the four articles constituting this themed section, and in so doing explore the ascendancy of search, the power it bestows upon those who control it, its role in shaping access to information, and its capacity to function as a mirror for society. We point to important outstanding questions and suggest some avenues for future work in this area.;Re: Search;Graham, M., Schroeder, R. & Taylor, G.;2013;Taylor, G.;"Digital Economies; Information Geography and Inequality"
A well-known myopic bidding strategy fails to support an equilibrium of simultaneous ascending proxy auctions for heterogeneous items when a hard-close rule is in place. This is because, in common with the single-auction case, last minute bidding (sniping) is a best response to naive behaviour. However, a modification to the myopic strategy in which all bidders submit an additional bid in the closing stages of the auction–a practice I call ‘defensive sniping’–is shown to yield an efficient, belief-free equilibrium of such environments. This equilibrium is essentially unique within the class of belief-free, efficient equilibria.;Defensive sniping and efficiency in simultaneous hard-close proxy auctions;Taylor, G.;2012;Taylor, G.;Digital Economies
Sending general advertisements with inflationary claims may attract additional visitors with whom an advertiser is poorly matched. This is costly when ads are priced per-click because many visitors (clickers) will not purchase. This renders per-click advertising particularly conducive to the transmission of information via ads. The admissibility of information transmission depends not only on advertiser behaviour, but also upon consumers' interpretation of and trust in ads. In less conducive environments, consumers quickly learn to place little stock in the claims they see advertised. This mechanism undermines the ability of advertisers and consumers to communicate under per-impression or per-sale fee structures. Consumers benefit from increased informativeness, but distortions introduced by the market power given to advertisers imply that society may be better-off with no information transmission taking place.;The informativeness of on-line advertising;Taylor, G.;2011;Taylor, G.;Digital Economies
This article investigates the various visions of virtuality which have been applied to public organizations over the past decade, discerning three ways in which we might expect organizations to become more virtual. It then examines the evidence to assess the extent to which organizations have experienced “virtualization.” Although the evidence does not appear to justify the wildest claims of some commentators, there is no doubt that two key trends in public management discussed elsewhere in this volume—e-government and new public management—are bringing virtuality in various guises. The final part of the article looks into the future, at the potential for these trends (and the relationship between them) for bringing further virtualization.;Virtual Organizations;Margetts, H.;2007;Margetts, H.;Digital Politics and Government
Digital government refers to the use by government of information and communication technology, including the Internet, both internally and to interact with citizens, businesses, and other governments. This chapter briefly outlines the development of digital government. It suggests three key ways in which digital government could be more transparent than government of the ‘pre-digital’ era and three ways in which it might become less transparent. The chapter goes on to identify some ways in which these ‘barriers’ to transparency might be overcome, such as the use of electronic tools like search engines and software. Finally, it discusses the strong variations in the potential for digitally aided transparency across countries, within countries, and within groups of Internet users and non-users. Some non-democratic states have resisted the potential of e-government to promote transparency, and have been more interested in trying to restrict usage of the Internet within their boundaries. This chapter investigates some of these variations in digitally aided transparency.;Transparency and Digital Government;Margetts, H.;2006;Margetts, H.;Digital Politics and Government
"This chapter does not attempt to verify the validity of the claims made for modernization, nor to develop the ‘theory of modernization’, but rather to discern from them analytically some meaning of the word that may be used to identify characteristics of a modernization reform, characteristics that might prove to be associated with surprise, disappointment, and other unintended or unanticipated consequences. It draws out from the most basic and earliest discussions of the term ‘modernization’ three possible candidates for the characteristics or ‘pillars’ of modernization reform: efficiency, integration, and specialization. It then looks at the work of analysts, proponents, and critics of three broad types of modernization: social modernization emerging from societal trends, such as changes in belief and value systems; state-centred modernization, where the state drives social change; and modernization of the state itself, geared at creating a more efficient and productive state. The final section considers whether the three characteristics of modernization (efficiency, integration, and specialization) have survived the analysis and how they might be used to classify a reform as ‘modernizing’ and be identified as possible sources of ‘paradoxes of modernization’ discussed in this book.";Modernization Dreams and Public Policy Reform;Margetts, H.;2010;Margetts, H.;Digital Politics and Government
This chapter asks three kinds of questions about the risks of unanticipated and unintended consequences arising from modernizing reforms. How do we understand and classify these risks? How should we best explain these outcomes, when they do arise? And can our explanations suggest anything important of a prescriptive character about how policy-makers might work to contain at least some of these risks? It presents a synthesis of what can be learnt from these case studies that might help to develop answers to these questions.;Modernization, Balance, and Variety;Margetts, H.;2010;Margetts, H.;Digital Politics and Government
"This chapter begins by presenting the rationale behind the writing of this book. It then discusses the meanings of ‘modern’, ‘modernity’, ‘modernization’, and ‘modernism’; the notion of unintended effects of human action, and the lessons that can we can learn from examining paradoxes of modernization in public services.";The Drive to Modernize: A World of Surprises?;Hood, C. & Margetts, H.;2010;Margetts, H.;Digital Politics and Government
This paper maps the national UK web presence on the basis of an analysis of the .uk domain from 1996 to 2010. It reviews previous attempts to use web archives to understand national web domains and describes the dataset. Next, it presents an analysis of the .uk domain, including the overall number of links in the archive and changes in the link density of different second-level domains over time. We then explore changes over time within a particular second-level domain, the academic subdomain .ac.uk, and compare linking practices with variables, including institutional affiliation, league table ranking, and geographic location. We do not detect institutional affiliation affecting linking practices and find only partial evidence of league table ranking affecting network centrality, but find a clear inverse relationship between the density of links and the geographical distance between universities. This echoes prior findings regarding offline academic activity, which allows us to argue that real-world factors like geography continue to shape academic relationships even in the Internet age. We conclude with directions for future uses of web archive resources in this emerging area of research.;Mapping the UK webspace: fifteen years of british universities on the web;Hale, S., Yasseri, T. & Cowls, J. et al.;2014;Margetts, H.;Digital Politics and Government
This paper tests the hypothesis that social information provided by the internet makes it  possible in large groups to exert social influence that Olson considered viable only for smaller groups. In two experiments - laboratory and field - subjects could choose to sign petitionsand donate money to support causes. Participants were randomised into treatment groupsthat received varying information about how many other people had participated and control groups receiving no social information. Results suggest that social information has avarying effect according to the numbers provided, strongest when there are more than amillion other participants, lending support to the social information hypothesis and to claimsabout critical mass and tipping points in political participation.;How many people does it take to change a petition? Experiments to investigate the impact of on-line social information on collective action;Margetts, H., John, P. & Escher, T.;2009;Margetts, H.;Digital Politics and Government
"This paper investigates the impact of the internet upon individual contributions to collective action. It examines how political participation may be stimulated by one particular characteristic of the internet: its ability to provide real-time feedback information on the participation of others in a political action. The paper tests the hypotheses: that such information makes it possible in large groups to exert the social pressure that Olson (writing in the pre- internet era) considered only viable for smaller groups; that such social pressure will be greatest when the number of other participants are large, making people aware of what sociologists have termed a critical mass of support; and that feedback information about small numbers of other participants will have greatest effect, convincing individuals that their participation will make a difference. The paper uses an experimental design, first with 47 laboratory-based subjects, who were invited to sign petitions and donate money, then with 668 subjects in the field. The participants were randomised into treatment and control groups, who saw the numbers of other people who had signed the petitions, and those who did not. The paper finds a statistically significant difference between these groups. Furthermore, the signing of petitions in the treatment group increased relative to other petitions when the numbers presented were greater than one million. Where the numbers were in the middle range, the treatment group were significantly more likely to donate money to the issue. There was no impact at the lower end of the scale, with numbers less than twelve. The findings lend support to Marwell and Oliver’s claims about critical mass. ";Can the internet overcome the logic of collective action? An experimental approach to investigating the impact of social pressure on political participation ;Margetts, H., John, P. & Escher, T. et al.;2009;Margetts, H.;Digital Politics and Government
The shift of much of political life on to the Internet and WWW has implications for understanding of political behaviour, particularly people's willingness to undertake collective action and organise around public goods. Web-based experiments are an under-used methodology to identify and investigate these Internet effects. This paper reports on two such experiments, one in the laboratory and the other in the field, which explored how one particular characteristic of the Internet - the ability to feed real-time information about the behaviour of others back to an individual user - can affect people's incentives to act collectively. The results suggest that information about high numbers of other participants positively affect an individual's willingness to participate, while information about low numbers of other participants can have a negative effect, suggesting that a revision of the 'logic' of collective action may be necessary for the Internet age. Collective action has been a key puzzle of political science since the 1960s. In The Logic of Collective Action, Mancur Olson (1965) put forward a thesis about when individuals can be incentivized to act collectively. He argued that, when organising around collective goods, 'small groups are more efficient and viable than large ones' and that if they are not, they need to be able to coerce their members or provide selective incentives to contributors. Generations of social scientists have worried over the implications of Olson's argument, which skews the influence of interest groups, limiting the ability of large groups to represent their interests. More recently, a number of studies have suggested that larger groups may actually find it easier to form, as their size makes it more likely they will be able to attain a 'critical mass' of activists who organise around public goods (Marwell and Oliver, 1993), and that the costs of collective action around many public goods vary little with group size, due to 'jointness of supply'. In these cases free-riding is unlikely to be problematic, larger groups are just as likely to exhibit collective action as smaller ones, and indeed under some conditions more likely, as they are more likely to be able to assemble a 'critical mass' of activists. This argument supported by claims for the so-called 'bandwagon effect', which causes people to sign up to a party or cause with large numbers of supporters because they feel that they are 'on a roll' (see Marsh, 1984). Various claims have been made for the effect of the Internet on the 'logic' of collective action (e.g. Lupia and Sin, 2003), but empirical evidence is scarce. This paper reports two experiments aimed at testing empirically how key aspects of Internet- based communication affect collective action decisions. Specifically, it examines the effect of providing internet users with real-time information about other people's participatory actions on people's incentives to act collectively and to organise around public goods. . In fact, although many theories of collective action are based on the assumption that potential participants know the level of participation of others, in the pre-Internet era this was most usually not the case. The Internet therefore, provides us with the opportunity to test these theories for the first time as well as investigating the effect of the Internet itself. Our hypothesis is that information about how many other people have undertaken a participatory activity (such as donating money to a cause or signing a petition) will affect people's decisions about whether to incur costs themselves in the pursuit of collective action. That is, if people know (for example) how many other people have signed a petition, we hypothesise that it will affect their willingness to sign or to incur other costs in the pursuit of the issue petitioned for. We also hypothesise that information about different levels of other signatories will have differential effects, building on the work of both Olson and Marwell and Oliver. That is, where very low numbers of people have signed a petition, the information could have a negative impact on one individual's propensity to sign, as they will consider it a hopeless cause - or it could have a positive effect, as any one individual will feel that their action will make a significant difference. Where very high numbers of people have signed, the information could have a positive impact, generating excitement, social pressure and a feeling that they can be part of change - or it could act negatively, making people feel that so many other people are acting that their contribution would be insignificant.;Experiments for Web Science: Examining the Effect of the Internet on Collective Action;Margetts, H., John, P. & Escher, T. et al.;2009;Margetts, H.;Digital Politics and Government
Economists studying commercial activity on-line argue that the most significant difference between on-line and off-line commerce is the ability of firms to ‘know who your customers are and treat them differently’ (Vulkan 2006), customizing prices and offerings. This difference comes from the huge amount of data generated by on-line transactions, in terms of historical records, usage statistics and real-time data. Yet in political life, governmental organizations and political parties have been far slower to use such data to improve their service offerings and devise innovative policy interventions, such as differential pricing and personalized information provision. Likewise, political scientists lag behind economists in terms of analyzing new on-line relationships between citizens and political organizations, for example through the use of experiments and modelling of transaction data. This paper investigates ways in which governments and political scientists might also further understanding of government- citizen interactions, using the results of laboratory experiments where subjects are incentivized to simulate social choices on-line. The findings might be used by governmental organizations to feed into service improvements and policy innovation processes. ;UNDERSTANDING GOVERNMENTS AND CITIZENS ON-LINE: LEARNING FROM E-COMMERCE ;Escher, T. & Margetts, H.;2007;Margetts, H.;Digital Politics and Government
What difference does e-government make to the capacity of governments to interact with citizens? This paper investigates how widespread use of the Internet by citizens and governments affects government’s place in social and informational networks - the ‘nodality’ of contemporary government - and how citizens experience government on-line. It uses methods from computer science (particularly webmetrics) and political science (a ‘tools of government’ approach) to go further than previous work in developing a methodology to quantitatively analyse the structure of 'government on the web', building on Petricek et al (2006). It applies structural metrics (via webcrawling) and user metrics (via user experiments) to the web sites of comparable ministries concerned with foreign affairs in three countries (Australia, the US and the UK). The results are used to assess the on-line presence of the three foreign offices along five dimensions: visibility, accessibility, extroversion, navigability and competitiveness. These dimensions might be developed further as indicators for use by both researchers (to assess e-government initiatives) and by governments (to improve the effectiveness and efficiency of their on-line presence). Governments which are successful in developing their web sites in this way are likely to have greater visibility to citizens, businesses and other governments, strengthening nodality as a policy tool.;Governing from the Centre? Comparing the Nodality of Digital Governments;Margetts, H. & Escher, T.;2006;Margetts, H.;Digital Politics and Government
In this paper we describe preliminary work that examines whether statistical properties of the structure of websites can be an informative measure of their quality. We aim to develop a new method for evaluating e-government. E-government websites are evaluated regularly by consulting companies, international organizations and academic researchers using a variety of subjective measures. We aim to improve on these evaluations using a range of techniques from webmetric and social network analysis. To pilot our methodology, we examine the structure of government audit office sites in Canada, the USA, the UK, New Zealand and the Czech Republic. We report experimental values for a variety of characteristics, including the connected components, the average distance between nodes, the distribution of paths lengths, and the indegree and outdegree. These measures are expected to correlate with (i) the navigability of a website and (ii) with its “nodality” which is a combination of hubness and authority. Comparison of websites based on these characteristics raised a number of issues, related to the proportion of non-hyperlinked content (e.g. pdf and doc files) within a site, and both the very significant differences in the size of the websites and their respective national populations. Methods to account for these issues are proposed and discussed. There appears to be some correlation between the values measured and the league tables reported in the literature. However, this multi-dimensional analysis provides a richer source of evaluative techniques than previous work. Our analysis indicates that the US and Canada provide better navigability, much better than the UK, however the UK site is shown to have the strongest “nodality” on the Web.;The Web Structure of E-Government - Developing a Methodology for Quantitative Evaluation ;Petricek, V., Escher, T. & Cox, I. et al.;2006;Margetts, H.;Digital Politics and Government
Political campaigning on social media is a core feature of contemporary democracy. However, evidence of the effectiveness of this type of campaigning is thin. This study tests three theories linking social media to vote outcomes, using a novel 6,000 observation panel data set from two British elections. We find that Twitter-based campaigning does seem to help win votes. The impact of Twitter use is small, though comparable with campaign spending. Our data suggest that social media campaign effects are achieved through using Twitter as a broadcast mechanism. Despite much literature encouraging politicians to engage with social platforms in an interactive fashion, we find no evidence that this style of communication improves electoral outcomes. In light of our results, theories of how social media are changing processes of campaigns and elections are discussed and enhanced.;Does Campaigning on Social Media Make a Difference? Evidence From Candidate Use of Twitter During the 2015 and 2017 U.K. Elections;Bright, J., Hale, S. & Ganesh, B. et al.;2019;Margetts, H.;Digital Politics and Government
The field of computational social science (CSS) has exploded in prominence over the past decade, with thousands of papers published using observational data, experimental designs, and large-scale simulations that were once unfeasible or unavailable to researchers. These studies have greatly improved our understanding of important phenomena, ranging from social inequality to the spread of infectious diseases. The institutions supporting CSS in the academy have also grown substantially, as evidenced by the proliferation of conferences, workshops, and summer schools across the globe, across disciplines, and across sources of data. But the field has also fallen short in important ways. Many institutional structures around the field—including research ethics, pedagogy, and data infrastructure—are still nascent. We suggest opportunities to address these issues, especially in improving the alignment between the organization of the 20th-century university and the intellectual requirements of the field.;Computational social science: Obstacles and opportunities;Lazer, D., Pentland, A. & Duncan, W. et al.;2020;Margetts, H.;Digital Politics and Government
To conclude this special section, this article looks at the possible avenues for regulatory reform in the field of digital campaigning. Diagnosing the need for a multi‐layered approach, we argue that action is needed from government, regulators, companies, and civil society. We take each actor in turn and consider the kind of change needed, the prospects for reform, and outline four recommendations for change.;Conclusion: Four Recommendations to Improve Digital Electoral Oversight in the UK;Margetts, H. & Dommett, K.;2020;Margetts, H.;Digital Politics and Government
Open data remains one of the most significant current trends in public administration, with hundreds of projects around the world seeking to open up stores of public sector information for future re-use. However, as open data has grown, a critical literature has also emerged which questions who the true beneficiaries of open data are, as well as highlighting the high costs it places on government. Hence systematic research on the actual outcomes of open data projects is urgently needed. This article seeks to explain the factors which promote greater levels of downloads of open government data. We show that the use of profit making datasets far exceeds those which could be used to enhance government transparency or service delivery. We also show that well updated datasets with high quality metadata are more likely to be downloaded. We conclude by supporting currently developing calls for prioritisation in open government data programmes.;Explaining download patterns in open government data: citizen participation or private enterprise?;Bright, J., Lee, S. & Margetts, H.;2019;Margetts, H.;Digital Politics and Government
Policymakers should harness data to deliver public services that are responsive, efficient and fair, urge Helen Margetts and Cosmina Dorobantu. People produce more than 2.5 quintillion bytes of data each day. Businesses are harnessing these riches using artificial intelligence (AI) to add trillions of dollars in value to goods and services each year. Amazon dispatches items it anticipates customers will buy to regional hubs before they are purchased. Thanks to the vast extractive might of Google and Facebook, every bakery and bicycle shop is the beneficiary of personalized targeted advertising. But governments have been slow to apply AI to hone their policies and services. The reams of data that governments collect about citizens could, in theory, be used to tailor education to the needs of each child or to fit health care to the genetics and lifestyle of each patient. They could help to predict and prevent traffic deaths, street crime or the necessity of taking children into care. Huge costs of floods, disease outbreaks and financial crises could be alleviated using state-of-the-art modelling. All of these services could become cheaper and more effective. This dream seems rather distant. Governments have long struggled with much simpler technologies. Flagship policies that rely on information technology (IT) regularly flounder. The Affordable Care Act of former US president Barack Obama nearly crumbled in 2013 when HealthCare.gov, the website enabling Americans to enrol in health insurance plans, kept crashing. Universal Credit, the biggest reform to the UK welfare state since the 1940s, is widely regarded as a disaster because of its failure to pay claimants properly. It has also wasted £837 million (US$1.1 billion) on developing one component of its digital system that was swiftly decommissioned. Canada’s Phoenix pay system, introduced in 2016 to overhaul the federal government’s payroll process, has remunerated 62% of employees incorrectly in each fiscal year since its launch. And My Health Record, Australia’s digital health-records system, saw more than 2.5 million people opt out by the end of January this year over privacy, security and efficacy concerns — roughly 1 in 10 of those who were eligible. Such failures matter. Technological innovation is essential for the state to maintain its position of authority in a data-intensive world. The digital realm is where citizens live and work, shop and play, meet and fight. Prices for goods are increasingly set by software. Work is mediated through online platforms such as Uber and Deliveroo. Voters receive targeted information — and disinformation — through social media. Thus the core tasks of governments, such as enforcing regulation, setting employment rights and ensuring fair elections require an understanding of data and algorithms. Here we highlight the main priorities, drawn from our experience of working with policymakers at The Alan Turing Institute in London.;Rethink government with AI;Margetts, H.;2019;Margetts, H.;Digital Politics and Government
SOCIAL MEDIA are blamed for almost everything that is wrong with democracy. They are held responsible for pollution of the democratic environment through fake news, junk science, computational propaganda and aggressive microtargeting and political advertising. They are accused of creating political filter bubbles, where citizens exist in ever narrower ‘echo chambers’ of personalised news and connections with like-minded people, which mean that they are exposed only to similar ideological viewpoints, feeding their own opinions back to themselves and creating a ‘Daily Me’ news environment. In turn, these phenomena have been implicated in the rise of populism, political polarisation, waves of hate against women and minorities, far-right extremism and radicalisation, post-truth, political chaos, the end of democracy and ultimately, the death of democracy.’ Discussion of social media's role in democracy sounds like a premature lament for a sick patient, without investigating the prognosis. Yet, actually we know rather little about the relationship between social media and democracy. In their ten years of existence, social media have injected volatility and instability into political systems, bringing a continual cast of unpredictable events. They have challenged normative models of democracy—by which we might understand the macro-level shifts at work—seeming to make possible the highest hopes and worst fears of republicanism and pluralism. They have transformed the ecology of interest groups and mobilisations which challenge elites and ruling institutions, bringing regulatory decay and policy sclerosis. They create undercurrents of political life that burst to the surface in seemingly random ways, making fools of opinion polls. But although the platforms themselves generating new sources of real-time transactional data that might be used to understand this changed environment, most of this data is proprietary and inaccessible to researchers, meaning that the revolution in big data and data science has passed by democracy research. This chapter looks at the available evidence regarding the effect of social media on democracy, for which—as for Mark Twain—the report of death may be an exaggeration?;Rethinking Democracy with Social Media;Margetts, H.;2018;Margetts, H.;Digital Politics and Government
Political behaviour increasingly takes place on digital platforms, where people are presented with a range of social information—real-time feedback about the behaviour of peers and reference groups—which can stimulate (or depress) participation. This social information is hypothesized to impact the distribution of political activity, stimulating participation in mobilizations that are increasing in popularity, and depressing participation in those that appear to be less popular, leading to a non-normal distribution. Changes to these platforms can generate natural experiments allowing for an estimate of the impact of different kinds of social information on participation. This paper tests the hypothesis that social information shapes the distribution of political mobilizations by examining the introduction of trending information to the homepage of the UK government petition platform. The introduction of the trending feature did not increase the overall number of signatures per day, but the distribution of signatures across petitions changed significantly—the most popular petitions gained more signatures at the expense of those with fewer signatories. We further find significant differences between petitions trending at different ranks on the homepage. This evidence suggests that the ubiquity of trending information on digital platforms is introducing instability into political markets, as has been shown for cultural markets. As well as highlighting the importance of digital design in shaping political behaviour, the findings suggest that a non-negligible group of individuals visit the homepage of the site looking for petitions to sign, without having decided the issues they wish to support in advance. These ‘aimless petitioners’ are particularly susceptible to changes in social information.;How digital design shapes political participation: A natural experiment with social information;Hale, S., John, P. & Margetts, H. et al.;2018;Margetts, H.;Digital Politics and Government
"Contemporary collective action, much of which involves social media and other Internet-based platforms, leaves a digital imprint which may be harvested to better understand the dynamics of mobilization. Petition signing is an example of collective action which has gained in popularity with rising use of social media and provides such data for the whole population of petition signatories for a given platform. This paper tracks the growth curves of all 20,000 petitions to the UK government petitions website (http://epetitions.direct.gov.uk) and 1,800 petitions to the US White House site (https://petitions.whitehouse.gov), analyzing the rate of growth and outreach mechanism. Previous research has suggested the importance of the first day to the ultimate success of a petition, but has not examined early growth within that day, made possible here through hourly resolution in the data. The analysis shows that the vast majority of petitions do not achieve any measure of success; over 99 percent fail to get the 10,000 signatures required for an official response and only 0.1 percent attain the 100,000 required for a parliamentary debate (0.7 percent in the US). We analyze the data through a multiplicative process model framework to explain the heterogeneous growth of signatures at the population level. We define and measure an average outreach factor for petitions and show that it decays very fast (reducing to 0.1% after 10 hours in the UK and 30 hours in the US). After a day or two, a petition’s fate is virtually set. The findings challenge conventional analyses of collective action from economics and political science, where the production function has been assumed to follow an S-shaped curve.";Rapid rise and decay in petition signing;Yasseri, T., Hale, S. & Margetts, H.;2017;Margetts, H.;Digital Politics and Government
Who won the 2017 General Election? Although the Conservatives won the largest share of the vote and seats, their disastrous campaign rendered the party reputationally damaged, disastrously split on Brexit and with a precarious minority government, expensively propped up by the Democratic Unionist Party. Neither the party nor Theresa May herself, whose much advertised ‘strong and stable’ leadership crumbled visibly through the campaign, looked like winners after the election, which had been called as a cynical but seemingly safe device to ratchet up her majority. Labour's campaign was widely regarded as a success and the fortunes of both the party and their leader turned around during the course of it, with Corbyn polling thirty-nine per cent behind May as ‘who would make the best PM’ at the start and ahead of May by the end,’ But their forty per cent of the vote and 262 seats put both party and leader clearly into second place, with no chance of forming a government, however much their prospective fortunes were revived. The smaller parties also emerged from the election as losers. The Liberal Democrats improved only incrementally on their disastrous 2015 performance, UKIP tanked at two per cent of the vote (down eleven per cent) and the Scottish National Party lost twenty-one seats. The tabloid press looked like losers too, after running a typically vicious campaign of attack against Corbyn, which failed in the face of history and all odds to halt the wave of support for him among young people. The polling companies failed to anticipate the change in parties’ fortunes, and their predictions during the campaign fluctuated so wildly over time and between companies that some commentators claimed they looked like random number generators. Party strategists, journalists and academics across the political spectrum completely failed to foresee the result, and one university lecturer who had promised to eat his book if his prediction was wrong was seen doing so on Sky news. So after the 2017 election, none of the usual protagonists look like winners. In contrast, social media had a good election. Indeed, 2017 may be remembered as the first election where it seems to have been the social media campaigns that really made the difference to the relative fortunes of the parties, rather than traditional media. And it could be the first election where the right-wing tabloids finally ceded their influence to new media, their power over politics broken according to some commentators. It may also be social media that explain why so many pollsters and pundits failed to realise what was happening during the campaign.;Why Social Media May Have Won the 2017 General Election;Margetts, H.;2017;Margetts, H.;Digital Politics and Government
This article engages with the methods content in Dowding’s book, examining the underlying assumptions regarding political science methodology and data and arguing that these issues deserve a more central role. In particular, the new possibilities for generation and analysis of large-scale data sources from political activity is driving methodological change, requiring a variety of multi-disciplinary expertise, and presenting new ethical challenges. Expansion of the toolkit of political science methodologies to incorporate the growing field of computational social science, or social data science, draws new scientists into the analysis of politics – such as mathematicians, computer scientists and physicists – who bring their own philosophy and practice of theory building, explanation and approach, which may shape the discipline in the future.;The Data Science of Politics;Margetts, H.;2017;Margetts, H.;Digital Politics and Government
Social networks are not a new phenomenon — people have always associated with like-minded others — but the advent of social media has led to a vast increase in the amount of social information that we see. We need data and experiments to understand how this information shapes our political landscape.;Political behaviour and the acoustics of social media;Margetts, H.;2017;Margetts, H.;Digital Politics and Government
"This editorial introduces a special issue resulting from a panel on Internet and policy organized by the Oxford Internet Institute (University of Oxford) at the 2015 International Conference on Public Policy (ICPP) held in Milan. Two main themes emerged from the panel: the challenges of high cost and low participation which many e‐participation initiatives have faced; and the potential Big Data seems to hold for remedying these problems. This introduction briefly presents these themes and links them to the papers in the issue. It argues that Big Data can fix some of the problems typically encountered by e‐participation initiatives: it can offer a solution to the problem of low turnout which is furthermore accessible to government bodies even if they have low levels of financial resources. However, the use of Big Data in this way is also a radically different approach to the problem of involving citizens in policymaking; and the editorial concludes by reflecting on the significance of this for the policymaking process.";Big Data and Public Policy: Can It Succeed Where E‐Participation Has Failed?;Bright, J. & Margetts, H.;2016;Margetts, H.;Digital Politics and Government
"Citizens and governments live increasingly digital lives, leaving trails of digital data that have the potential to support unprecedented levels of mutual government–citizen understanding, and in turn, vast improvements to public policies and services. Open data and open government initiatives promise to “open up” government operations to citizens. New forms of “big data” analysis can be used by government itself to understand citizens' behavior and reveal the strengths and weaknesses of policy and service delivery. In practice, however, open data emerges as a reform development directed to a range of goals, including the stimulation of economic development, and not strictly transparency or public service improvement. Meanwhile, governments have been slow to capitalize on the potential of big data, while the largest data they do collect remain “closed” and under‐exploited within the confines of intelligence agencies. Drawing on interviews with civil servants and researchers in Canada, the United Kingdom, and the United States between 2011 and 2014, this article argues that a big data approach could offer the greatest potential as a vehicle for improving mutual government–citizen understanding, thus embodying the core tenets of Digital Era Governance, argued by some authors to be the most viable public management model for the digital age (Dunleavy, Margetts, Bastow, & Tinkler, 2005, 2006; Margetts & Dunleavy, 2013).";Governments and Citizens Getting to Know Each Other? Open, Closed, and Big Data in Public Management Reform;Clarke, A. & Margetts, H.;2014;Margetts, H.;Digital Politics and Government
Investigating Political Participation and Social Information using Big Data and a Natural Experiment Social information is particularly prominent in digital settings where the design of platforms can more easily give real-time information about the behaviour of peers and reference groups and thereby stimulate political activity. Changes to these platforms can generate natural experiments allowing an assessment of the impact of changes in social information and design on participation. This paper investigates the impact of the introduction of trending information on the homepage of the UK government petitions platform. Using interrupted time series and a regression discontinuity design, we find that the introduction of the trending feature had no statistically significant effect on the overall number of signatures per day, but that the distribution of signatures across petitions changes — the most popular petitions gain even more signatures at the expense of those with less signatories. We find significant differences between petitions trending at different ranks, even after controlling for each petition’s individual growth prior to trending. The findings suggest a non-negligible group of individuals visit the homepage of the site looking for petitions to sign and therefore see the list of trending petitions, and a significant proportion of this group responds to the social information that it provides. These findings contribute to our understanding of how social information, and the form in which it is presented, affects individual political behaviour in digital settings.;Investigating Political Participation and Social Information Using Big Data and a Natural Experiment;Hale, S., John, P. & Margetts, H. et al.;2014;Margetts, H.;Digital Politics and Government
The internet has been ascribed a prominent role in collective action, particularly with widespread use of social media. But most mobilisations fail. We investigate the characteristics of those few mobilisations that succeed and hypothesise that the presence of ‘starters’ with low thresholds for joining will determine whether a mobilisation achieves success, as suggested by threshold models. We use experimental data from public good games to identify personality types associated with willingness to start in collective action. We find a significant association between both extraversion and internal locus of control, and willingness to start, while agreeableness is associated with a tendency to follow. Rounds without at least a minimum level of extraversion among the participants are unlikely to be funded, providing some support for the hypothesis.;Leadership without Leaders? Starters and Followers in Online Collective Action;Margetts, H., John, P. & Hale, S. et al.;2013;Margetts, H.;Digital Politics and Government
"Vast amounts of transactional data are collected about us as we go about our daily lives online. Shopping, mobile phones, using transport, entertaining ourselves—all leave a data trail that enables companies, even the state, to track the most mundane aspects of our lives. Add to this all the personal data we willingly share on blogs and social networks like Facebook and Twitter, and academics potentially have access to terabytes of data that can provide new and surprising insights into human behavior and social structure. This “big data” not only offers enormous scope for understanding citizens' willingness—or unwillingness—in terms of civic engagement; it allows social and political scientists to tackle longstanding problems that have hitherto been impossible to address, such as how political movements like the “Arab Spring” and Occupy originate and spread (Bastos, Travitzki, & Raimundo, 2012; Lindgren, 2012). It also holds promise in terms of the design of efficient policy and administrative change. However, collection and use of these data also raises a whole range of ethical challenges (Mayer‐Schönberger & Cukier, 2013). Obvious issues surround data protection and privacy of subjects (Schroeder & Meyer, 2012), for example individual identities become discoverable from supposedly disaggregated and “anonymized” data sets; the trail of digital footprints we leave is certainly long and hard to erase (Mayer‐Schönberger, 2009). Big data can also be used for algorithmic and probabilistic policymaking, raising issues of justice and equity. It holds the potential for more coercive modes of governance, whether by introducing conditionality into public policy and services or simply exerting “nudges.” We are also faced with the challenge that traditional databases and desktop computers may be inadequate for the collection, storage, and analysis of larger, more complex social media data sets. More computing power, alternative tools for storage, and new methods for utilizing computational clusters are needed, including computing system architectures suited to constant real‐time data collection, and new forms of fast, flexible data storage for retrieval and curation (Walker, Eckert, Hemsley, Mason, & Nahon, 2012). These data are also often the result of “mechanisms of observation, inscription, and representation that serve specific ends—ends, in the case of big data, which are very often commercial” (Barocas, 2012). This raises its own challenges: not only in terms of how we obtain access to proprietary or walled data (e.g., from Google or Facebook), but also how we use it to answer questions that are of interest to us as social scientists, particularly when we have no control over what data are collected or how it is done.";Addressing the policy challenges and opportunities of “Big data”;Margetts, H. & Sutcliffe, D.;2013;Margetts, H.;Digital Politics and Government
Widespread use of the Internet and the Web has transformed the public management ‘quasi-paradigm’ in advanced industrial countries. The toolkit for public management reform has shifted away from a ‘new public management’ (NPM) approach stressing fragmentation, competition and incentivization and towards a ‘digital-era governance’ (DEG) one, focusing on reintegrating services, providing holistic services for citizens and implementing thoroughgoing digital changes in administration. We review the current status of NPM and DEG approaches, showing how the development of the social Web has already helped trigger a ‘second wave’ of DEG2 changes. Web science and organizational studies are converging swiftly in public management and public services, opening up an extensive agenda for future redesign of state organization and interventions. So far, DEG changes have survived austerity pressures well, whereas key NPM elements have been rolled back.;The second wave of digital-era governance: a quasi-paradigm for government on the Web;Margetts, H. & Dunleavy, P.;2013;Margetts, H.;Digital Politics and Government
TRANSPARENCY is a multifaceted concept with a long history in discussion of governance and institutional design.’ Variations on the transparency theme include openness, surveillance, account- ability, simplicity and notions of rule - governed, predictable governance processes which fulfil citizens’ ‘right to know’ about government and policy making and acting as the ‘key to better governance’ by enhancing certain administrative values, such as integrity, fairness and efficiency.” This article investigates the effect of widespread use of the Internet on transparency, outlining the Internet-based applications that seem to be drawing institutions towards greater transparency and examining the variant of transparency emerging. It could be that this latest twist in the transparency tale emphasises certain elements of the concept—openness and surveillance—over others, such as predictability and accountability. By reducing trust in governmental institutions and necessitating trust in other groups or individuals, Internet-based transparency may be injecting uncertainty and unpredictability into governance.;The Internet and Transparency;Margetts, H.;2011;Margetts, H.;Digital Politics and Government
This paper tests whether the social information provided by the internet affects the decision to participate in politics. In a field experiment, subjects could choose to sign petitions and donate money to support causes. Participants were randomized into treatment groups that received varying information about how many other people had participated and a control group receiving no social information. Results show that social information has a varying effect according to the numbers provided, which is strongest when there are more than a million other participants, supporting claims about critical mass, and tipping points in political participation.;Social information and political participation on the internet: an experiment;Margetts, H., John, P. & Escher, T. et al.;2011;Margetts, H.;Digital Politics and Government
"Although there has been a ‘dramatic drift’ towards experimentation in political science, the methodology remains scarce in public management research. This article considers the potential for the experimental method for public management. It discusses the benefits and costs of an experimental design. It identifies three barriers to the use of experiments distinctive to public management; a stress on realism and practical solutions; a focus on organizations rather than individuals; and ethical and logistical challenges. It re-evaluates these barriers in the light of recent experiments, arguing that experimental approaches should now be added to the toolkit of public management research.";Experiments for Public Management Research;Margetts, H.;2011;Margetts, H.;Digital Politics and Government
The OII forum was organised to facilitate a dialogue between policymakers, police authorities, representatives of the computing industry and leading international academics on issues of mapping and measuring cybercrime. Its primary aims were to inform decision making in the policy area of cybercrime response and to support a more sophisticated rounded understanding of the issues involved. While the forum was not asked to reach a consensus in any area it raised a number of critical issues and points for further research.;Mapping and Measuring Cybercrime;Fafinski, S., Dutton, W. & Margetts, H.;2010;Margetts, H.;Digital Politics and Government
Widespread use of the Internet and the Web has transformed the public management 'quasi-paradigm' in advanced industrial countries. The toolkit for public management reform has shifted away from a 'new public management' (NPM) approach stressing fragmentation, competition and incentivization and towards a 'digital-era governance' (DEG) one, focusing on reintegrating services, providing holistic services for citizens and implementing thoroughgoing digital changes in administration. We review the current status of NPM and DEG approaches, showing how the development of the social Web has already helped trigger a 'second wave' of DEG(2) changes. Web science and organizational studies are converging swiftly in public management and public services, opening up an extensive agenda for future redesign of state organization and interventions. So far, DEG changes have survived austerity pressures well, whereas key NPM elements have been rolled back.;The Second Wave of Digital-Era Governance: A Quasi-Paradigm for Government on the Web;Margetts, H. & Dunleavy, P.;2013;Margetts, H.;Digital Politics and Government
This article argues that the extreme right in Britain has a higher level of latent support than would be indicated by its polling performance. After reviewing the likely salience of demand, supply and opportunity structures as factors that could explain the level of support of the extreme right in British politics, the article analyses survey evidence and voting data on the British National Party (BNP) from 2004 to 2007. The article presents results from surveys of the liking for and propensity to vote for the major and minor parties, and explores the patterns of preferences in the London elections. It argues that supporters of the UK Independence Party (UKIP) provide another source of latent support due to linkages perceived by the electorate between the BNP and the UKIP, especially through their perception of the most important policy problem. The article argues that the BNP has entered the mainstream of British politics and suggests that the potential support for the extreme right in Britain is more solid than many comparative studies indicate.;The Latent Support for the Extreme Right in British Politics;John, P. & Margetts, H.;2009;Margetts, H.;Digital Politics and Government
"This article looks at the role of the Internet in policymaking, identifying potential policy effects of widespread use of the Internet by citizens, firms, governments and voluntary organizations. It considers how the Internet and Internet‐enabled social change might impact upon each of the four ‘tools’ of government policy — nodality, authority, treasure and organization — and how it might impact upon the mix of tools that policymakers select. It suggests a number of values normally associated with the Internet — innovation, trust, openness and equity — that might be expected to emerge in policy trends. It discusses the implications of Internet‐driven change for public policy research, pinpointing some key methodologies that will become increasingly important; generation of large‐scale transactional data; network analysis and experimental methods. The article argues that we cannot understand, analyse or make public policy without understanding the technological, social and economic shifts associated with the Internet — a task that the journal Policy & Internet is poised to undertake.";The Internet and Public Policy;Margetts, H.;2012;Margetts, H.;Digital Politics and Government
"Progress towards realizing the full potential of eGovernment -using digital technologies to improve public services and government-citizen engagements- has been slower and less effective than the technologies’ take-up in spheres such as eCommerce.  This paper, based on the Breaking Barriers to eGovernment Project, presents seven categories of barriers to eGovernment progression and identifies eight associated legal areas that underpin these barriers. The discussion then turns to four organizational solutions to overcome the top barriers to eGovernment as identified by an online survey of eGovernment stakeholders conducted by the Oxford Internet Institute, University of Oxford. These barriers are: coordination across central, regional and local levels of government; resistance to change by government officials; lack of interoperability between IT systems; low levels of Internet use amongst certain groups; and lack of political support for eGovernment. We make some explicit recommendations which aim to further the objectives of the European Commission’s i2010 eGovernment Action Plan. The four solutions discussed here are: creating a network of eGovernment champions, segmentation of citizens, working with chaotic co-ordination and encouraging an eLiterate workforce. It is worth noting that some of these solutions could be used to tackle more than one barrier. In this way, implementation of the proposed solutions can reinforce each other and have a generalised effect in promoting IT-enabled business change across a range of government activities.";Organisational Solutions for Overcoming Barriers to eGovernment ;Eynon, R. & Margetts, H.;2007;Margetts, H.;"Digital Politics and Government; Education, Digital Life and Wellbeing; Information Geography and Inequality"
The initial emergence of e-governance appeared to be part of a more general government modernization process with the major focus concerning the potential for service delivery online and saving resources. Governments in Australia (and internationally) quickly raced towards grand e-government strategies. However, subsequent implementation has proved more problematic. e-Government has also raised wider questions about government policy making, structures of decision making and the perennial question of joined-up government. Drawing on empirical material from a seven-nation study (Australia, Canada, New Zealand, Japan, US, UK and The Netherlands), this article explores some of these themes in the Australian context and also seeks to place Australian initiatives in a comparative international context.;Australian e-Government in comparative perspective;Dunleavy, P., Margetts, H. & Bastow, S. et al.;2008;Margetts, H.;Digital Politics and Government
The “new public management” (NPM) wave in public sector organizational change was founded on themes of disaggregation, competition, and incentivization. Although its effects are still working through in countries new to NPM, this wave has now largely stalled or been reversed in some key “leading-edge” countries. This ebbing chiefly reflects the cumulation of adverse indirect effects on citizens' capacities for solving social problems because NPM has radically increased institutional and policy complexity. The character of the post-NPM regime is currently being formed. We set out the case that a range of connected and information technology–centered changes will be critical for the current and next wave of change, and we focus on themes of reintegration, needs-based holism, and digitization changes. The overall movement incorporating these new shifts is toward “digital-era governance” (DEG), which involves reintegrating functions into the governmental sphere, adopting holistic and needs-oriented structures, and progressing digitalization of administrative processes. DEG offers a perhaps unique opportunity to create self-sustaining change, in a broad range of closely connected technological, organizational, cultural, and social effects. But there are alternative scenarios as to how far DEG will be recognized as a coherent phenomenon and implemented successfully.;New Public Management Is Dead—Long Live Digital-Era Governance;Dunleavy, P., Margetts, H. & Bastow, S. et al.;2006;Margetts, H.;Digital Politics and Government
DURING the decade 1995 to 2005, the Internet transformed the way in which many UK citizens and businesses interact with banks, shops, travel companies, airlines, the media and a whole host of social groups. This article examines the extent to which a similar change has taken place with respect to interactions between society and government. Looking at it one way, e-government—defined as the use by government of information technology internally and to interact with citizens, businesses and other governments—has been developing in Britain for far more than a decade—the last 50 years, in fact, since the first computers entered large transaction processing departments (such as the Post Office) in the 1950s. But these early information systems were largely internally facing.;E-Government in Britain—A Decade On;Margetts, H.;2006;Margetts, H.;Digital Politics and Government
"This article identifies emerging risks in electronic government: the risk of government lagging behind a 'smart' society, where increasing proportions of citizens and businesses are accustomed to conduct their affairs electronically; risk involved in information technology outsourcing; and risk brought about by the integration of technology into governmental activities, particularly policy innovation. All three represent the introduction of risks more commonly associated with the private sector into government. For each of these three risk factors, the article suggests some potential strategies for dealing with them. First, government needs to use the capability of internet-based technologies to understand 'smart society' more fully. Second, the UK government in particular needs to develop a more relational style of contracting and learn from the sharp variations in contracting regimes across the world. Third, leaders of governmental organisations need to accept that digital technologies are now at the core of a wide range of their activities and adapt policy-making processes accordingly.";Smartening up to risk in electronic government;Margetts, H.;2005;Margetts, H.;Digital Politics and Government
In the immediate aftermath of the general election the Independent ran a whole-page headline illustrated with contrasting graphics showing ‘What we voted for’ and ‘What we got’, followed up by ‘…and why it’s time for change’. The paper launched a petition calling for a shift to a system that is fairer and more proportional, which in rapid time attracted tens of thousands of signatories, initially at a rate of more than 500 people a day. These developments highlighted the extent to which the plurality rule voting system for general elections (also still used for council elections in England and Wales) itself became an election issue. During the campaign itself the normal bi-polarizing statements from Labour and Conservative politicians proclaiming a ‘straight choice’ between them were typically no sooner issued that drowned out in a chorus of dissent. The Guardian featured a prominent campaign by Polly Toynbee for readers to voter Labour with the aid of a clothes peg, symbolizing distaste for the Hobson’s choice of either supporting a government with disliked policies like the invasion of Iraq, or voting for other parties and possibly ‘letting in’ the Conservatives (with more disliked policies, notably on immigration).2 The corollary of accepting the clothes peg was said to be a vigorous post-election campaign to make 2005 the last plurality rule general election.;The Impact of UK Electoral Systems;Dunleavy, P. & Margetts, H.;2005;Margetts, H.;Digital Politics and Government
This article explores the performance of the additional member systems of proportional representation introduced in the UK for elections to the Scottish Parliament, the National Assembly for Wales and the Greater London Assembly. It sets out the key features of these systems and examines how they have fared in terms of delivering (more) proportional results, using a range of measures of deviation from proportionality. On all measures the British AMS systems have relatively high levels of deviation from proportionality, but still rather less than the plurality system still used for Westminster.;How proportional are the ‘British AMS’ systems?;Dunleavy, P. & Margetts, H.;2007;Margetts, H.;Digital Politics and Government
"Jones and Baumgartner's punctuated equilibrium model of agenda change has reinvigorated decision‐making theory; moreover their US budget project offers a set of techniques to apply to UK data. We replicate the method by plotting percentage budget changes in central government budgets to see whether the distribution is normally distributed as predicted by the incrementalist account or leptokurtic as hypothesized by the punctuated equilibrium model. Taking the period 1951–96, we create 405 data points from budget changes from the National Income Accounts (‘Blue Book’) on agriculture, defence, social security, education, health, housing, industry, law and order and transport, all adjusted using the GDP deflator at factor cost. We find that the budget changes form a leptokurtic distribution. Such a pattern appears in most policy sectors.";Policy punctuations in the UK: fluctuations and equilibria in central government expenditure since 1951;John, P. & Margetts, H.;2003;Margetts, H.;Digital Politics and Government
“We can continue with the over-centralised, secretive and discredited system of government that we have at present. Or we can change and trust the people to take more control over their own lives.” Tony Blair, June 1996. The late 1980s and early-to-middle 1990s are now widely seen as a low period in British political and constitutional history. The apparent long-term exclusion of Labour from power encouraged excesses by the governing Conservatives in several areas. There was a series of costly policy disasters such as the poll tax, the under-valuation of early privatisations and the BSE crisis. Patronage and ‘sleaze’ in party finances and parliamentary lobbying became a major public concern. The over-concentration of power in Westminster and Whitehall led to inroads into civil liberties! and some extended efforts to reshape state activities in a preference-shaping way at national and local government levels.? At the time, Labour in opposition was highly critical of the decline of constitutional checks and balances, and of the accompanying slump in public confidence in the system of government and main political institutions. In a speech at Edinburgh University (from which the epigraph above is taken) Tony Blair posed an apparently clear choice of future directions for the British polity.;Constitutional Reform, New Labour in Power and Public Trust in Government;Dunleavy, P., Margetts, H. & Smith, T. et al.;2001;Margetts, H.;Digital Politics and Government
"The rapid changes in electoral systems and party systems in Britain since 1997 pose fundamental problems of explanation both for electoral system analysts and for students of British politics. We first describe the main types of electoral system change introduced and show how the new systems have already brought about important differences in party systems and patterns of party government across the UK. Possible explanations of the changes include: general trends across liberal democracies to re-appraise their historic voting systems; the UK’s historical and political distinctiveness in comparative terms; long-run processes specific to Britain triggering a lagged or ‘catch-up’ political and constitutional modernization; and short run (‘why now?’) causal factors. In our view change is already irreversible. For the foreseeable future either plurality rule and new electoral systems will co-exist within a primary/secondary elections structure; or the new systems will over time erode the previously foundational position of plurality rule, in tandem with a realignment of party politics UK-wide.";From Majoritarian to Pluralist Democracy?: Electoral Reform in Britain Since 1997;Dunleavy, P. & Margetts, H.;2001;Margetts, H.;Digital Politics and Government
In many ways, it is time for celebration. Four decades ago, on January 28, 1981, the Council of Europe opened Convention 108 for signature. Coming into force in 1985, it was followed by numerous national data protection laws, the European Union Privacy Directive in 1995, and most recently the EU GDPR, giving more than two dozen European nations a single, strong, and enforceable data protection regime that even California is attempting to emulate. The protection of personal data has not only evolved, it seems that it has become a central focus in our hyperconnected world. Decades of conceptual thinking, legislative entrepreneurship and continuous tinkering finally paid off. We may not yet have arrived at privacy nirvana, but we surely seem to have a comprehensive regulatory framework in place.;Paradigm shift;Mayer-Schönberger, V.;2020;Mayer-Schönberger, V.;Information Governance and Security
"Big Data promises huge benefits for medical research. Looking beyond superficial increases in the amount of data collected, we identify three key areas where Big Data differs from conventional analyses of data samples: (i) data are captured more comprehensively relative to the phenomenon under study; this reduces some bias but surfaces important trade‐offs, such as between data quantity and data quality; (ii) data are often analysed using machine learning tools, such as neural networks rather than conventional statistical methods resulting in systems that over time capture insights implicit in data, but remain black boxes, rarely revealing causal connections; and (iii) the purpose of the analyses of data is no longer simply answering existing questions, but hinting at novel ones and generating promising new hypotheses. As a consequence, when performed right, Big Data analyses can accelerate research. Because Big Data approaches differ so fundamentally from small data ones, research structures, processes and mindsets need to adjust. The latent value of data is being reaped through repeated reuse of data, which runs counter to existing practices not only regarding data privacy, but data management more generally. Consequently, we suggest a number of adjustments such as boards reviewing responsible data use, and incentives to facilitate comprehensive data sharing. As data's role changes to a resource of insight, we also need to acknowledge the importance of collecting and making data available as a crucial part of our research endeavours, and reassess our formal processes from career advancement to treatment approval.";Big Data and medicine: a big deal?;Mayer-Schönberger, V. & Ingelsson, E.;2017;Mayer-Schönberger, V.;Information Governance and Security
The GDPR will bring substantial changes to the legal framework of information privacy in the European Union. In this article we take a look at the GDPR from the vantage point of Big Data analysis: will it facilitate or hinder Big Data in Europe? While we find that data collection is further restricted, we point at a somewhat surprising change that may enable data reuse in the context of Big Data without reconsent, as well as extended data retention possibilities. We also point out important conceptual shortcomings of the GDPR with respect to Big Data, and offer suggestions for further information privacy development in Europe.;Regime Change? Enabling Big Data Through Europe's New Data Protection Regulation;Mayer-Schönberger, V.;2016;Mayer-Schönberger, V.;Information Governance and Security
"Big Data promises to change cardiology through a massive increase in the data gathered and analysed; but its impact goes beyond improving incrementally existing methods. The potential of comprehensive data sets for scientific discovery is examined, and its impact on the scientific method generally and cardiology in particular is posited, together with likely consequences for research and practice. Big Data in cardiology changes how new insights are being discovered. For it to flourish, significant modifications in the methods, structures, and institutions of the profession are necessary.";Big Data for cardiology: novel discovery?;Mayer-Schönberger, V.;2015;Mayer-Schönberger, V.;Information Governance and Security
Big data denotes our capacity to gain insights from (in relative terms!) large amounts of data that we could not have had by just looking at samples. Our difficulty in working with data has shaped our methods in the small data age. As these limitations with respect to data diminish, we will have to rethink and adjust our scientific methods. In return, we will gain a wealth of new insights, perhaps leading towards a new golden era of scientific discovery. Big Data power demands, however, that we also are cognizant of its limitations and the significant dangers of abusing it.;Big Data – Eine Revolution, die unser Leben verändern wird;Mayer-Schönberger, V.;2015;Mayer-Schönberger, V.;Information Governance and Security
Troves of our personal data are being collected and analyzed every day by players who have the power to influence what we see online and how we are seen in real life. The methods by which this information is collected and analyzed are shockingly opaque, and attempts to protect our privacy are no longer effective (if they ever were). Viktor Mayer-Schönberger advances the ongoing debate over Internet privacy in a review of The Black Box Society: The Secret Algorithms that Control Money and Information.;Connecting the dots;Mayer-Schönberger, V.;2015;Mayer-Schönberger, V.;Information Governance and Security
This paper presents the findings of the Gone Dark Project, a joint study between the Institute of Social and Cultural Anthropology and the Oxford Internet Institute at Oxford University. The project has sought to give substance to frequent reports of Web sites “disappearing” (URLs that generate “404 not found” errors) by tracking and investigating cases of excellent and important Web sites which are no longer accessible online. We first address the rationale and research methods for the project before focusing on several key case studies illustrating some important challenges in Web preservation. Followed by a brief overview of the strengths and weaknesses of current Web archiving practice, the lessons learned from these case studies will inform practical recommendations that might be considered in order to improve the preservation of online content within and beyond existing approaches to Web preservation and archiving.;Learning from failure: The case of the disappearing Web site;Barone, F., Zeitlyn, D. & Mayer-Schönberger, V.;2015;Mayer-Schönberger, V.;Information Governance and Security
This study investigates the monetary value of virtual goods in the context of 24 most popular massively multiplayer online role-playing games (MMORPGs). Building on classic economic theory, we approach this issue through a combination of experimentation and cross-sectional time series data analysis. Our findings suggest that more intensive social networking and flatter social hierarchical structures are associated with lower monetary value of virtual goods across various MMORPGs. Instead, a larger base of active users increases the potential demand and thus the monetary value of virtual goods in the short run. A steeper social hierarchical structure further strengthens the effect. The implication is that social networking and hierarchical structure can be two effective angles for game developers or policy makers to address the issue of real-money trading of virtual goods.;The determinants of monetary value of virtual goods: An empirical study for a cross-section of MMORPGs;Wang, Q-H., Mayer-Schönberger, V. & Yang, X.;2013;Mayer-Schönberger, V.;Information Governance and Security
Nowadays individuals are often presented with long and complex privacy notices routinely written by lawyers for lawyers, and are then requested to either ‘consent’ or abandon the use of the desired service. The over-use of notice and consent presents increasing challenges in an age of ‘Big Data’. These phenomena are receiving attention particularly in the context of the current review of the OECD Privacy Guidelines. In 2012 Microsoft sponsored an initiative designed to engage leading regulators, industry executives, public interest advocates, and academic experts in frank discussions about the role of individual control and notice and consent in data protection today, and alternative models for providing better protection for both information privacy and valuable data flows in the emerging world of Big Data and cloud computing.;Notice and consent in a world of Big Data;Cate, F. & Mayer-Schönberger, V.;2013;Mayer-Schönberger, V.;Information Governance and Security
The emerging concept of “big data” concerns the large amount of information that is being processed, analyzed, and put to extraordinary new uses. Big data is already being used by Google and New York City authorities, among others, and is likely to be used more and more in the future.;The Rise of Big Data: How It's Changing the Way We Think About the World;Cukier, K. & Mayer-Schönberger, V.;2013;Mayer-Schönberger, V.;Information Governance and Security
Through Harry Potter, a series of books about a fictional young boy wizard, J. K. Rowling introduced a generation of children to a literary world of wizardry and witchcraft. Weaving complex plots about Harry Potter and his friends as they faced the evil Lord Voldemort, Rowling’s series has generated billions of dollars and has become a franchise that encompasses successful filmic incarnations, a themed amusement park, and countless varieties of merchandise. However, in 2007 and 2008, Rowling received widespread attention for something else: suing one of her most devoted fans.;Fan or Foe? Fan Fiction, Authorship, and the Fight for Control;Mayer-Schönberger, V. & Wong, L.;2013;Mayer-Schönberger, V.;Information Governance and Security
The popularity of virtual worlds is growing. Millions of people around the globe interact in these worlds every week, transact with others, and even make their living trading virtual goods. As virtual worlds turn into a mainstream mass phenomenon, lawmakers and courts are beginning to turn their focus on them. Increasingly, conflicts arise in these worlds among their participants that they desire to have settled in the virtual world, by the virtual world provider, or by real-world courts. At other times, those participating in virtual worlds so thoroughly disagree with the commercial entities that provide and manage a virtual world as to take action, individually or collectively, to change the rules that govern it. Finally, alerted by media coverage, real-world policymakers—legislators and regulatory agencies—ponder and even advocate new rules to curb behavior in virtual worlds.;Virtual Heisenberg: The Limits of Virtual World Regulability ;Mayer-Schönberger, V.;2009;Mayer-Schönberger, V.;Information Governance and Security
As we weather the deepest recession in recent times, lawmakers everywhere search for mechanisms to revive the economy. This paper argues that in addition to financial stimuli, the law, too, has substantial, yet underutilized capacity to foster economic growth. In particular I examine the legal system's potential to facilitate innovative entrepreneurship in difficult economic times. In Part II of the paper I suggest three distinct roles leveling, protecting, and enabling that law can play to foster entrepreneurship. Part III develops a comprehensive framework for crafting laws that facilitate entrepreneurship based on the theory of risk. Utilizing expected utility theory I explain why lawmakers may want to focus less on direct financial losses or gains for entrepreneurs (like subsidies or tax breaks), and more on information by improving the predictability of legal processes. Insights from behavioral economics take this one step further by suggesting lawmakers need to be careful how they frame laws intended to facilitate entrepreneurship. Such a risk-based framework rests on two important assumptions: that the linearity of the innovation process and the central importance of the individual entrepreneur rarely happen in practice. Thus, *Director and Associate Professor of Public Policy, The LKY School of Public Policy, National University of Singapore, and faculty affiliate BCSJA, Harvard University. Earlier versions of this paper have been presented at symposia at Stanford University, the Searle Center on Law, Regulation, and Economic Growth, Northwestern University School of Law, and the Hudson Institute. I am grateful for the feedback from colleagues at these events, as well as the research help of Malte Ziewitz, Ben Roberts, and Shakhi Majumndar. 154 I/S: A JOURNAL OF LAW AND POLICY [Vol. 6:2 Part IV of the paper shows how through a more nuanced understanding of innovation, law may take on a significantly more active role: not be conceptualized as static and exogenous, but potentially entrepreneurial in nature, thereby actively creating market tensions that entrepreneurs then successfully exploit. More research is needed to better understand such an active role of the law, but it could offer lawmakers a much more powerful tool at their disposal to shape entrepreneurial activity in our nation than has been thought so far.;The Law as Stimulus: The Role of Law in Fostering Innovative Entrepreneurship;Mayer-Schönberger, V.;2010;Mayer-Schönberger, V.;Information Governance and Security
For decades, we have refined concepts of information privacy, as well as intellectual property, that are largely based on individual rights. Such an approach is undeniably appealing. It does not necessitate a large enforcement bureaucracy, ostensibly enhances human freedom and self-determination, and ensures efficient information allocation through robust markets. As this article explains, a rights-based approach may even lead us to a convergent and coherent concept of information governance on either side of the Atlantic. Such a convergent conception would, however, not be able to extend to both the United States and Europe. For that it may behoove us to take a serious look at the bidirectional information rights structures emerging in Europe. The problem with such rights based approaches is that they have largely failed in practice. In contrast, information privacy protection works when it rests on a rich and deep network of information governance intermediaries. This article concludes by suggesting that studying the system of information privacy and copyright in particular, and of information governance in general, and examining what mechanisms of governance are employed by the various intermediaries may yield a richer, more accurate, and more effective strategy for information governance than the current rights-based approach.;"Beyond Privacy, Beyond Rights-Toward a ""Systems"" Theory of Information Governance";Mayer-Schönberger, V.;2010;Mayer-Schönberger, V.;Information Governance and Security
There is hardly a more important case regarding policies and politics of individual identities than the adoption of the British Identity Cards Act, which in time will lead to both a compulsory national identity card for well over 50 million British citizens and the incorporation of their personal information into a centralized, digital national register. It reflects a dramatic reversal for a nation that is often portrayed as the cradle of individual liberty and freedom, and which was among the first to subject the absolute power of the Monarch to limitations. It is a remarkable case because of the size of the logistical challenge (and likely cost), and because of the high hopes the government has pinned on the cards’ and register’s existence. Equally thought provoking (to put it mildly) is the decision by the British government, contrary to a global trend, to choose a centralized identity infrastructure. But grasping the meaning of British Identity Cards Act only in terms of a public management issue or a technical infrastructure challenge would fail to do it justice. The case of the British Identity Cards Act is much richer, as well as more important. It is a perfect example to reflect upon not only how liberal democracies ought to approach the question of individual identity in our times, but — on a meta-level — how liberal democracies ought to conduct the public deliberations about such an important matter. “Global Challenges for Identity Policies” offers a very rich, in-depth study of this most important case. This in itself is more than sufficient reason to applaud the authors for their efforts, and to read their work. But the authors are not your average commentator on the politics and policies of identity and privacy (like this humble reviewer). Edgar A. Whitley and Ian Hosein are the two LSE academics responsible for much of the substance that fueled the public opposition against the Identity Cards Act. Their LSE Identity Project issued a number of reports and comments during the prolonged phase of the legislative process that unraveled most of the shiny but unfounded government rhetoric. They were the targets of government spin, much of which it turned out unfair and inaccurate, meant to undermine their authority as objective academics. This is potentially both a blessing and a curse. A blessing because they know more about the issues as well as how the debate unfolded inside parliament and in the public discourse than pretty much anybody else, which gives them all the ingredients for a tell-all gripping tale. A curse because as they have taken sides in the debate, they may have become susceptible to bias. The result would still be most useful and impressive: telling their side of the story. True to their academic calling, in their book Whitley and Hosein chose a different path — more honorable, as well as frustrating. The book tells how the Identity Cards Act became law despite its substantive and conceptual flaws, dubious cost and questionable outcome, powerful parliamentary opposition — vocal but not sufficient in numbers in the House of Commons, but quite overwhelming in the House of Lords —, sustained public debate, and repeated government public relations blunders. Make no mistake: theirs is a gripping, and sobering tale. As experts of information systems they do a great job in framing the identity issue. They detail the various phases of the legislative and public debates, and tease out the implications of the various legislative proposals, objections and compromises. As they describe the dynamics between the House of Commons and the House of Lords, the reader begins to grasp how fundamental and important much beyond identity policies the Identity Cards Act case is, and why the authors place the emphasis of their analysis correctly on the process of deliberation, rather than the outcome. I found fascinating the similarly more general discussion of the role of academic inputs into the policy-making process. Their tale is also somewhat frustrating, because despite having been in the first row witnessing the unfolding they painstakingly distance themselves from the events, retelling them with the dispassionate zeal of a forensic scientist. Some of the theoretical superstructure they bring to bear (and that is my only real criticism) is more distracting (especially the dense literature reviews), too limited in size and scope (a full Actor Network Theory analysis of the Identity Cards Act would itself require a book or two), and sometimes feels not sufficiently connected to the main narrative (especially chapter 6, which as the authors acknowledge was published separately before — it still feels that way). This is not to say that Science and Technology Studies analysis would not have to offer intriguing insights into the dynamics of the debate and the unfolding of events. In fact, the authors repeatedly point out such insights (although I would have wanted to read more about these) and I found them very powerful. I wonder, though, whether this analysis could perhaps have been made more powerful by dedicating a separate volume to it, or combining it in a concise chapter or two. Due to my background in public policy analysis and political science I wondered whether theories from my fields could not help explain some of the dynamics of the Identity Cards Act. But this is little more than nitpicking. The bottom line is much simpler: This is a very valuable work about arguably the most significant case of national identity policies and politics of our time, and the authors with their extensive knowledge not just of the subject matter, but of how the process unfolded have done a truly admirable job in presenting this impressively objective and mightily important study.;Edgar A. Whitley & Ian Hosein: Global challenges for identity policies;Mayer-Schönberger, V.;2009;Mayer-Schönberger, V.;Information Governance and Security
Recently, researchers who support “network neutrality” have become worried that the Internet may lose its innovative edge. They are concerned that control could be shifting from the edges of the Internet toward the service providers at the center, which would allow the providers to have “gatekeeper” capacity and would contradict the Internet's “end-to-end” principle (1–3). This core tenet states that control over information flows should take place, to the extent possible, at the end points of the network (4). President Obama supported net neutrality during his campaign (4) and in recent statements (5), and the European Parliament has added net neutrality to its recent telecom bill (6). Taking the end-to-end principle from protocols to users, Jonathan Zittrain has called for maintaining the Internet's “generativity,” the ability of users at the network's end points to create, distribute, and run whatever software code they choose (7). There are good reasons to preserve network neutrality and generativity, but it is unclear whether these are sufficient to ensure continued innovation. The larger issue is what policies are optimal to foster innovation on the Internet?;Can We Reinvent the Internet?;Mayer-Schönberger, V.;2009;Mayer-Schönberger, V.;Information Governance and Security
"Lawrence Lessig shaped the nascent field of cyberlaw. In particular his argument that ""code is law"" has become a central tenant of the writings in the field. This Article offers a fundamental critique of Lessig's core argument—and thus of core assumptions of cyberlaw scholarship. It first focuses on the role Lessig ascribes to the market and how he sees it functioning. By emphasizing market choices, Lessig conceptualizes societal problems through a particular lens of atomistic decisions, of outcome rather than process, thereby failing to capture the full dynamic at play in free speech, intellectual property and privacy cases on the Internet. Second, for Lessig, markets function because of assumed or regulated information symmetry (that may not exist in most market transactions) causing him to overvalue transparency. A third fundamental weakness of Lessig's theory is the relationship between technology and society. For Lessig, markets drive technology, which in turn shapes society. This linear, directional view has been discredited by much of the research in science and technology studies over the last four decades. Using two examples of the path of a particular technology (one from Lessig and one more recent)—cookies and podcasts— I show how Lessig's technological determinism fails to capture the complex dynamics of innovation.";Demystifying Lessig;Mayer-Schönberger, V.;2008;Mayer-Schönberger, V.;Information Governance and Security
"Traditional economic theory rejects the notion that government can offer significant contributions to entrepreneurial development, beyond the structural and legal roles of protecting property and the rule of law. Ignorant or disbelieving of this fact, many national and local governments have sought to jump-start entrepreneurial activity on a large scale. Given this persistence, it is a valuable effort to determine what, if anything, government can do to encourage entrepreneurs. This paper analyzes the traditional impact of government on entrepreneurship, and goes further to consider whether government can shape the risk-based environment in which entrepreneurs exist in order to increase entrepreneurship. When such analysis fails to yield a useful role for policy-makers, Mayer-Schoenberger questions the uni-directional nature of entrepreneurship, and offers examples of ways in which government regulation created opportunities for entrepreneurship. From these examples, he draws out a framework for ""entrepreneurial policy-making"", in which policy-makers experiment with regulation to encourage entrepreneurship.";Schumpeterian Policy Makers: Pro-Active Policies for Innovative Entrepreneurship;Mayer-Schönberger, V.;2007;Mayer-Schönberger, V.;Information Governance and Security
As humans we have the capacity to remember - and to forget. For millennia remembering was hard, and forgetting easy. By default, we would forget. Digital technology has inverted this. Today, with affordable storage, effortless retrieval and global access remembering has become the default, for us individually and for society as a whole. We store our digital photos irrespective of whether they are good or not - because even choosing which to throw away is too time-consuming, and keep different versions of the documents we work on, just in case we ever need to go back to an earlier one. Google saves every search query, and millions of video surveillance cameras retain our movements. In this article I analyze this shift and link it to technological innovation and information economics. Then I suggest why we may want to worry about the shift, and call for what I term data ecology. In contrast to others I do not call for comprehensive new laws or constitutional adjudication. Instead I propose a simple rule that reinstates the default of forgetting our societies have experienced for millennia, and I show how a combination of law and technology can achieve this shift.;Useful Void: The Art of Forgetting in the Age of Ubiquitous Computing;Mayer-Schönberger, V.;2007;Mayer-Schönberger, V.;Information Governance and Security
Innovative entrepreneurship is seen as a central driver of economic growth. Lawmakers around the world have attempted to use law to foster such entrepreneurship. Yet, frequently law is described as the enemy of entrepreneurs. This paper argues that this is a fundamental misconception. In part I of the paper I suggest three distinct roles - leveling, protecting, and enabling - that law can play to foster entrepreneurship. Part II develops a comprehensive framework for crafting laws that facilitate entrepreneurship based on risk theory. Utilizing expected utility theory I propose that lawmakers may want to focus less on direct financial losses or gains for entrepreneurs (like subsidies or tax breaks), and more on the predictability of legal processes. Behavioral economics suggests that lawmakers need to be careful how they frame laws intended to facilitate entrepreneurship. A risk-based framework for entrepreneurial law rests on an important assumption: on the linearity of the innovation process and the central importance of the individual entrepreneur. Part III of the paper shows how a more nuanced understanding of innovation has fundamental repercussions for the role we assign law. I suggest (and demonstrate through cases) that the most appropriate role for law may not be reactive (however well thought out), but entrepreneurial - actively creating market tensions that entrepreneurs then successfully exploit. I conclude that lawmakers have a much more central and important role in shaping entrepreneurial activity in our nation than has traditionally been ascribed to them.;Entrepreneurial Law;Mayer-Schönberger, V.;2007;Mayer-Schönberger, V.;Information Governance and Security
The demise of the Keynesian National Welfare State and its transformation into a more competitive and interactive unit of governance has given rise to an increased interest in the processes that are shaping the legal framework for markets. For several decades, one force has been taken to be tantamount to the law of nature governing the interaction between jurisdictions, namely, the force of regulatory competition. However, this model is open to severe criticism of its emphasis on efficiency. First, elected decision‐makers may not be interested in efficiency gains regardless of where the resulting distributive consequences may fall. Second, we suggest the theory of regulatory competition has a federalist bias that potentially blinds it to institutional alternatives. The model also rests on unexamined normative premises. Research has shown that competition is only one mode of regulatory behaviour. Cooperation and information flows play important roles in shaping regulatory activity as well. We contend that a more satisfactory model of regulatory interaction needs to take into account a variety of agents, standards, and systems. In devising such an alternative model, a satisfactory theory would have to understand the multiplicity of relevant agents beyond the narrow confines of the traditional nation‐centred federal model. Standards guarding regulatory interaction would—not dissimilar to competition law—have to state its own limitations.;Governing Regulatory Interaction: the Normative Question;Mayer-Schönberger, V. & Somek, A.;2006;Mayer-Schönberger, V.;Information Governance and Security
This paper examines the existing statutory frameworks in the US limiting government use of individual fingerprint, DMV, and tax data, drawing lessons for the existing statutory limitations on the use of government‐controlled offender DNA databanks.;Statutory Frameworks for Regulating Information Flows: Drawing Lessons for the DNA Data Banks from other Government Data Systems;Lazer, D., & Mayer-Schönberger, V.;2006;Mayer-Schönberger, V.;Information Governance and Security
Over the last several years, many have called for an internationalization of Internet governance in general, and Internet naming and numbering in particular. The multi-year WSIS process that culminated in November 2005 was intended to create momentum in such direction. The United States has long resisted such internationalization, fearing in particular the growing influence of China and similar nations. In September 2005 the European Union put forward a proposal which would have offered a constitutional moment for Internet governance by suggesting internationalization based on fundamental values of the Internet community. The swift rejection of the proposal by the US was surprising, both from a tactical as well as – in light of its own constitutional history – a substantive viewpoint. In this article we describe the main features of the European proposal and what it might have created. We evaluate four possible arguments explaining US rejection: delegation of power, objective rights, public choice, and de-legitimization of international regimes.;Jefferson Rebuffed--The United States and the Future of Internet Governance;Mayer-Schönberger, V.;2006;Mayer-Schönberger, V.;Information Governance and Security
This article examines an understudied area of telecommunication policy, namely the regulation of public safety radio communication to bring about communication interoperability. Commonality and funding are the two intertwined policy challenges need to be overcome to achieve this interoperability. Addressing these challenges separately, US federal policy has struggled to establish communication interoperability. By contrast, the integrated approach of the Europeans has resulted in a relatively successful outcome. It is argued that this integrated approach may even permit the transplantation of a more general telecommunication policy thread—competition—to the public safety radio communication field. However, whether interoperability is established may also be linked to intra-agency fears of a potential re-direction of hierarchical communication flows brought about by new (and configurable) communication infrastructure. If this is the case, policy-makers regulating for interoperability will need to take into account inter- and intra-agency organizational dynamics as well.;The politics of public safety communication interoperability regulation;Mayer-Schönberger, V.;2005;Mayer-Schönberger, V.;Information Governance and Security
"Protecting infrastructure from calamity has always been important for industry, government and society. Yet with more activities dependent on computer networks - from banking and aviation to emergency services - the reliability and security of information and communication systems against disasters, both natural and man-made, are in doubt. The question of protection is difficult because the majority of critical information infrastructure is privately-owned, interlinked with other firms, and crosses international borders. Evidence suggests there are currently insufficient incentives for protection to be adequately implemented. Companies internalize the costs and hope for the best; governments are loath to regulate lest they do it badly. Indeed, without really knowing the risk profile, it is not even clear what constitutes adequate protection in the first place. And, as always, it poses the question: who should pay? To understand the obstacles for protecting critical information infrastructure and to consider solutions, over 25 experts from industry, government and academia met for the fifth annual Conference on Information Law and Policy for the Information Economy, organized by Professors Lewis M. Branscomb and Viktor Mayer-Schonberger of Harvard University's John F. Kennedy School of Government, with the support of Swiss Re, from June 16-18, 2005 at the Swiss Re Center for Global Dialogue in Rueschlikon, Switzerland. The report that is meant not only as an analytical summary of the discussion, but also as a roadmap for future work. It is comprised of five sections. The first explains the problems of protecting critical information infrastructure, and the second section considers the economics of it. The third section examines different models of network security, and the fourth identifies roles for business, government and the insurance industry. The fifth section takes a practical turn, and proposes a series of next steps that the private and public sectors can take. The report concludes that global economic development may be the force that best addresses the problem. As society increasingly depends on critical information infrastructure, it is important for new forms of partnerships to develop, involving numerous stakeholders. As a first step, information-sharing requires a permissible legal framework, regarding both antitrust and liability concerns. Moreover, the introduction of insurance could provide a foundation for market-based risk analysis, and cooperation among infrastructure operators. The participants of the Rueschlikon conference were largely optimistic that provided market forces could be brought to bear on the issue of critical information infrastructure protection, many of today's challenges could be alleviated.";Ensuring (and Insuring?) Critical Information Infrastructure Protection;Cukier, K., Mayer-Schönberger, V. & Branscomb, L.;2005;Mayer-Schönberger, V.;Information Governance and Security
More than a decade ago John Perry Barlow envisioned a cyberspace free from real-world regulation. His vision was flawed. But virtual worlds, in which millions of users around the world spend significant amounts of their time (and money) interacting and transacting with each other, may prove Barlow right after all. In this paper, we look at the universe of these virtual worlds and how virtual world providers compete with each other, trace the likely development of regulatory interdependence and suggest how real-world lawmakers may want to facilitate virtual world self-governance. As virtual world providers transform themselves from offering content to offering a virtual space in which users can settle with their intellectual property, they begin to compete over the regulatory frameworks they offer their users. Users free to take their property and move to a different virtual world at relatively low cost, unleash intriguing regulatory dynamics between the virtual worlds. Will they engage in touch regulatory competition? Will pockets of cooperation develop and if so why and where? Or will virtual world providers much like Tiebout suggested differentiate based on user preferences, and will the virtual world universe hence reach a stable equilibrium? A similar dynamic may ensue among real-world lawmakers attempting to regulate virtual worlds. Virtual world providers may relocate to more welcoming jurisdictions, taking revenue streams with them. Coordination among real-world regulators may offer reprieve from a potential regulatory race to the bottom, but only temporarily. The more real-world lawmakers are tempted to reign in virtual worlds, the likelier that virtual worlds will become decentralized like peer-to-peer networks, leaving real-world jurisdictions without an easily identifiable entity to regulate. To avoid the birth of such a Barlowian virtual space, we suggest real-world lawmakers are better off facilitating the inculcation of real-world governance values into the nascent virtual worlds of self-governance.;Napster's Second Life?--The Regulatory Challenges of Virtual Worlds;Mayer-Schönberger, V. & Crowley, J.;2005;Mayer-Schönberger, V.;Information Governance and Security
This article suggests that metaphors are essential to understanding leadership. Metaphors can serve as underlying organizing structures of leadership thinking and experience, and they can be mobilized in order to accomplish interpersonal goals. The literature on leadership abounds with metaphors such as leadership as game, sports, art or machine. While the multitude of leadership metaphors used by authors and leaders alike appears determined by a complex interplay of personal, situational, and cultural factors, the analysis of a leadership interview indicates that these metaphors center around experientially significant nuclei of meaning. By examining the entailments of leadership metaphors on such dimensions as highlighted and hidden leadership aspects or the suggested relationship between leader and followers, metaphor analysis allows the exploration of leadership conceptualizations on an experiential level. An exploratory grid presents possible entailments of selected metaphors on important dimensions of leadership. We propose that the study of leadership metaphors can provide valuable lessons to leaders. For example, effective leadership may require a rich and situationally attuned metaphorical vocabulary. Because leadership metaphors carry implicit suggestions about values (e.g., what is good, what should be done, and how), they may also allow for new insights into the ethics of leadership.;Through Their Own Words: Towards a New Understanding of Leadership Through Metaphors;Mayer-Schönberger, V. & Oberlechner, T.;2002;Mayer-Schönberger, V.;Information Governance and Security
When on September 11, 2001, the Pentagon stood ablaze, responding fire companies from Maryland could not communicate with those from Washington, D.C. and Northern Virginia. Runners had to be used instead, stalling rescue efforts: a powerful reminder that even in the age of digital networks and ubiquitous cell phones, communication interoperability, the ability of public safety personnel to communicate by radio with staff from other agencies, on demand and in real time, remains an elusive goal. As almost 60,000 federal, state and local public safety agencies plan to upgrade their communications systems in the wake of 9/11, this essay takes a hard look at communications interoperability and its implementation, here in the United States and in Europe. Three steps have been seen as requirements for interoperability: inventing the appropriate technology, setting common standards and frequencies, and providing adequate funding. This essay looks at each of these steps in the U.S. and European contexts and analyzes successes and failures, rendering a fuller picture both of the challenges for interoperability and of best practices to meet them. Over the last few years (and surprisingly given the complex political structures) the Europeans have pulled ahead of the U.S. in implementing interoperability, although with determination and the right set of strategies, U.S. policymakers can easily make up lost ground. Enhanced Federal Communications Commission (FCC) leadership in defining frequencies and standards and a clearly formulated and thoroughly executed comprehensive funding strategy, based either on public funds or innovative public-private partnerships, would go a long way toward enabling communications interoperability to take hold. Yet, this essay is not simply about how to overcome obstacles on the path to interoperability. The case of interoperability, its elusiveness in the United States and its successes elsewhere, reveals a deeper, more troubling story - a story not so much of technical hurdles, as of structural and political hurdles, as more of perceived than actual constraints, unduly limiting the nation¹s ability to cope with an important public policy need in these uncertain times. There are no abstract silver bullets to overcome the problem. Instead, policymakers have to look carefully at how well the policy strategy they select is aligned with their means and the policy context. In the United States, interoperability has suffered from strategic misalignment and haphazard implementation. European interoperability policies have fared better, not because of a general advantage in the strategies chosen, but because of a better fit between means and ends. Thus interoperability also provides an intriguing test case, highlighting the transcending importance of strategic alignment, agency innovation, and leadership.;Emergency Communications: The Quest for Interoperability in the United States and Europe;Mayer-Schönberger, V.;2002;Mayer-Schönberger, V.;Information Governance and Security
Modern information and communication technologies are reshaping the environment in which foreign policy and international relations are conducted. According to conventional wisdom the information revolution has increased the influence of non-state actors and undermined the authority of the nation-state. While there is mounting anecdotal evidence supporting this trend, the nature and scope of the transmission belt between technological change and political outcomes remain largely unexplored. This paper provides a framework for assessing the impact of the information revolution on power structures in international affairs. The ability to control information access is increasingly becoming a source of power. Shifting information between different access categories can be a deliberate policy tool, or the unintentional consequence of changes in the control of information infrastructures. The latter phenomenon has, in recent years, often taken the shape of a “denationalization” of state-controlled information infrastructures, facilitating shifts in information power away from state actors.;Information Power: International Affairs in the Cyber Age;Mayer-Schönberger, V. & Brodnig, G.;2001;Mayer-Schönberger, V.;Information Governance and Security
In this chapter we bring together the work on digital divides and social justice with the field of Technology Enhanced Learning. We provide an overview of four key papers that help to conceptualise the complex set of relationships that explain the different ways that people use (or do not use) technology and the likely outcomes this has in terms of social and educational opportunities. We then move on to explore the ways that policy makers and practitioners in TEL may help to address these differences, both within formal and informal contexts of learning, and highlight why many well intended initiatives fail. We argue that a more nuanced understanding of the complex nature of the digital divide, alongside a multi-faced research approach is required, and highlight two potential fruitful areas for further work for TEL researchers—recognising learning outside the mainstream education system and education for citizenship and political engagement. Ultimately, whatever the focus taken, we suggest that TEL researchers need to bring together efforts to tackle digital inequalities and digital approaches to tackling social and educational inequalities within a broader programme of policy and practice that commits to tackling inequalities at every level of society.;Digital Divides and Social Justice in Technology-Enhanced Learning;Grant, L. & Eynon, R.;2017;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
The use of technology offers researchers new ways of carrying out explorations in learning and education, yet alongside these opportunities come some new ethical challenges. The use of technology in education research does not mean that an entirely new ethical code of practice is required, but there does need to be consideration and discussion about the potential ethical consequences of particularly novel or innovative techniques.;The Ethics of Learning and Technology Research;Eynon, R., Schroeder, R. & Fry, J.;2016;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
Massive open online courses (MOOCs) offer unprecedented opportunities to learn at scale. For those engaged in learning analytics and educational data mining, MOOCs have provided an exciting opportunity to develop innovative methodologies that harness big data in education. This chapter argues that in order to capture learning in mass‐scale crowd‐based environments, a mixed method approach is required, which combines data mining with a wide set of social science techniques that are primarily qualitative in nature. These include observation, interviews, and surveys, more traditionally used in education research. The chapter discusses a constructive way of addressing the issue of mixed method research by adopting a pragmatic paradigm, where the primary attention is given to the research question asked, as opposed to holding a particular allegiance to a philosophy or methodology when carrying out the research. As the hype around MOOCs begins to fall away, research opportunities remain very rich both for online education and beyond.;UNDERSTANDING COMMUNICATION PATTERNS IN MOOCs;Eynon, R., Hjorth, I. & Yasseri, T. et al.;2016;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality; Digital Knowledge and Culture"
The use of the Companion could have broad learning benefits. For example, cenabling learners to have control over their projects of learning could potentially lead to improved level of self efficacy beliefs (Kim and Balyor, 2006), cimproved IT skills or a general confidence to try new things. We believe that this broader view of the benefits of such a Companion makes the positive cimplications outweigh the negative.;A Companion for learning in everyday life;Eynon, R. & Davies, C.;2010;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
The variety and availability of networked devices such as tablets, mobile phones, and laptops, and the proliferation of online environments such as social networking sites, and computer gaming worlds are reconfiguring learning and education. As Haythornthwaite suggests, new technologies are ‘rewriting communication networks, creating new spaces and relationships, restructuring knowledge networks, challenging identities, and changing the location, evaluation, and accessibility of information resources and people’ (Haythornthwaite, 2015, p. 292). Given this context, it is important to conceptualise and research the digital aspects of learning and education.;Researching in Digital Environments;Eynon, R.;2017;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
To fully understand and support learners in MOOCs we need a way of detecting and differentiating distinct learner profiles (reflecting motivations, goals, preferences for course elements and online behaviours) - particularly given the “crowd like” nature of these settings. This paper highlights some steps towards this goal. Using a range of data sources from one case study MOOC on business strategy we address three questions: 1) can we distinguish coherent profiles of learners’ interactions within a MOOC? 2) how do these interaction profiles relate to learner characteristics? And 3) what is the relationship between these interaction profiles and learners’ performance and experiences of learning within a MOOC?;‘Vote Me Up If You Like My Ideas!’ Experiences of Learning in a MOOC;Eynon, R., Hjorth, I. & Gillani, N. et al.;2014;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality; Digital Knowledge and Culture"
"In the UK, the Internet has become an important feature of the lives of the majority of young people for all aspects of their lives. However, there is a significant minority of young people who are not able to navigate or connect properly with the online world. They are, in other words, outside the digital mainstream. Evidence for this group has been found in nationally representative surveys, where around 10% of young people (aged 17–23) define themselves as lapsed Internet users. That is, they used to use the Internet but no longer do so (OxIS, 2011). This study aims to find out more about this group. Specifically we aim to: Examine why young people are outside the digital mainstream, and determine the extent to which this is due to reasons of exclusion or choice. Explore the implications this has in their daily lives. Consider how the experiences of these young people can inform the digital inclusion strategy in the UK. This nine month qualitative study investigated these objectives in four overlapping steps: a literature review of academic research and policy documents; analysis of the Oxford Internet Survey (2011) and the Learner and their Context Survey (2009), which contain valuable information on lapsed Internet users; 36 in–depth interviews with young people who consider themselves to be infrequent or lapsed Internet users; and a workshop with key experts in the field.";On the Periphery? Understanding Low and Discontinued Internet Use Amongst Young People in Britain ;Eynon, R. & Geniets, A.;2012;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
"Artificial intelligence (AI) is again attracting significant attention across all areas of social life. One important sphere of focus is education; many policy makers across the globe view lifelong learning as an essential means to prepare society for an “AI future” and look to AI as a way to “deliver” learning opportunities to meet these needs. AI is a complex social, cultural, and material artifact that is understood and constructed by different stakeholders in varied ways, and these differences have significant social and educational implications that need to be explored. Through analysis of thirty-four in-depth interviews with stakeholders from academia, commerce, and policy, alongside document analysis, we draw on the social construction of technology (SCOT) to illuminate the diverse understandings, perceptions of, and practices around AI. We find three different technological frames emerging from the three social groups and argue that commercial sector practices wield most power. We propose that greater awareness of the differing technical frames, more interactions among a wider set of relevant social groups, and a stronger focus on the kinds of educational outcomes society seeks are needed in order to design AI for learning in ways that facilitate a democratic education for all.";Methodology, Legend, and Rhetoric: The Constructions of AI by Academia, Industry, and Policy Groups for Lifelong Learning;Eynon, R. & Young, E.;2020;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
The development of digital skills for all is a key focus of many educational policies across the globe. Despite the significant attention paid to the nature and suitability of such policies targeted at young people, there has been far less focus on digital skills policies targeted at adults. This article contributes to this literature. It outlines current digital skills policy in England. Having established this background, it analyses 30 interviews with digitally competent adults from lower socio‐economic backgrounds about their experiences of learning to use the Internet. In doing so, the article highlights that a narrow and instrumental digital skills agenda is emerging in the education of adults, driven by the needs of the commercial sector, that is in stark contrast to the experiences, motivations and hopes of adults who learn about, and use, digital technologies. Reframing digital skills as part of a broader adult education agenda may offer a way to facilitate the development of digital literacies that individuals seek.;Becoming digitally literate: Reinstating an educational lens to digital skills policies for adults;Eynon, R.;2020;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
"This paper uses nationally representative survey data of adults Internet use in Britain to examine current patterns in the uptake of lifelong learning via the Internet. We develop and test a model that accounts for structure, agency and outcomes using structural equation modelling to address two questions: (1) how structure (as measured by age, gender, SES, Education and ACORN) is related to personal and capital enhancing outcomes of learning online; and (2) how agency (as measured by digital skills and engagement with online learning) mediates this relationship. We demonstrate that social structure remains an important factor in understanding patterns of uptake and outcomes of online learning, alongside an individual’s agentic behaviours. We suggest that countries such as the UK, which have become overly focused on individual interventions to increase the uptake of lifelong learning via the Internet, are going in the wrong direction. Such interventions have failed in the past, and we suggest that they will continue to do so unless policy makers reconceptualise lifelong learning and the Internet in ways that take social structures into account.";Lifelong learning and the Internet: Who benefits most from learning online?;Eynon, R. & Malmberg, L-E.;2020;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
"Artificial intelligence has become a routine presence in everyday life. Accessing information over the Web, consuming news and entertainment, the performance of financial markets, the ways surveillance systems identify individuals, how drivers and pedestrians navigate, and how citizens receive welfare payments are among myriad examples of how AI has penetrated into human lives, social institutions, cultural practices, and political and economic processes. The effects of the algorithmic techniques employed to enable AI are far-reaching and have inspired considerable epochal hype and hope, as well as dystopian dread, although they remain largely opaque and weakly understood outside of the social networks of technical experts (Rieder 2020). The profound social and ethical implications of AI, however, are becoming increasingly apparent and the objects of significant critical attention. AI is at the centre of controversies concerning, for example, automation in workplaces and public services; algorithmic forms of bias and discrimination; automated reproduction of inequalities and disadvantage; regimes of data-centred surveillance and algorithmic profiling; disregard of data protections and privacy; political and commercial micro targeting; and the power of technology corporations to control and shape all sectors and spaces they penetrate, from whole cities and citizen populations to specific collectives, individuals or even human bodies (Whittaker et al. 2018). Numerous ethical frameworks and professional codes of conduct have been developed to attempt to mitigate the potential dangers and risks of AI in society, though important debates persist about their concrete effects on companies or the way such frameworks and codes may serve to protect commercial interests (Greene, Hoffman, and Stark 2019).";Historical threads, missing links, and future directions in AI in education;Williamson, B. & Eynon, R.;2020;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
"The first special issue of Learning, Media and Technology of 2020, entitled ‘Education and technology into the 2020s: speculative futures’, presented a series of papers looking to the future of critical research on educational technologies. As we write, just a few months later, with the coronavirus pandemic sweeping around the world, the future appears more uncertain than ever. Global infection and illness, population lockdowns, and mass closures of educational institutions have engulfed countries across the planet in the short time between issues of this journal. The global pandemic is of course not only a serious public health emergency, but a political, economic and social emergency too. Scholarship across myriad disciplines in years to come will examine the medical, political, economic and social factors defining our present moment. Many of these issues will be of interest to readers of Learning, Media and Technology. They include political manoeuvring in relation to the pandemic, from misinformation and economic measures to policies of social distancing, quarantining and isolation; the use and misuse of large-scale data, statistics and visualizations; new forms of digitally mediated work, culture and personal life; surveillance systems for ‘contact tracing’; the use of predictive epidemiological modelling; the development of techniques for better public understanding of science; and the political use of behavioural economics as a public pedagogy of population management. Future papers in this journal will be written in the context of changes currently being experienced at planetary scale, and potentially dramatic shifts in the relationships between science, technology and society. In one key area we feel Learning, Media and Technology can and should make a more direct contribution to knowledge and practice during the COVID-19 pandemic: the switch to online and digital education formats and the rise of ‘remote’ forms of teaching and learning as a consequence of mass closures of schools, colleges and universities. In this moment of pandemic politics, where contests are being fought at multiple scales and levels over the ways to handle and resolve the crisis, distance education has become a widespread matter of concern for political authorities, education businesses, charities, teachers, parents and students alike. Education has become an emergency matter, and along with it, educational technologies have been positioned as a frontline emergency service. In recent years Learning, Media and Technology has become a key publication for critical studies of education and technology. Other outlets have responded to the rapid switch to online education with useful guidance, advice, and references to extant research from promising studies that might support educators to make the best of this new educational emergency. But the need remains for critical reflection on the planetary pivot to digitally mediated remote and distance education. We have no wish to denigrate or criticize online distance education, but rather, the aim of this brief editorial is twofold. First, we want to raise a series of critical cautions, based on previous papers and special issues published in the journal, against simplistic and opportunistic claims that educational technologies are a ready-made remedy for the current crisis. Second, we want to issue a call for future research to examine, in up-close detail, the effects and consequences of the expansion and embedding of digital technologies and media in education systems, institutions and practices across the world. We don’t necessarily see these issues as new or unique to the pandemic, but they are currently being experienced more acutely and affectively by educators, students and parents around the world, from the early years through to higher education. Within our own specialist area of research and practice, pandemic politics is now playing out through attempts to thoroughly embed public education systems and practices, at international reach, in increasingly powerful technological systems. We raise here four significant issues in education and technology for reinvigorated exploration.";Pandemic politics, pedagogies and practices: digital technologies and distance education during the coronavirus emergency;Williamson, B., Eynon, R. & Potter, J.;2020;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
"This paper introduces the concept of digital structural violence and examines the negative role it could have in future learning systems. To address it, we propose a new interdisciplinary research agenda at the intersection of three current but disparate lines of work that: Use the concept of epistemic privilege to theorise the inclusion of marginalised learners in the design of learning systems, and utilise participatory action research and emancipatory methodologies to pragmatically ensure this happens; Support young learners and teachers to understand and build their own artificial intelligence algorithms; Develop sustainable interdisciplinary links with computer science to address digital structural violence at the algorithmic level and to make its societal implications and underlying processes more widely understood, especially by teachers. Taken together, these provide for a material form of resistance to digital structural violence and a theoretically and methodologically coherent future research agenda for building just learning systems.";Can we avoid digital structural violence in future learning systems?;Winters, N., Eynon, R. & Geniets, A. et al.;2019;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
In many ways, the passing of another decade is nothing remarkable. The world does not transform periodically every ten years. Nevertheless, the fact that the 2020s are now upon us provides good reason to reflect on how education (and wider society) is changing. This special issue of Learning Media & Technology takes the new decade as a prompt to look forward to the near-future. It asks what issues relating to education, media and technology might be at the forefront of our minds when 2030 comes around? More importantly, it calls us to consider how we should be preparing ourselves in the meantime. Regardless of what the future holds, we are undeniably in the midst of what is a very distinct (and perhaps even unusual) time. Popularism and political instability is gaining hold in all manner of alarming ways. It is now regularly claimed that globalisation is dead, that we are living in a post-digital age, and that we are on the cusp of an ‘industrial revolution 4.0’. Even if we discount these headline claims as hyperbole, it is clear that digital technologies are a significant factor in the ways in which our day-to-day lives are now distinctly different than they were 20 years ago. It makes sense then to expect that digital technologies will continue to be a significant part of how our future is shaped as the nature of the world’s economies, politics, cultures, and societies steadily (and often unpredictably) shift. As major shifts unfold, education the world over faces considerable change, but many of the problems that have long blighted education systems stubbornly persist. Schools around the world continue to face deficiencies in resourcing, significant inequalities of educational opportunity, alongside poor-quality teaching, curriculum and school organisation. These are all issues that pre-date the first ‘computers in the classroom’ and the subsequent forays into ‘digital education’. Indeed, the simple goal of securing access to basic primary education for all children looks unlikely to be realised by 2030 (if ever at all). Such fundamental problems have haunted education for centuries and are likely to remain long beyond 2030 even though many of the ways education is organised may change. All told, these are worryingly familiar and unfamiliar times for everyone in education.;What’s next for Ed-Tech? Critical hopes and concerns for the 2020s;Selwyn, N., Hillman, T. & Eynon, R. et al.;2019;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
Many universal access policies are based on the assumption that the removal of the digital divide will enhance social mobility. But this assumption is not supported by the available evidence. Using four waves of longitudinal data from Britain we show that Internet use has a positive effect on social class mobility, while controlling for age, gender, education, health, and previous social class membership between 1997 and 2013. In doing so, we contribute to a wider discussion of the relationships between social and digital inequalities and highlight the challenges and potential of this methodological approach for future research.;Moving on up in the information society? A longitudinal analysis of the relationship between Internet use and social class mobility in Britain;Eynon, R., Deetjen, U. & Malmberg, L-E.;2018;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
"Looking back over the editorials for Learning, Media and Technology, it is possible to discern a common theme. The central plea is for those working in the fields of ‘Ed Tech’ to take a more critical approach to research, over the far more common instrumental endeavours (e.g., Eynon 2018; Potter 2017; Selwyn 2012). Yet, over recent years there has been an interesting shift. Critical Ed Tech research is becoming more mainstream. The most appropriate way to evidence this would be through a sophisticated combination of bibliometric, web metric and discourse analysis – but instead I simply offer some recent instances I have experienced as co-editor. Over the past few months I have been at three quite different conferences, where work from the journal has featured. At the American Educational Research Association Conference this year it was possible to create a ‘Critical Ed Tech programme’ from the variety of panels, workshops and other events on offer that were focused on this topic, with many contributions by both the LMT editorial boardand authors in this journal. At the Networked Learning Conference, critical pedagogy and technology research was both practised and theorised. Similarly, at the London Festival of Learning, that encompassed the International Conference of the Learning Sciences, Learning at Scale and the International Conference on Artificial Intelligence in Education, a number of keynote speakers spoke passionately about the need to connect the work in these fields with wider debates in the Sociology of Ed Tech, including topics such as algorithmic accountability, knowledge infrastructures, and agency. In all instances, authors and papers that feature in this journal were mentioned. In June, the editorial team could not help but be pleased to find out that Learning, Media and Technology now ranks 13 out of 238 Education journals and has a 2017 impact factor of 3.175. While the problems with such metrics are well established, it can perhaps be taken as another indication of the interest and engagement with studies of education and technology that take a more critical stance. Certainly, the numbers of submissions to the journal or numbers of people reading it have never been higher. It seems then like an exciting moment for this area of study: might it be the case that we are at last becoming more mainstream? It is perhaps too early to say, but if these trends are supported by evidence, then it is something to celebrate. Yet, there is a risk that much of this effort is isolated within our own communities, and whilst the number of experts engaged in this area grows, we should ask whether or not we are connecting sufficiently well with wider networks to enable this area of focus to develop fully. In particular, we could be more mindful of connecting with the following: Academics in related disciplines: There is a huge amount of work going on in other areas of social science and technology studies that are relevant to our area. Indeed, authors in Learning, Media and Technology often cite this work on a regular basis. But those working in related areas tend not to look to Education for inspiration, and do not cite our work from this journal. This is something that needs to change to develop and innovate in this area of study. Developers of Ed Tech from an array of disciplines including computer science and engineering: It is of course important to maintain a theoretical stance, yet such links can sometimes enable the building of more innovative and potentially valuable social data science methods, and encourage stronger understanding of the technology that we are trying to theorise about. Both are crucial for the currency and future directions of our work. New stakeholders in the Ed Tech space: there are increasing arrays of organisations working outside or on the edges of academia that have strong connections to policy. Often the people employed in these organisations have come from academia. Creative ways of connecting with the practices in these new and emerging networks may be important for the relevance and impact of our work more broadly. Of course, the amount of instrumental work and hype that remains in this area outweighs our contribution in terms of sheer scale. But it is nevertheless time for some celebration and recognition of the fact that Critical Ed Tech Research has in some ways ‘made it’, even if there is more to do for it to become a fully-fledged field of study. Contributions that aim to work toward this goal are always welcome in Learning, Media and Technology.";Into the mainstream: where next for Critical Ed Tech research?;Eynon, R.;2018;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
The British government is claiming digital skills will deliver economic growth to the country and social mobility to young people: its ministers call it ‘a pipeline to prosperity’. While declaring this pipeline, the government assumes the needs of the economy and young people’s needs are (or should be) synchronised. We challenge this assumption and the policy it sustains with data from questionnaires, workshops and interviews with 50 young people from communities in South Wales (including a former mining town and a deprived inner city area) about digital technology’s role in their everyday life. We use a new typography to compare the reality of their socially and economically structured lives to the governmental policy discourse that makes them responsible for their country’s future economic success. To explain these young people’s creative and transgressive use of technology, we also make an empirically grounded contribution to the ongoing theoretical debates about structure and agency.;Is digital upskilling the next generation our ‘pipeline to prosperity’?;Davies, H. & Eynon, R.;2018;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
"There is a long and rich history of feminist perspectives on technology (e.g. Cockburn 1985; Haraway 1988; Wajcman 1991) and pedagogy (hooks 1994; Lather 1992; Shrewsbury 1987). These have been informed in turn by feminist epistemology and philosophy, challenging Western rationalist assumptions about the subject of these powerful knowledges (Harding, 1991), or what Haraway (1988) calls the decontextualised and disembodied ""gaze from nowhere"". As Rose (1983) has said: ""science … is neither sexless nor classless; she is a man, bourgeois, and infected too."" Earlier feminist frameworks for thinking about education and technology include a feminist ethics of care as relational (Gilligan 1977; Tronto 1993) in contrast to an ethics of justice (Kohlberg 1971); and a feminist epistemology of knowing as difference (Belenky et al 1986) in contrast to a linear model of development (Perry 1970). There are echoes of these concerns in current thinking about digital pedagogy. Adam (2000) observed that AI projects of the 1990s ‘deleted’ subjects other than the assumed white male norm: today, ‘other’ subjects are still routinely deleted or oppressed through algorithmic bias. The aim of this Special Issue is to explore how this diverse legacy of feminist scholarship and activism might be deployed to interrogate the power relations of current educational technologies and practices. Our efforts are aligned with Weber’s claim that: At the heart of feminist studies lies the search for better, or at least more visible, ways to design and use categories, knowledge, and technologies, to shape objects, artifacts, and worlds in order to make exclusions visible and to overcome the hardships of gender-asymmetries, reductionism, and injustice. (2006, p. 402). There is also a history of contestation amongst feminists, as ""first-wave"" and ""second-wave"" feminisms focused on gender equality have been challenged by an intersectional ""third-wave"" that examines how interactions between gender, sex, race, class, disability and colonialism produce distinct forms of oppression (Crenshaw 1991). These intersectional perspectives have led to alliances with queer scholarship (Anzaldúa, 1987), posthumanist thinking (Braidotti, 2007) and postcolonial studies (Dussel 2015; Pollock & Subramaniam, 2016), reflecting the belief that ""a singular focus on gender, race, coloniality, or indigeneity alone leaves numerous gaps in our understanding of the constitution of science and society"" (Subramaniam et al., 2017, p. 407). Feminist perspectives on technology and education are not only of interest to women, and not only because of their focus on gender inequality. In this time of pandemic and inter-related health, social, political, economic and environmental crises, how can feminist epistemologies, theories, methods, frameworks and debates support more emancipatory research and practice – for the benefit of everyone?";Feminist perspectives on learning, media and technology: recognition and future contributions;Eynon, R.;2018;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
This year has seen some editorial changes for LMT. Neil’s editorial role for the journal came to an end, and John joined as co-editor of LMT. Given this context, we thought it was an appropriate moment to take stock of the journal: where we were in 2010, where we are now, and where we hope to be in 2020. Below, each of us reflects on our experiences of editing LMT, and our hopes for its future.;A decade of Learning Media and Technology: looking back and looking forward;Selwyn, N., Eynon, R. & Potter, J.;2017;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
"Despite the ongoing discourse about the constantly connected and digitally savvy youth in the UK, a growing evidence base demonstrates that there are still significant inequalities in young people’s ability to access and use the internet. There is a small, but significant, proportion of young people who do not have internet access at home, nor have sufficient digital skills to engage online in ways that are meaningful to them. This paper presents findings from a two-year school and local council-run initiative in England to provide 30 such digitally disadvantaged young people with a laptop and stable internet connection at home as well as school support. Drawing on rich qualitative data (home and school visits; parent, student, and teacher interviews), we explore the experiences of young people, parents, and teachers who were part of this digital inclusion scheme. Specifically, we examine how the long-standing essentialist discourses around ‘digital youth’ and determinist ideas of technology and social change inform how such a scheme is perceived, enacted, and experienced by the teachers, parents, and young people involved with the initiative, as well as the implications these discourses have for the ways the outcomes of such projects are judged.";Addressing digital inequalities amongst young people: conflicting discourses and complex outcomes;Wilkin, S., Davies, H. & Eynon, R.;2017;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
"Numerous academic studies highlight the significant differences in the ways that young people access, use and engage with the Internet and the implications it has in their lives. Trying to address such inequalities is complex, and the outcomes of digital inclusion schemes are rarely uniformly positive or transformative for the people involved. Therefore the hope of such schemes that if sufficiently empowered, incentivised and aspirational, the disadvantaged can use access to technology to transform or transcend what Bourdieu [1992. The logic of practice. Studies in philosophy and education. Cambridge: Polity Press; New Ed edition] calls their ‘class of conditions’ (p. 53) is largely misplaced. This gap between expectation and reality demands theoretical attention. Focusing on a two-year digital inclusion scheme for 30 teenagers and their families in one area of England, this qualitative study analyses why, despite the good intentions of the scheme’s stakeholders, it fell short of its ambitions. Instead, our theoretical analysis explains how the neoliberalist systems of governance that are increasingly shaping the cultures and behaviours of our Internet service providers and schools cannot solve the problems they create.";Neoliberal gremlins? How a scheme to help disadvantaged young people thrive online fell short of its ambitions;Davies, H., Eynon, R. & Wilkin, S.;2017;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
Through a survey with a representative sample of Dutch Internet users, this article examines compound digital exclusion: whether a person who lacks a particular digital skill also lacks another kind of skill, whether a person who does not engage in a particular way online is also less likely to engage in other ways, and whether a person who does not achieve a certain outcome online is also less likely to achieve another type of outcome. We also tested sequential digital exclusion: whether a lower level of digital skills leads to lower levels of engagement with the Internet, resulting in a lower likelihood for an individual to achieve tangible outcomes. Both types of digital exclusion are a reality. Certain use can have a strong relation with an outcome in a different domain. Furthermore, those who achieve outcomes in one domain do not necessarily achieve outcomes in another domain. To get a comprehensive picture of the nature of digital exclusion, it is necessary to account for different domains in research. ;The compoundness and sequentiality of digital inequality ;van Deursen, A., Helsper, E. & Eynon R. et al.;2017;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
Over the past few years, two forms of crowd-based initiatives have been supported and facilitated by universities. First is the focus on Massive Open Online Courses (MOOCs) where thousands of people are encouraged to learn together, and the second is digitally enabled citizen science initiatives where the general public are encouraged to help out on a voluntary basis with varied tasks related to university projects in the sciences and the humanities. Both of these initiatives can be critiqued. The most prevalent form of MOOC design that has emerged in recent years prioritises individual learning of a style that has strong behaviourist undertones, lacks awareness of the global audience, and plays to the strengths of those learners with significant experience of education. In citizen science, much is made of the learning benefits to the individual from participation. However, often the care taken over supporting learning in such settings is relatively limited. Far more time is spent in ensuring the organisation gets the most useful results for their project. Learning is at best seen as a happy by-product of the initiative and at worst as a time-consuming overhead that needs to be controlled in order not to interfere with the main task at hand. In both cases the aspect that makes these initiatives significant and interesting from a learning perspective – a crowd of people with an array of different ideas, experiences and motivations coming together around a particular topic or problem – is largely being ignored or even designed out. This needs some attention, as universities have a responsibility to individuals outside academe – not just to provide any kind of open learning opportunities but also to provide the very best kinds of learning designs within such settings. Similarly, forms of citizen science should place equal attention on designing initiatives that lead to benefits for the individual and achieving outcomes for the organisation. As both crowd-based forms are essentially about trying to support knowledge construction of various forms, it is surprising how little research or practice has drawn relationships between these two areas. Yet, at the very least both kinds of initiative have a responsibility to address specific issues of learning in the crowd – whether it is to engage with a topic of interest in the case of MOOCs or support a university project in the case of citizen science. This is not to say that all learning is now social, or we should ignore individual models of engagement with these platforms, but rather to stress that individual interactions with MOOC or citizen science content are not the only way that people learn. The second key reason for paying attention to ‘the crowd’ is that for universities committed to global and open forms of education, interaction with different individuals from varied backgrounds may lead to a shift in interpretations and perspectives on educational content, and in so doing potentially shift the power dynamics between academia and the citizen. Open education is an area that requires significant thought and critique. There is no space in this short editorial to do justice to this important topic (see instead Bayne, Knox, and Ross’s (2015) special issue). Nevertheless, it is reasonable to argue that supporting the social construction of knowledge in MOOCs and reframing citizen science as a learning project could lead to benefits for the institutions concerned not only in quantitative terms of retention, satisfaction, widening participation, and (in the case of citizen science) productivity, but could also make an important qualitative difference in the nature of the research insights produced in citizen science projects, and a more meaningful engagement with a wider set of learners to develop open education initiatives that engage with ‘global complexity’ (Urry 2003). As readers of Learning, Media and Technology know, enabling interaction in crowd-like settings to support learning is a non-trivial task. Structuring interaction and information sharing in ways that ensure all can participate, supporting a diversity of individuals that reflect the different ways and reasons that people are engaging with the platforms, and that account for varied educational backgrounds and digital inequalities, are just some of the factors that need to be considered to facilitate any kind of widening participation agenda and crowd-based interaction. Crowds are not the same as communities, classes or groups but, despite this, many of the well-established concepts and theories from previous research into online learning can be leveraged to inform practice. However, to-date, these ideas have been ignored in much of the literature and design of crowd-based phenomena for education. Instead, often the crowd is viewed as inherently capable of self-organising in highly problematic ways that lead to it being only well-educated people who have experience of Western, online education being able to participate fully and have their voices heard. Despite the difficulties of supporting learning in a crowd, these challenges need to be addressed. Important here is changing the mindset, or overall goals of such initiatives from a focus on efficiency (e.g., minimising the cost/time of accomplishing a content creation or translation task in citizen science, or packaging of information in stylish ways that can be re-used at anytime in the case of MOOCs) to one of learning and knowledge construction. This can only be achieved with a real awareness (not set of assumptions) about why people want to be part of the initiative in the first place, and a perspective where the crowd is treated as a partner in the initiative, not an end-user or provider of labour, where individuals are motivated and rewarded through participation. Crowd-based interactions online have emerged as an important way for people to connect around topics that are meaningful to them. Thus, thinking more critically about how best to support these initiatives in ways that explore power imbalances and ways to value the knowledge that can emerge from such interactions are important areas for future research. Such debates would be welcome in future editions of Learning, Media and Technology.;Crowds, learning and knowledge construction: questions of power and responsibility for the academy;Eynon, R.;2017;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
"Digital skills are an important aspect of ensuring that all young people are digitally included. Yet, there tends to be an assumption in popular discourse that young people can simply learn these skills by themselves. While experience of technologies forms an important part of the learning process, other resources (i.e., access to technology and support networks) plus clear motivations are required. Through in-depth interviews with 20 young people who are digitally excluded, this paper highlights the kinds of digital skills these young people find problematic, and the reasons why they find developing these skills so challenging. We demonstrate how poor access to technology, limited support networks and their current situation prevent these young people from gaining the experiences they need to support the development of their digital skills; and how lack of experience and inadequate skills limit the extent to which they perceive the internet to be valuable in their lives. These individual experiences, shaped very much by the wider social structure of which they are part, show how young people cannot simply be left to learn digital skills by themselves and that intervention is required to try to address some of the digital inequalities apparent in younger generations.";The digital skills paradox: how do digitally excluded youth develop skills to use the internet?;Eynon, R. & Geniets, A.;2015;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
Although a number of instruments have been used to measure Internet skills in nationally representative surveys, there are several challenges with the measures available: incompleteness and over-simplification, conceptual ambiguity, and the use of self-reports. Here, we aim to overcome these challenges by developing a set of reliable measures for use in research, practice, and policy evaluations based on a strong conceptual framework. To achieve this goal, we carried out a literature review of skills-related studies to develop the initial Internet skills framework and associated instrument. After the development of this instrument, we used a three-fold approach to test the validity and reliability of the latent skill constructs and the corresponding items. The first step consisted of cognitive interviews held in both the UK and the Netherlands. Based on the cognitive interview results, we made several amendments to the proposed skill items to improve clarity. The second step consisted of a pilot survey of digital skills, both in the UK and in the Netherlands. During the final step, we examined the consistency of the five Internet skill scales and their characteristics when measured in a representative sample survey of Dutch Internet users. The result is a theoretical, empirically and cross-nationally consistent instrument consisting of five types of Internet skills: operational, navigation information, social, creative, and mobile.;Development and validation of the Internet Skills Scale (ISS);van Deursen, A., Helsper, E. & Eynon R. et al.;2015;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
While the quantified self-community remains a relatively niche group, using technology to record and track information about at least one part of our lives is becoming more commonplace. Particularly in health, where people can now use technology to monitor exercise, calorie intake and other metrics to improve their overall fitness (e.g., Fitbit) or assist their own health or others through collecting and sharing data (e.g., PatientsLikeMe), we are starting to see data as a way to help inform our own life choices through providing us with a means of collecting information about ourselves over time, and sharing and comparing this data with others if we wish to do so. In learning and education, less has been said specifically about the quantified self, although it can be seen in the current trends of ‘gamification’ for certain learning activities and connects closely to learning analytics and wider discourses about personalisation.;The quantified self for learning: critical questions for education;Eynon, R.;2015;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
The importance of considering the family context in the adoption and use of the Internet are well recognised. Supporters of the digital inclusion agenda often see children as a way to increase the digital skills and use of the Internet by parents and older adults. However, there is a limited amount of research that has explored whether this is really the case. Using two nationally representative survey data sets from Britain, this paper aims to better understand the links between children and adults' use of the Internet within the same household. In this paper, we ask what influence children have on adults' Internet use, skills and engagement. The paper concludes that while children might influence uptake, characteristics of the adult (for example. education, age and social capital) are more important in relation to their skills and engagement with the Internet.;Family dynamics and Internet use in Britain: What role do children play in adults' engagement with the Internet?;Eynon, R. & Helsper, E.;2014;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
Despite the hype and speculation about the role massively open online courses (MOOCs) may play in higher education, empirical research that explores the realities of interacting and learning in MOOCs is in its infancy. MOOCs have evolved from previous incarnations of online learning but are distinguished in their global reach and semi-synchronicity. Thus, it is important to understand the ways that learners from around the world interact in these settings. In this paper, we ask three questions: (1) What are the demographic characteristics of students that participate in MOOC discussion forums? (2) What are the discussion patterns that characterize their interactions? And (3) How does participation in discussion forums relate to students' final scores? Analysis of nearly 87,000 individuals from one MOOC reveals three key trends. First, forum participants tend to be young adults from the Western world. Secondly, these participants assemble and disperse as crowds, not communities, of learners. Finally, those that engage explicitly in the discussion forums are often higher-performing than those that do not, although the vast majority of forum participants receive “failing” marks. These findings have implications for the design and implementation of future MOOCs, and how they are conceptualised as part of higher education.;Communication patterns in massively open online courses;Gillani, N. & Eynon, R.;2014;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
"Peer reviewing is essential in ensuring high-quality academic publishing. Yet the whole process of reviewing for an academic journal remains a relatively hidden process. Very little guidance is given about what editors need from reviewers in order to make a good and fair decision about an article, or what kinds of elements make for a good review. For early career researchers, new to academic publishing, reviewing articles can be a puzzling process. At Learning, Media, and Technology, we operate a two-stage review process. At Stage 1, one or both editors review the paper and ensure that the paper: (1) fits the kinds of articles we are looking to publish; (2) has a strong enough methodological/theoretical basis to have a reasonable chance of being published; and (3) fits all our general criteria (e.g., word length, format). We do this because we do not want to waste the time of our reviewers and because it is far better for authors to know quickly if their article is not suitable for publication in the journal so that they can submit it somewhere else. Once we decide to send an article out for review, we aim to get each paper reviewed by a minimum of two or (ideally) three experts. These experts can be selected in a number of ways based on their methodological and topic expertise. These are typically from (in no particular order) appropriate members of the editorial board, papers used in the article, the knowledge of the editor, automated suggestions from our article submission system based on prior authors and reviewers, and author preferred reviewers. Getting the right mix of academics (with different viewpoints, country perspectives etc.) to review a paper is crucial, and so selecting reviewers is one of the most time-consuming aspects of being an editor. Once we invite someone to review an article, we are essentially asking for a review of around 300–500 words, a recommendation of what to do with the article (accept, minor corrections, major corrections, revise and resubmit, and reject) and an agreement to review the article in the given time frame. In return, the reviewer gets a chance to read about work in their field before anyone else, a discount on Routledge books, an insight into the process of getting work published (as our reviewers are sent the final editorial decision plus the anonymised feedback from the other reviewers), and another line on the academic CV.";How to review a journal article: questions of quality, contribution, and appeal;Eynon, R.;2014;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
Massive Open Online Courses (MOOCs) bring together a global crowd of thousands of learners for several weeks or months. In theory, the openness and scale of MOOCs can promote iterative dialogue that facilitates group cognition and knowledge construction. Using data from two successive instances of a popular business strategy MOOC, we filter observed communication patterns to arrive at the “significant” interaction networks between learners and use complex network analysis to explore the vulnerability and information diffusion potential of the discussion forums. We find that different discussion topics and pedagogical practices promote varying levels of 1) “significant” peer-to-peer engagement, 2) participant inclusiveness in dialogue and ultimately, 3) modularity, which impacts information diffusion to prevent a truly “global” exchange of knowledge and learning. These results indicate the structural limitations of large-scale crowd-based learning and highlight the different ways that learners in MOOCs leverage and learn within, social contexts. We conclude by exploring how these insights may inspire new developments in online education.;Structural limitations of learning in a crowd: communication vulnerability and information diffusion in MOOCs;Gillani, N., Yasseri, T. & Eynon, R. et al.;2014;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
Digital literacy and inclusion have been two important, largely separate, areas of study that examine the relationships between Internet skills and engagement. This article brings together these areas of research by testing a model that assumes specific pathways to inclusion: specific sociodemographic factors predict specific digital skills and specific digital skills predict related types of engagement with the Internet. Analyses of nationally representative survey data of Internet use in Britain highlight considerable measurement and conceptual challenges that complicate digital literacy research. The findings suggest that linking literacy and exclusion frameworks allows for a more nuanced understanding of digital engagement. Different groups lacked different skills, which related to how their engagement with the Internet varied.;Distinct skill pathways to digital engagement;Helsper, E. & Eynon, R.;2013;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
The interest in Big Data is growing exponentially. Research calls, commercial insights, and government initiatives all seem to be focused on exploiting the potential of technology to capture and analyse massive amounts of data in increasingly powerful ways. Big Data, that is, data that are too big for standard database software to process, or the more future-proof, ‘capacity to search, aggregate, and cross-reference large data sets’ (boyd and Crawford 2012, 663), is everywhere. For some, Big Data represents a paradigm shift in the ways that we understand and study our world, and at the very least it is seen as a way to better utilise and creatively analyse fine grained data for public and private benefit. In some ways, this is not a new phenomenon. Those working in the commercial sector have been collecting and combing large data sets to improve segmentation of goods to customers and better understand their market for many years (Manyika et al. 2011). Nor is it particularly big news to those working in certain fields in the natural sciences. Yet, in recent years, a far wider range of stakeholders have become more involved and more excited about the potential of Big Data.;The rise of Big Data: what does it mean for education, technology, and media research?;Eynon, R.;2013;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
"Studies have shown (e.g. Reeves and Nass, 1996; Veletsianos and Miller, 2008; Turkle, 2010) that humans are willing to treat electronic media artefacts of various kinds as real in certain situations. This study looks at this phenomenon with respect to interactions between human subjects and an onscreen embodied conversational agent (ECA), known as the Learning Companion, in order to explore the views of these people as to the viability of engaging in conversations about learning with such a device. 20 older adults, mostly retired, participated in this study where they engaged in experimental conversations with the Learning Companion about their interests, and about using the Internet. Findings suggest that there was a marked division within the sample, between those who did and those who did not consider the Learning Companion to be personable and credible as a conversation partner. There was a strong correlation in this respect between the educational backgrounds of the subjects, with the more academically self-confident being more resistant to the Learning Companion’s attempts to engage them in conversation than the less academically self-confident participants.";Believers and Non-Believers: How Potential Users Respond to the Prospect of an Onscreen Learning Assistant;Davies, H. & Eynon, R.;2013;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
Information seeking is one of the most popular online activities for young people and can provide an additional information channel, which may enhance learning. In this study, we propose and test a model that adds to the existing literature by examining the ways in which parents, schools, and friends (what we call networks of support) effect young people's online information behaviours, while at the same time taking into account young people's individual characteristics, confidence, and skills to use the Internet. Using path analysis, we demonstrate the significance of networks of support in understanding the uptake of online information seeking both directly and indirectly (through enhancing self‐concept for learning and online skills). Young people who have better networks of support, particularly friends who are engaged in technology, are more likely to engage in online information seeking. While quantitative models of this nature cannot capture the complexity of individual online search practices, these findings may assist in the development of policy and practice to support young people to make the most effective use of the Internet for information seeking.;Understanding the online information‐seeking behaviours of young people: the role of networks of support;Eynon, R. & Malmberg, L-E.;2011;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
Universities across the globe are increasingly being asked to justify their existence by demonstrating their impact on society. It is an issue that has raised a great deal of debate in academic circles, with many concerned about what this shift means in terms of the role and purpose of universities, the kinds of research that is valued (or not) and the longer term impact on innovation and thinking. As well as these bigger questions, each field or discipline will have their own perspective on the implications of the impact agenda for their practice, and I would like to highlight some of the significant challenges as well as opportunities that I see for those engaged in research around learning, media and technology. Indeed, the research that readers and contributors to this journal carry out is perhaps a particularly interesting case, as the extent to which our research is rooted in practice varies significantly (Czerniewicz 2008). It follows then, that our views about the debate around impact are also likely to differ. As a starting point, it is important to recognise that we all hope to make a difference of some kind, and often do – through our teaching, conversations with colleagues and our research and dissemination – but these ‘impacts’ are wide-ranging and often subtle, diffuse and difficult to measure. In reality, our activities are often not easily translatable or directly aligned with the kinds of impact that are currently being required from academia to prove the worth of our endeavours. It is perhaps the matching of our ‘everyday’ impacts with the ‘required’ impacts where the heart of the challenge lies.;The challenges and possibilities of the impact agenda;Eynon, R.;2012;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
Universities across the globe are increasingly being asked to justify their existence by demonstrating their impact on society. It is an issue that has raised a great deal of debate in academic circles, with many concerned about what this shift means in terms of the role and purpose of universities, the kinds of research that is valued (or not) and the longer term impact on innovation and thinking. As well as these bigger questions, each field or discipline will have their own perspective on the implications of the impact agenda for their practice, and I would like to highlight some of the significant challenges as well as opportunities that I see for those engaged in research around learning, media and technology ;Editorial: The Challenges and Possibilities of the Impact Agenda;Eynon, R.;2012;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
In recent years, there has been a growing interest in how uptake and use of the Internet by individuals is influenced by other members of the same household. Nevertheless, the relationships between the use of the Internet by young people and adults in the same household are complex, multifaceted and underexplored (Selwyn 2004). The majority of research has focused on how parents have influenced their child’s use of the Internet. Yet given the current interest in family learning and generational differences in Internet use, the opposite relationship (i.e., how children may be influencing parents use of the Internet) is likely to be just as important. Indeed, there are some tentative indications that this relationship is important to explore. In Britain, individuals over 18 who have children in the household are more likely to use the Internet (Helsper and Eynon 2010). There is also some evidence that parental Internet use, skills and their understanding of the Internet has become closer to that of their children (Livingstone et al, in press). This may be for a number of reasons. For example, it is well documented that parents often purchase the Internet in their homes to support their child’s education. Thus, simply having home access may encourage parents to use the Internet more frequently for a wider range of purposes. Secondly, adults could be supported by their children in using the Internet. Indeed, some authors have highlighted that young people can sometimes (although not always effectively) support parents uses of ICTs by improving their parents skills in this domain (e.g. Holloway and Valentine 2003). Finally, adults may also use the Internet to encourage interaction and dialogue with their children (Aarsand, 2007) thus using it more extensively than they otherwise would. The existing research offers a better understanding of the relationships between parents’ digital inclusion and children’s use of the Internet. However, the extent to which having children in the household is related to how an adult Internet user engages with the Internet for a range of activities remains an under researched area. Using data from the 2009 and 2011 Oxford Internet Surveys (OxIS) which contain information on how the British population access and use the Internet and the EU Kids Online II Survey which is a survey of 25,000 Internet using children and their parents across Europe, this paper aims to better understand the links between children and adults’ use of the Internet within the same household. Specifically, the paper will address two questions: 1) how significant are child / parent dynamic s compared to other factors (e.g. social capital, civic engagement) in understanding adults’ engagement with the Internet? 2) To what extent do the characteristics of the child and the parent determine parental engagement with the Internet? The analysis will provide policy makers and practitioners with an informed understanding of the relationships between adult and child Internet use. The findings may assist in the development of policies and practices that seek to support digital inclusion and provide a useful quantitative framework for other qualitative research in this area.;Understanding the Relationship between Family Dynamics and Internet Use in Britain;Eynon, R. & Helsper, E.;2011;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
Using a nationally representative British survey, this article explores the extent to which adults are using the internet for learning activities because they choose to (digital choice) or because of (involuntary) digital exclusion. Key findings suggest that reasons for (dis)engagement with the internet or the uptake of different kinds of online learning opportunities are somewhat varied for different groups, but that both digital choice and exclusion play a role. Thus, it is important for policy initiatives to better understand these groups and treat them differently. Furthermore, the more informal the learning activity, the more factors that play a significant role in explaining uptake. Policies designed to support individuals’ everyday interests, as opposed to more formal kinds of learning, are likely to be more effective in increasing people’s productive engagement with online learning opportunities.;Adults learning online: Digital choice and/or digital exclusion?;Eynon, R. & Helsper, E.;2011;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
"Using data from a nationally representative survey of over a 1000 young people in the UK this paper proposes a typology of the ways young people are using the Internet outside formal educational settings; and examines the individual and contextual factors that help to explain why young people are using the Internet in this way. Specifically, this paper addresses two research questions. First, can we distinguish coherent profiles of young people’s Internet use? Second, how do these usage profiles relate to individual and contextual factors associated with the Internet user? From the results of latent profile analysis and multinomial regression four types of Internet usage profiles are identified: the peripherals, normatives, all-rounders and active participators, which were differentiated by individual characteristics and contextual features. Such research enables practitioners, researchers and policy makers to better understand how young people are using the Internet in order to think in a more informed way about how new technologies could be used to enhance education and learning; and to develop initiatives that more specifically target and support different segments of the population.";A typology of young people’s Internet use: Implications for education;Eynon, R. & Malmberg, L-E.;2010;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
Generational differences are seen as the cause of wide shifts in our ability to engage with technologies and the concept of the digital native has gained popularity in certain areas of policy and practice. This paper provides evidence, through the analysis of a nationally representative survey in the UK, that generation is only one of the predictors of advanced interaction with the Internet. Breadth of use, experience, gender and educational levels are also important, indeed in some cases more important than generational differences, in explaining the extent to which people can be defined as a digital native. The evidence provided suggests that it is possible for adults to become digital natives, especially in the area of learning, by acquiring skills and experience in interacting with information and communication technologies. This paper argues that we often erroneously presume a gap between educators and students and that if such a gap does exist, it is definitely possible to close it.;Digital natives: Where is the evidence?;Helsper, E. & Eynon, R.;2013;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
The notion of the “digital native” has become pervasive in popular discourse about young people and new technologies. In this discourse, parents and teachers (“the digital immigrants”) have been characterised as unable to support young people in their uses of the Internet and other new technologies because (unlike the digital natives) the immigrants were not born into a world surrounded by new technologies. Yet in contrast, empirical research has shown that there is limited empirical basis for a distinction in the ways that people use new technologies because of when they were born and that young people are not all the same – they engage with new technologies in a variety of ways and vary considerably in their skills to use new technologies. Given this empirical evidence, it is important to better understand why and in what ways young people use computers and the Internet and if and how they need to be supported in this use. This paper aims to add to existing research by using empirical survey data on how and why young people in Britain use the Internet outside formal educational settings. The data is based on a nationally representative face to face survey of 1000 young people in Britain aged 8, 12, 14 and 17-19. The survey was conducted between December 2008 and January 2009 utilising a stratified sampling strategy. The survey forms part of the Learner and their Context study, commissioned by Becta, which explores young people’s views and experiences of new technologies outside school and is designed to inform the next phase of the UK’s Harnessing Technology Strategy. This paper will provide an overview of the ways young people are using the Internet for a range of activities (e.g. for homework, information seeking and creating content) and examine the factors that help to explain why young people are using new technologies for these purposes. The results demonstrate that there are a range of individual and contextual factors that help to understand use of the Internet and that formal contexts of education have an important role to play in supporting the “digital natives.”;Supporting the 'Digital Natives': What is the Role of Schools?;Eynon, R.;2010;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
"The internet presents many potential opportunities for people to learn for both formal and informal purposes. However, not everyone is able to make the most of the internet for learning. This paper utilises quantitative nationally representative survey data of internet use in Britain in order to explore the digital divide in relation to learning activities online. The results from this analysis give a detailed picture of the digital divide in Britain; illustrating those who are non‐users and users of the internet and the reasons that are important in explaining the diversity in non‐use and use of the internet for learning (e.g., age, educational background, skills, attitudes and experience). The findings may assist in the development of policies that seek to support under‐served groups to make the most effective use of the internet for formal and informal learning opportunities.";Mapping the digital divide in Britain: implications for learning and education;Eynon, R.;2009;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
The possibilities for carrying out online research are growing rapidly, enabling researchers to collect previously unavailable data about online behaviours and interactions. While such techniques provide exciting opportunities for researchers they can present ethical challenges. In this paper we explore these ethical dilemmas with particular reference to the new methods used to explore online virtual environments, the ability to re-use online data from multiple sources and the global reach of the Internet. We conclude by highlighting the key issues that need to be considered by researchers both in terms of developing their own ethical viewpoints as well as the development of future institutional and professional ethical codes of practice for online research.;New techniques in online research: challenges for research ethics;Eynon, R., Schroeder, R. & Fry, J.;2009;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
Discussion of the role of the Internet in government and research tends to be “institution-centric” in that e-government and e-research initiatives are both anchored in top-down strategies to provide information resources to citizens or researchers by place-based institutions, including governments and universities. In both institutional arenas, the diffusion of these services has been limited to small albeit growing proportions of their target audiences. In contrast, individuals with access to the Internet have taken bottom-up initiatives to obtain information and services from the space of flows of the Internet in ways that reach beyond the boundaries of both governmental and research institutions, but in ways that could compete with but also enhance existing institutions, such as by making them more accountable to their respective constituencies. Institutional actors in government and research need to more explicitly recognize and strategically adapt to the practices and tools taken up by networked individuals, such as by creating e-infrastructures that—like the Internet—enable rather than constrain bottom-up innovation.;Networked Individuals and Institutions: A Cross-Sector Comparative Perspective on Patterns and Strategies in Government and Research;Dutton, W. & Eynon, R.;2009;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
The report analyses the data gathered in a nationally representative survey of children and young people in England and provides an analysis of the findings on ICT use and related access and supervision issues involving parents and teachers.;Mapping young people's uses of technology in their own contexts: a nationally representative survey;Eynon, R.;2009;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
Discussion of the role of the Internet in government and research tends to be 'institution-centric' in that e-Government and e-Research initiatives are both anchored in top-down strategies to provide information resources to citizens or researchers by place-based institutions, including governments and universities. In both institutional arenas, the diffusion of these services has been limited to small albeit growing proportions of their target audiences. In contrast, individuals with access to the Internet have taken bottom-up initiatives to obtain information and services from the space of flows of the Internet in ways that reach beyond the boundaries of both governmental and research institutions, but in ways that could compete with but also enhance existing institutions, such as by making them more accountable to their respective constituencies. Institutional actors in government and research need to more explicitly recognize and strategically adapt to the practices and tools taken up by networked individuals, such as by creating e-infrastructures that -- like the Internet -- enable rather than constrain bottom-up innovation.;The Tools of Networked Individuals: Parallel Patterns and Strategies for Governmental and Research Institutions;Dutton, W. & Eynon, R.;2008;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
"This article reports major themes emerging from 41 semi‐structured interviews conducted with staff from one ‘old’ and one ‘new’ university in England about the use of the web in teaching and learning for campus‐based students. The research set out to explore real‐life instances of the use of the web in teaching and learning to determine the relationship between this reality and the rhetoric surrounding the use of information and communication technologies (ICTs) in universities. The focus of this paper is the motivations and/or barriers to adopting the WWW in teaching and learning at the institutional and individual staff level. The paper concludes by stressing that while there may be great potential for the use of ICTs for some aspects of teaching and learning, adoption of these new technologies is not straightforward; and, in the cases studied here, the use of the web in teaching and learning neither appears to be radically transforming teaching and learning within the university, nor to be providing (or even regarded as) a ready solution to the problems the sector currently encounters.";The use of the world wide web in learning and teaching in higher education: reality and rhetoric;Eynon, R.;2008;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
"Are universities on the cusp of significant and rapid change tied to information and communication technologies, like the Internet? Have they already been transformed in major ways? These two related questions have attracted a great deal of debate both within and outside the academic community over the past decade. The three books under review address these questions from distinctly different perspectives: Tiffin and Rajasingham suggest a model for a global virtual university where teaching and learning take place in HyperReality, addressing the needs of the new global society whilst retaining the universals of the university; Raschke promotes a radical rethink of higher education as the diffusion of the networked computer means that the current model of the university will soon be obsolete; and Robins and Webster present an edited collection whose contributors question the role of technology in driving economic, social, political and technological developments in the changes in the contemporary university.";Universities in Transition: Commercialization and the 21st Century University;Eynon, R.;2007;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
"Purpose to explore academics’ experiences of using information and communication technologies (ICTs) for teaching and learning. Analysis of three discipline‐specific focus group discussions held with academics based in Higher Education Institutions (HEIs) that use ICTs for teaching their students. The most common use of ICTs in all subjects was to provide students with access to a range of online resources. Academics’ motivations for using ICTs included: enhancing the educational experience for their students; to compensate for some of the changes occurring in higher education, such as the rise in student numbers and demand for flexible learning opportunities; and personal interest and enjoyment. The difficulties academics encountered when using these technologies for teaching included: a lack of time; dissatisfaction with the software available; and copyright. This is a small scale, exploratory study. Further research is required that is sampled in such a way as to ensure that the findings can be generalized to all academics in all institutions in the UK. The institutional, middle managerial, staff and student level all need to be considered when encouraging the further adoption of new technologies for teaching and learning in higher education. Institutional level strategies must also account for the diversity of ways ICTs may be used in teaching in different contexts across the institution. Research exploring academics’ experiences of using ICTs for teaching and learning is scarce. Further work is required to ensure the successful development and implementation of future technological and policy developments in this area.";The use of the internet in higher education: Academics’ experiences of using ICTs for teaching and learning;Eynon, R.;2005;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
An increasing number of professional groups are adopting some form of competence-based approach to education and training. Despite their increasing popularity, there remains a fierce debate in the academic literature as to the appropriateness of such approaches for the professions. This paper explores how the teaching, engineering, medical and management professions are using competence-based approaches in the workplace. Some of the key difficulties of the competence-based approach as identified in the literature are examined from a perspective rooted in the practice of these four professional groups.;Competence-based Approaches: A discussion of issues for professional groups;Eynon, R. & Wall, D.;2010;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
Massive Open Online Courses (MOOCs) bring together thousands of people from different geographies and demographic backgrounds -- but to date, little is known about how they learn or communicate. We introduce a new content-analysed MOOC dataset and use Bayesian Non-negative Matrix Factorization (BNMF) to extract communities of learners based on the nature of their online forum posts. We see that BNMF yields a superior probabilistic generative model for online discussions when compared to other models, and that the communities it learns are differentiated by their composite students' demographic and course performance indicators. These findings suggest that computationally efficient probabilistic generative modelling of MOOCs can reveal important insights for educational researchers and practitioners and help to develop more intelligent and responsive online learning environments.;Communication Communities in MOOCs;Gillani, N., Eynon, R. & Osborne, M. et al.;2014;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
"As the Internet becomes part of everyday life, policy makers have developed a range of initiatives to try to ensure that all individuals have Internet access to benefit from a wide range of online learning, employment, networking, and informational opportunities. Simultaneously, academic research in this field has proliferated rapidly, and we now have a great deal of research that demonstrates the complexity of factors that help us understand how and why people use the Internet. However, there is recognition amongst researchers in this field that the measures typically used in empirical work are not sufficiently nuanced. They do not fully reflect current theoretical thinking about digital inclusion and have not kept up with the changes in the ways that people use and understand the Internet. In 2014, the authors of this report started a project with the main objective to develop an instrument that follows the theoretical model proposed by Helsper (2012). This model hypothesises that the digital and social are related for similar (economic, cultural, social and personal) types of fields. The influence of offline exclusion on engagement with digital activities is mediated by access, skills and attitudinal or motivational aspects; and the relevance, quality, ownership and sustainability of engagement with these activities is said to mediate their influence on offline outcomes. The project’s objective was to develop measures that allow for testing of the model’s suggested paths from social to digital inclusion and vice versa by constructing indicators for digital engagement and outcomes and a set of digital skills that influences these links. The focus of this report is to propose a set of new measures of Internet skills. Internet skills form a key part of digital inclusion. Yet at present few measures have been developed that examine skills within a wider framework that makes theoretical links between individuals’ skills, types of engagement with online services and activities, and the tangible outcomes achieved from this engagement. Our focus on this more holistic view, has led to a search for instruments that are capable of measuring which skills people have, how these are related to certain types of engagement and how these subsequently might impact specific aspects of everyday life. Such measures are essential in order to properly track who is or who is not digitally included, to assess the effectiveness of interventions designed to support digital inclusion and to provide better models of the relationships between Internet skills, engagement and outcomes. In this report, we focus on measurements for Internet skills. Further outputs, based on measures of engagements and outcomes, will follow later in 2014. The main research question is: What is the best set of reliable measures of Internet skills for use in research, practical, and policy impact evaluation settings? While nationally representative surveys are one of the most appropriate ways to collect data on Internet skills when testing generalizable models of digital inclusion, we have found four key challenges with the current measures available: 1) incompleteness – often only some skills are measured and digital skills related to more recent web 2.0 activities are not always fully explored; 2) conceptually blurred – as skills questions can be closely linked to Internet use (e.g. are you good at blogging / how often do you blog); 3) over-simplified – as Internet skills are often measured as a single dimension; and 4) reliant on self-reported measures that are context dependent and positively biased. The aim of this study is to propose a more elaborate conceptualization of Internet skills that aims to overcome these challenges, while taking into account the role skills play in a broader model of digital inclusion, and test the proposed scales for reliability and validity. In order to construct such an instrument, we took several steps. First, we conducted a systematic literature review of skills related studies, and developed our Internet skills framework and associated instrument based on this work (summarised in section 2). Then, we tested this instrument in three stages: cognitive interviews held in the UK and the Netherlands to refine the scales (section 3); online survey pilot tests of the instrument in the UK and in the Netherlands, to test the internal validity of the scales through both exploratory and confirmative factor analysis (section 4); and conducting a full survey in the Netherlands to test the skills framework for both internal and external validity (section 5). The concluding section (section 6) proposes two types of instruments for Internet skills: a short version and a more extensive version that could be used in future surveys. The focus on two countries, the UK and the Netherlands enabled the research team to begin to explore the cross-cultural validity of our proposed scale. ";Tangible outcomes of internet use: from digital skills to tangible outcomes project report;van Deursen, A., Helsper, E. & Eynon, R.;2014;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
This report from research commissioned by Becta investigates what, how and why people learn outside formal educational settings using technology. This study, The Learner in Context, explores the: ways in which many learners individually and as members of social groups manage and develop their engagement with digital technologies in order to adapt it to their needs sources of frustration, lack of skills and lack of opportunity that separate many other learners from the benefits of the same new technologies. This research is designed to inform the revised Harnessing Technology strategy1 and, in the first year, consists of three phases: Phase 1 involved analysis of existing literature on this topic and, at the Phases 2 and 3 of this research will explore these issues in greater depth and breadth. Phase 2 will consist of 40 case studies, carried out mainly in learners’ homes, and Phase 3 will involve a representative national survey of 1,000 learners across the age range. The case studies will be carried out with selected individuals from the initial interviews, and will provide an opportunity to explore in greater depth issues of skills, internet safety, family learning, formal and informal learning, and children in the care of children’s services. The national survey will enable the study to contextualise the findings from the first two phases in terms of the national picture, and provide greater insight into issues of entitlement, access and options for flexible learning for all learners. ;The learner and their context – Interim report: Benefits of ICT use outside formal education ;Davies, C., Carter, A. & Cranmer, S. et al.;2008;Eynon, R.;"Education, Digital Life and Wellbeing; Information Geography and Inequality"
"In October 2016, the White House, the European Parliament, and the UK House of Commons each issued a report outlining their visions on how to preparesociety for the widespread use of artiﬁcial intelligence (AI). In this article, weprovide a comparative assessment of these three reports in order to facilitate thedesign of policies favourable to the development of a ‘good AI society’. To do so,we examine how each report addresses the following three topics: (a) the devel-opment of a ‘good AI society’; (b) the role and responsibility of the government, theprivate sector, and the research community (including academia) in pursuing such adevelopment; and (c) where the recommendations to support such a developmentmay be in need of improvement. Our analysis concludes that the reports addressadequately various ethical, social, and economic topics, but come short of providingan overarching political vision and long-term strategy for the development of a‘good AI society’. In order to contribute to ﬁll this gap, in the conclusion we suggesta two-pronged approach.";Artificial Intelligence and the 'Good Society': the US, EU, and UK approach;Cath, C., Wachter, S. & Mittelstadt, B. et al.;2017;Cath, C.;"Ethics and Philosophy of Information; Digital Politics and Government; Information Governance and Security"
Internet engineering and networked systems research improves our understanding of the underlying technical processes of the Internet. Internet engineers therefore analyse data transfers on the Internet, typically by collecting data from devices of large groups of individuals as well as organisations. The designs of Internet engineering and research projects reflect human decisions and therefore may create new moral systems. This interplay of technology and society creates new practices that can impact the lives of individuals in many ways. These actions can raise new ethical dilemmas, or challenge existing ethics methodologies within the new and complex information environment presented by the Internet. To further the discussion on Internet research and engineering ethics, the Ethics in Networked System Research (“ESRN”) project hosted a workshop at Green Templeton College, University of Oxford, on 13 March 2015. The aim of the workshop was to understand how different disciplines involved in Internet research approach ethical dilemmas and justify their reasoning. To this end, a group of 25 researchers and practitioners from two distinct groups of researchers attended the workshop: (1) Computer scientists, network engineers and other technical researchers who have faced ethical and legal dilemmas in their work, and (2) philosophers, practical ethicists, legal philosophers, and related disciplines who are interested in Internet engineering and the ethical dilemmas posed by the Internet, but may not be aware of the details, subtleties, and dilemmas of the field. Several computer scientists gave short presentations about their projects, which were then discussed in-depth by the workshop participants. The inter-disciplinary discussions led to some interesting confrontations of cross-disciplinary reasoning. This is a perspectives paper, in which we present several of the cases discussed, as well as the reasoning applied by the different groups. The arguments made during the workshop reveal some underlying assumptions and values, which lead to some emerging themes that in turn uncover particular conceptual gaps between the disciplines. This paper is by no means intended to be a comprehensive overview of computer ethics or Internet research ethics, but merely an exploration of the themes that emerged during the workshop.;Philosophy Meets Internet Engineering: Ethics in Networked Systems Research;Zevenbergen, B., Mittelstadt, B. & Véliz, C. et al.;2015;Cath, C.;Ethics and Philosophy of Information
The debate on whether and how the Internet can protect and foster human rights has become a defining issue of our time. This debate often focuses on Internet governance from a regulatory perspective, underestimating the influence and power of the governance of the Internet’s architecture. The technical decisions made by Internet Standard Developing Organisations (SDOs) that build and maintain the technical infrastructure of the Internet influences how information flows. They rearrange the shape of the technically mediated public sphere, including which rights it protects and which practices it enables. In this article, we contribute to the debate on SDOs’ ethical responsibility to bring their work in line with human rights. We defend three theses. First, SDOs’ work is inherently political. Second, the Internet Engineering Task Force (IETF), one of the most influential SDOs, has a moral obligation to ensure its work is coherent with, and fosters, human rights. Third, the IETF should enable the actualisation of human rights through the protocols and standards it designs by implementing a responsibility-by-design approach to engineering. We conclude by presenting some initial recommendations on how to ensure that work carried out by the IETF may enable human rights.;The Design of the Internet's Architecture by the Internet Engineering Task Force (IETF) and Human Rights;Cath, C. & Floridi, L.;2017;Cath, C.;Ethics and Philosophy of Information
"Artificial agents (AAs), particularly but not only those in Cyberspace, extend the class of entities that can be involved in moral situations. For they can be conceived of as moral patients (as entities that can be acted upon for good or evil) and also as moral agents (as entities that can perform actions, again for good or evil). In this paper, we clarify the concept of agent and go on to separate the concerns of morality and responsibility of agents (most interestingly for us, of AAs). We conclude that there is substantial and important scope, particularly in Computer Ethics, for the concept of moral agent not necessarily exhibiting free will, mental states or responsibility. This complements the more traditional approach, common at least since Montaigne and Descartes, which considers whether or not (artificial) agents have mental states, feelings, emotions and so on. By focussing directly on ‘mind-less morality’ we are able to avoid that question and also many of the concerns of Artificial Intelligence. A vital component in our approach is the ‘Method of Abstraction’ for analysing the level of abstraction (LoA) at which an agent is considered to act. The LoA is determined by the way in which one chooses to describe, analyse and discuss a system and its context. The ‘Method of Abstraction’ is explained in terms of an ‘interface’ or set of features or observables at a given ‘LoA’. Agenthood, and in particular moral agenthood, depends on a LoA. Our guidelines for agenthood are: interactivity (response to stimulus by change of state), autonomy (ability to change state without stimulus) and adaptability (ability to change the ‘transition rules’ by which state is changed) at a given LoA. Morality may be thought of as a ‘threshold’ defined on the observables in the interface determining the LoA under consideration. An agent is morally good if its actions all respect that threshold; and it is morally evil if some action violates it. That view is particularly informative when the agent constitutes a software or digital system, and the observables are numerical. Finally we review the consequences for Computer Ethics of our approach. In conclusion, this approach facilitates the discussion of the morality of agents not only in Cyberspace but also in the biosphere, where animals can be considered moral agents without their having to display free will, emotions or mental states, and in social contexts, where systems like organizations can play the role of moral agents. The primary ‘cost’ of this facility is the extension of the class of agents and moral agents to embrace AAs. Artificial agents, particularly but not only those in Cyberspace, extend the class of entities that can be involved in moral situations. Moral situations commonly involve agents and patients. Complex biochemical compounds and abstruse mathematical concepts have at least one thing in common: they may be unintuitive, but once understood they are all definable with total precision, by listing a finite number of necessary and sufficient properties. The idea of a ‘level of abstraction’ plays an absolutely crucial role in the previous account. The term variable is commonly used throughout science for a symbol that acts as a place-holder for an unknown or changeable referent. A LoA qualifies the level at which an entity is considered. In this paper, the people follow the Method of Abstraction and insist that each LoA be made precise before the properties of the entity can sensibly be discussed.";On the Morality of Artificial Agents;Floridi, L. & Sanders, J.;2017;Floridi, L.;Ethics and Philosophy of Information
"On the 8th of February 2013, The Onlife Manifesto  was released at an inaugural event held in Brussels by DG Connect, the European Commission Directorate General for Communications Networks, Content & Technology. The Manifesto was the outcome of the work of a group of scholars, organised by DG Connect, which I had the privilege to chair: Stefana Broadbent, Nicole Dewandre, Charles Ess, Jean-Gabriel Ganascia, Mireille Hildebrandt, Yiannis Laouris, Claire Lobet-Maris, Sarah Oates, Ugo Pagallo, Judith Simon, May Thorseth, and Peter-Paul Verbeek. During the previous year, we had worked quite intensely on a project entitled The Onlife Initiative: concept reengineering for rethinking societal concerns in the digital transition. We decided to adopt the neologism “onlife” that I had coined in the past in order to refer to the new experience of a hyperconnected reality within which it is no longer sensible to ask whether one may be online or offline. Also thanks to a series of workshops organised by DG Connect, we had investigated the challenges brought about by the new digital technologies. We had debated the impact that ICTs are having on human life, and hence how one may re-engineer key concepts—such as attention, ownership, privacy , and responsibility—that are essential in order to gain the relevant and adequate framework within which our onlife experience may be understood and improved. In the course of our investigations, we soon realised that the output of our efforts would have been more fruitful by summarising it in a short document—which soon became known as The Onlife Manifesto —and a series of short commentaries (volunteered by some of us) and longer essays (contributed by each of us) that would explain and position The Manifesto within the current debates on Information and Communication Technologies (ICTs). The inaugural event represented the official opening of the public discussion of our work. Many more public meetings and international presentations followed.4 As a result, this book is actually a synthesis of the research done in 2012 and the feedback received in 2013. The book is organised in such a way as to give priority to The Onlife Manifesto . This is the document around which the rest of the book revolves. It is followed by eight short commentaries by Ess, my self, Ganascia, Hildebrandt, Laouris, Pagallo, Simon, and Thorseth. The next chapter is the background document. This contains the material that was used to start and frame the conversations during the initial phases of the project. There follow 12 chapters. In them, members of the group, myself included, have presented some of the ideas that guided our contribution to the Manifesto. Although each chapter may be read independently of the rest of the book, it is a modular part of the scaffolding that led to the Manifesto. A short conclusion, which is more a “to be continued”, ends the book. In terms of authorship, any material that is not explicitly attributed to some author is to be attributed to the whole group, as a collaborative work, endorsed by each of us. So much for the outline of the project. I shall not add any further details because these can be found in the background document. In terms of an overview of the book’s contents, in the following pages we argue that the development and widespread use of ICTs are having a radical impact on the human condition . More specifically, we believe (see the Preface that introduces The Manifesto) that ICTs are not mere tools but rather environmental forces that are increasingly affecting: 1. our self-conception (who we are); 2. our mutual interactions (how we socialise); 3. our conception of reality (our metaphysics); and 4. our interactions with reality (our agency). In each case, ICTs have a huge ethical, legal, and political significance, yet one with which we have begun to come to terms only recently. We are also convinced that the aforementioned impact exercised by ICTs is due to at least four major transformations: a. the blurring of the distinction between reality and virtuality; b. the blurring of the distinction between human, machine and nature; c. the reversal from information scarcity to information abundance; and d. the shift from the primacy of stand-alone things, properties, and binary relations, to the primacy of interactions, processes and networks. The impact summarised in (1)–(4) and the transformations behind such an impact, listed in (a)–(d), are testing the foundations of our philosophy, in the following sense. Our perception and understanding of the realities surrounding us are necessarily mediated by concepts. These work like interfaces through which we experience, interact with, and semanticise (in the sense of making sense of, and giving meaning to), the world. In short, we grasp reality through concepts, so, when reality changes too quickly and dramatically, as it is happening nowadays because of ICTs, we are conceptually wrong-footed. It is a widespread impression that our current conceptual toolbox is no longer fitted to address new ICT-related challenges. This is not only a problem in itself. It is also a risk, because the lack of a clear conceptual grasp of our present time may easily lead to negative projections about the future: we fear and reject what we fail to semanticise. The goal of The Manifesto, and of the rest of the book that contextualises, is therefore that of contributing to the update of our conceptual framework. It is a constructive goal. We do not intend to encourage a philosophy of mistrust. On the contrary, this book is meant to be a positive contribution to rethinking the philosophy on which policies are built in a hyperconnected world, so that we may have a better chance of understanding our ICT-related problems and solving them satisfactorily. Redesigning or reengineering our hermeneutics, to put it more dramatically, seems essential, in order to have a good chance of understanding and dealing with the transformations in (a)–(d) and hence shape in the best way the novelties in (1)–(4). It is clearly an enormous and ambitious task, to which this book can only aspire to contribute.";The Onlife Manifesto: Introduction;Floridi, L.;2014;Floridi, L.;Ethics and Philosophy of Information
The post-Westphalian Nation State developed by becoming more and more an Information Society. However, in so doing, it progressively made itself less and less the main information agent, because one of the main forces that made the Nation State possible and then predominant, as a historical driving force in human politics, namely Information and Communication Technologies (ICTs), is also what is now making it less central, in the social, political and economic life of humanity across the world. ICTs enable and promote the agile, temporary and timely aggregation, disaggregation and re-aggregation of distributed () groups around shared interests across old, rigid boundaries represented by social classes, political parties, ethnicity, language barriers, physical and geographical barriers, and so forth. Similar novelties call for a serious exercise in conceptual re-engineering. We need to understand how the new informational multiagent systems may be designed in such a way as to take full advantage of the socio-political progress made so far, while being able to deal successfully with the new global challenges (from the environment to the financial markets) that are undermining the legacy of that very progress.;Hyperhistory and the Philosophy of Information Policies;Floridi, L.;2014;Floridi, L.;Ethics and Philosophy of Information
Artificial agents, particularly but not only those in the infosphere Floridi (Information – A very short introduction. Oxford University Press, Oxford, 2010a), extend the class of entities that can be involved in moral situations, for they can be correctly interpreted as entities that can perform actions with good or evil impact (moral agents). In this chapter, I clarify the concepts of agent and of artificial agent and then distinguish between issues concerning their moral behaviour vs. issues concerning their responsibility. The conclusion is that there is substantial and important scope, particularly in information ethics, for the concept of moral artificial agents not necessarily exhibiting free will, mental states or responsibility. This complements the more traditional approach, which considers whether artificial agents may have mental states, feelings, emotions and so forth. By focussing directly on “mind-less morality”, one is able to by-pass such question as well as other difficulties arising in Artificial Intelligence, in order to tackle some vital issues in contexts where artificial agents are increasingly part of the everyday environment (Floridi L, Metaphilos 39(4/5): 651–655, 2008a).;Artificial Agents and Their Moral Nature;Floridi, L.;2013;Floridi, L.;Ethics and Philosophy of Information
The post-Westphalian Nation State developed by becoming more and more an Information Society. However, in so doing, it progressively made itself less and less the main information agent, because what made the Nation State possible and then predominant, as a historical driving force in human politics, namely ICTs, is also what is now making it less central, in the social, political and economic life of humanity across the world. ICTs fluidify the topology of politics. They do not merely enable but actually promote (through management and empowerment) the agile, temporary and timely aggregation, disaggregation and re-aggregation of distributed groups around shared interests across old, rigid boundaries represented by social classes, political parties, ethnicity, language barriers, physical barriers, and so forth. This is generating a new tension between the Nation State, still understood as a major organisational institution, yet no longer monolithic but increasingly morphing into a multiagent system itself, and a variety of equally powerful, indeed sometimes even more politically influential and powerful, non-Statal organisations. Geo-politics is now global and increasingly non-territorial, but the Nation State still defines its identity and political legitimacy in terms of a sovereign territorial unit, as a Country. Such tension calls for a serious exercise in conceptual re-engineering: how should the new informational multiagent systems (MASs) be designed in such a way as to take full advantage of the socio-political progress made so far, while being able to deal successfully with the new global challenges (from the environment to the financial markets) that are undermining the legacy of that very progress? In the lecture, I shall defend an answer to this question in terms of a design of political MAS based on principles borrowed from information ethics.;The Rise of the MASs;Floridi, L.;2014;Floridi, L.;Ethics and Philosophy of Information
This paper is divided into two parts. In the first, I shall briefly analyse the phenomenon of “big data”, and argue that the real epistemological challenge posed by the zettabyte era is small patterns. The valuable undercurrents in the ocean of data that we are accumulating are invisible to the computationally-naked eye, so more and better technology will help. However, because the problem with big data is small patterns, ultimately, the game will be won by those who “know how to ask and answer questions” (Plato, Cratylus, 390c). This introduces the second part, concerning information quality (IQ): which data may be useful and relevant, and so worth collecting, curating, and querying, in order to exploit their valuable (small) patterns? I shall argue that the standard way of seeing IQ in terms of fit-for-purpose is correct but needs to be complemented by a methodology of abstraction, which allows IQ to be indexed to different purposes. This fundamental step can be taken by adopting a bi-categorical approach. This means distinguishing between purpose/s for which some information is produced (P-purpose) and purpose/s for which the same information is consumed (C-purpose). Such a bi-categorical approach in turn allows one to analyse a variety of so-called IQ dimensions, such as accuracy, completeness, consistency, and timeliness. I shall show that the bi-categorical approach lends itself to simple visualisations in terms of radar charts.;Big Data and Information Quality;Floridi, L.;2014;Floridi, L.;Ethics and Philosophy of Information
The topic of this paper may be introduced by fast zooming in and out of the philosophy of information. In recent years, philosophical interest in the nature of information has been increasing steadily. This has led to a focus on semantic information, and then on the logic of being informed, which has attracted analyses concentrating both on the statal sense in which Sholds the information that p (this is what I mean by logic of being informed in the rest of this article) and on the actional sense in which S becomes informed that p. One of the consequences of the logic debate has been a renewed epistemological interest in the principle of information closure (henceforth PIC), which finally has motivated a revival of a sceptical objection against its tenability first made popular by Dretske. This is the topic of the paper, in which I seek to defend PIC against the sceptical objection. If I am successful, this means – and we are now zooming out – that the plausibility of PIC is not undermined by the sceptical objection, and therefore that a major epistemological argument against the formalization of the logic of being informed based on the axiom of distribution in modal logic is removed. But since the axiom of distribution discriminates between normal and non-normal modal logics, this means that a potentially good reason to look for a formalization of the logic of being informed among the non-normal modal logics, which reject the axiom, is also removed. And this in turn means that a formalization of the logic of being informed in terms of the normal modal logic B (also known as KTB) is still very plausible, at least insofar as this specific obstacle is concerned. In short, I shall argue that the sceptical objection against PIC fails, so it is not a good reason to abandon the normal modal logic B as a good formalization of the logic of being informed.;A Defence of the Principle of Information Closure against the Sceptical Objection;Floridi, L.;2013;Floridi, L.;Ethics and Philosophy of Information
The history of the transmission, recovery and posthumous influence of ancient scepticism is a fascinating chapter in the history of ideas. An extraordinary collection of philosophical texts and some of the most challenging arguments ever devised were first lost, then only partly recovered philologically, and finally rediscovered conceptually, leaving Cicero and Sextus Empiricus as the main champions of Academic and Pyrrhonian scepticism respectively. This chapter outlines what we know about this shipwreck and what was later salvaged from it. It cannot provide many details, given its length. And, being a review, it does not try to solve the many puzzles and mysteries still unsolved. But, as an introduction, it does seek to give a general idea of what happened to ancient scepticism in the long span of time occurring between Augustine and Descartes. It covers a dozen centuries of Western philosophy, so a few generalizations, some schematism and a good degree of abstraction from specific information will be inevitable.;The rediscovery and posthumous influence of scepticism;Floridi, L.;2010;Floridi, L.;Ethics and Philosophy of Information
The technology for Artificial Companions is already largely available, and the question is when rather than whether ACs will become commodities (Benyon and Mival (2007). The difficulties are still formidable, but they are not insurmountable. On the contrary, they seem rather well-understood, and the path from theoretical problems to technical solutions looks steep but climbable. cIn the following pages, I wish to concentrate not on the technological challenges, which are important, but on some philosophical issues that a growing population of AC will make increasingly pressing.;Artificial Companions and their philosophical challenges;Floridi, L.;2010;Floridi, L.;Ethics and Philosophy of Information
In Floridi (2005), I argued that a definition of semantic information in terms of alethically-neutral content – that is, strings of well-formed and meaningful data that can be additionally qualified as true or untrue (false, for the classicists among us), depending on supervening evaluations – provides only necessary but insufficient conditions: if some content is to qualify as semantic information, it must also be true. One speaks of false information in the same way as one qualifies someone as a false friend, i.e. not a friend at all. According to it, semantic information is, strictly speaking, inherently truth-constituted and not a contingent truth-bearer, exactly like knowledge but unlike propositions or beliefs, for example, which are what they are independently of their truth values and then, because of their truth-aptness, may be further qualified alethically.;How to Account for Information;Floridi, L.;2010;Floridi, L.;Ethics and Philosophy of Information
The article argues that Information Ethics (IE) can provide a successful approach for coping with the challenges posed by our increasingly globalized reality. After a brief review of some of the most fundamental transformations brought about by the phenomenon of globalization, the article distinguishes between two ways of understanding Global Information Ethics, as an ethics of global communication or as a global-information ethics. It is then argued that cross-cultural, successful interactions among micro and macro agents call for a high level of successful communication, that the latter requires a shared ontology friendly towards the implementation of moral actions, and that this is provided by IE. There follows a brief account of IE and of the ontic trust, the hypothetical pact between all agents and patients presupposed by IE.;Global Information Ethics: The Importance of Being Environmentally Earnest;Floridi, L.;2009;Floridi, L.;Ethics and Philosophy of Information
"A frequent complaint about current theories of information1 is that they are utterly useless when it comes to establishing the actual relevance of some specific piece of information. As a rule, agents assume that some content is by default an instance of information (Sperber and Wilson [1995]). What they often wonder is whether and how far that content may contribute to the formulation of their choices and purposes, the development of their deci- sion processes and eventually the successful pursuit of their goals. In light of this problem, this paper pursues two goals. The first is to pro- vide a subjectivist interpretation of epistemic relevance (i.e. epistemically relevant semantic information, more on this presently), thus satisfying those critics who lament its absence and, because of it, may be sceptical about the utility of using information-theoretical concepts to tackle conceptual prob- lems and cognitive issues in real life. The second goal is to show that such a subjectivist interpretation can (indeed must) be built on a veridical concep- tion of semantic information, thus vindicating a strongly semantic theory of information (Floridi [2004b]) and proving wrong those critics who argue that misinformation can be relevant. The two goals are achieved through a strategy of progressive refine- ments. In § 2, the distinction between system-based or causal and agent-ori- ented or epistemic relevance is introduced. In § 3, I discuss the most com- mon and basic sense in which semantic information is said to be epistemically relevant. This has some serious shortcomings, so, in § 4, the basic case is refined probabilistically. The new version too can be shown to be only partly satisfactory, so in § 5 there will be a second, counterfactual revision. The limits of this version are finally overcome in § 6, where the analysis is completed by providing a conclusive, meta-informational refine- ment. In § 7, some of the advantages of the metatheoretical revision are illustrated. In § 8, I briefly outline some important applications of what I shall label the subjectivist interpretation of epistemic relevance. In § 9, I return to the problem of the connection between a strongly semantic theory of information and the concept of epistemic relevance and explain why mis- information cannot be relevant. In § 10, I conclude by briefly summarising the results obtained and the possible work that lies ahead. A final warning before starting: “information” can mean many things (Floridi [2004a]; Floridi [2005a]). In what follows, I concentrate only on information understood as semantic information about reality, i.e. factual information with an epistemic or cognitive value. A train timetable, a the- ory in a physics book, the map of the London underground, a police report about a road accident, the description of Peter’s breakfast, the bell ringing when someone is at the door, are all typical illustrations that may be kept in mind.";A Subjectivist Interpretation of Relevant Information;Floridi, L.;2007;Floridi, L.;Ethics and Philosophy of Information
“I love information upon all subjects that come in my way, and especially upon those that are most important.” Thus boldly declares Euphranor, one of the defenders of Christian faith in Berkley’s Alciphron. Evidently, information has been an object of philosophical desire and puzzlement for some time, well before the computer revolution, Internet or the dot.com pandemonium. Yet what does Euphranor love, exactly? What is information? As with many other field-questions (consider for example “what is being?”, “what is morally good?” or “what is knowledge?”), “what is information?” is to be taken not as a request for a dictionary definition, but as a means to demarcate a wide area of research. The latter has recently been defined as the philosophy of information (Floridi [2002], Floridi [2003b]). The task of this chapter is to review some interesting research trends in the philosophy of information (henceforth also PI). This will be achieved in three steps. We shall first look at a definition of PI. On this basis, we shall then consider a series of open problems in PI on which philosophers are currently working. The conclusion will then highlight the innovative character of this new area of research. ;TRENDS IN THE PHILOSOPHY OF INFORMATION;Floridi, L.;2008;Floridi, L.;Ethics and Philosophy of Information
Information Ethics (IE) has come to mean different things to different researchers working in a variety of disciplines, including computer ethics, business ethics, medical ethics, computer science and information science. The chapter explores what IE is and what counts as a moral agent and moral patient according to IE. It questions our responsibilities as moral agents, according to IE and the fundamental principles of IE. The resource-product-target (RPT) model, summarized in this chapter, helps one to get some initial orientation in the multiplicity of issues belonging to different interpretations of IE. The chapter also gives an overview of Information Ethics understood as a macroethics. Since the early nineties, when the author first introduced IE as an environmental macroethics and a foundationalist approach to computer ethics, some standard objections have circulated that seem to be based on a few basic misunderstandings.;Information ethics;Floridi, L.;2010;Floridi, L.;Ethics and Philosophy of Information
The previous chapters have provided a detailed overview of the variety of ethical challenges posed by the development of ICTs. By way of conclusion, in this epilogue I would like to invite the reader to look into the possible future of Information and Computer Ethics. More specifically, I shall try to forecast how the convergence of two fundamental trends of our times, globalization and the development of the information society, may interact with the ethical problems analysed in this book. The exercise will not be based on some untenable technological determinism. Humanity is, and will remain, firmly in charge of its destiny and hence be responsible for it. Rather, it will mean adopting the farmer's view that, with enough intelligence, toil and a bit of luck, one might be able to tell today what one will probably reap tomorrow. Before trying to ‘look into the seeds of time, and say which grain will grow and which will not’ (Shakespeare, Macbeth, Act I, Scene III, 59–62), two clarifications might be in order. First, the future of globalization is a phenomenon too complex even to sketch in this brief epilogue. For a synthetic, well-balanced and informed overview, the reader may wish to consult Held and McGrew (2001) and consider that this chapter is written from what Held et al. (1999)) have defined as a ‘transformationalist perspective’, according to which ‘globalization does not simply denote a shift in the extensity or scale of social relations and activity.;Epilogue: The ethics of the information society in a globalized world;Floridi, L.;2010;Floridi, L.;Ethics and Philosophy of Information
This chapter discusses some conceptual undercurrents, which flow beneath the surface of the literature on information and computer ethics (ICE). It focuses on the potential impact of Information and Communication Technologies (ICTs) on our lives. Because of their 'data superconductivity', ICTs are well known for being among the most influential factors that affect the ontological friction in the infosphere. As a full expression of techne, the information society has already posed fundamental ethical problems, whose complexity and global dimensions are rapidly evolving. The task is to formulate an ethical framework that can treat the infosphere as a new environment worth the moral attention and care of the human inforgs inhabiting it. We have begun to see ourselves as connected informational organisms (inforgs), not through some fanciful transformation in our body, but, more seriously and realistically, through the re-ontologization of our environment and of ourselves.;Ethics after the Information Revolution;Floridi, L.;2010;Floridi, L.;Ethics and Philosophy of Information
In this paper I shall discuss some consequences of robots’ increasingly smart, autonomous, and social behaviour. The main thesis supported is that robotics (and AI in general) offers a historical opportunity to rethink human exceptionalism in at least three ways. Intelligent behaviour is confronted by smart behaviour, which can be adaptively more successful in the infosphere. Free behaviour is confronted by the predictability and manipulability of human choices, and by the development of artificial autonomy. And human sociability is confronted by its artificial counterpart, which can be both attractive for humans and indistinguishable by them. In the conclusion, I shall suggest that the development of artificial agents does not lead to any fanciful realization of science fiction scenario, which are irresponsibly distracting. It rather invites us to reflect more seriously and less complacently on who we are, could be, and would like to become. I shall suggest that human exceptionalism is not incorrect but that it lies in a special and perhaps irreproducible way of being successfully dysfunctional. We are a glitch in the natural system, not the ultimate app. We shall remain a bug, while robots will be more and more a feature in Galileo’s mathematical book of nature. ;Smart, Autonomous, and Social: Robots As Challenge to Human Exceptionalism;Floridi, L.;2014;Floridi, L.;Ethics and Philosophy of Information
At least since the sixties, there have been many attempts to understand the logic of design, when the latter is broadly understood as a purposeful way of realizing an artefact. In this talk, I shall explore current methodologies to see how they may be adapted to cases in which what is being designed is information, in the sense of both a semantic artefact (e.g. a train timetable) and a communication process (e.g. the announcement that a specific train is leaving from particular platform).;Keynote talk: the logic of information design;Floridi, L.;2014;Floridi, L.;Ethics and Philosophy of Information
"Information and Communication Technologies (ICTs) have profoundly altered many aspects of life, including the nature of entertainment, work, communication, education, health care, industrial production and business, social relations and conflicts. As a consequence, they have had a radical and widespread impact on our moral lives and hence on contemporary ethical debates. Consider the following list: PAPA (privacy, accuracy, intellectual property and access); ‘the triple A’ (availability, accessibility and accuracy of information); ownership and piracy; the digital divide; infoglut and research ethics; safety, reliability and trustworthiness of complex systems; viruses, hacking and other forms of digital vandalism; freedom of expression and censorship; pornography; monitoring and surveillance; security and secrecy; propaganda; identity theft; the construction of the self; panmnemonic issues and personal identity; new forms of agency (artificial and hybrid), of responsibility and accountability; roboethics and the moral status of artificial agents; e-conflicts; the re-prioritization of values and virtues…these are only some of the pressing issues that characterize the ethical discourse in our information societies. They are the subject of information and computer ethics (ICE), a new branch of applied ethics that investigates the transformations brought about by ICTs and their implications for the future of human life and society, for the evolution of moral values and rights, and for the evaluation of agents' behaviours. Since the seventies, ICE has been a standard topic in many curricula. In recent years, there has been a flourishing of new university courses, international conferences, workshops, professional organizations, specialized publications and research centres.";The Cambridge Handbook of Information and Computer Ethics: Preface;Floridi, L.;2010;Floridi, L.;Ethics and Philosophy of Information
This article examines the problem of categorising dimensions of information quality (IQ), against the background of a serious engagement with the hypothesis that IQ is purpose-relative. We examine some attempts to offer categories for IQ, and diagnose a specific problem that impedes convergence in such categorisations. Based on this new understanding, we suggest a new way of categorising both IQ dimensions and the metrics used in im- plementation of IQ improvement programmes according to what they are properties of. We conclude by outlining an initial categorisation of some IQ dimensions and metrics in standard use to illustrate the value of the approach. ;IQ: PURPOSE AND DIMENSIONS ;Illari, P. & Floridi, L.;2012;Floridi, L.;Ethics and Philosophy of Information
Deductive inference is usually regarded as being “tautological” or “analytical”: the information conveyed by the conclusion is contained in the information conveyed by the premises. This idea, however, clashes with the undecidability of first-order logic and with the (likely) intractability of Boolean logic. In this article, we address the problem both from the semantic and the proof-theoretical point of view. We propose a hierarchy of propositional logics that are all tractable (i.e. decidable in polynomial time), although by means of growing computational resources, and converge towards classical propositional logic. The underlying claim is that this hierarchy can be used to represent increasing levels of “depth” or “informativeness” of Boolean reasoning. Special attention is paid to the most basic logic in this hierarchy, the pure “intelim logic”, which satisfies all the requirements of a natural deduction system (allowing both introduction and elimination rules for each logical operator) while admitting of a feasible (quadratic) decision procedure. We argue that this logic is “analytic” in a particularly strict sense, in that it rules out any use of “virtual information”, which is chiefly responsible for the combinatorial explosion of standard classical systems. As a result, analyticity and tractability are reconciled and growing degrees of computational complexity are associated with the depth at which the use of virtual information is allowed.;The enduring scandal of deduction: is propositional logic really uninformative?;D'Agostino, M. & Floridi, L.;2009;Floridi, L.;Ethics and Philosophy of Information
I love information upon all subjects that come in my way, and especially upon those that are most important. Thus boldly declares Euphranor, one of the defenders of Christian faith in Berkley’s Alciphron(Berkeley, (1732), Dialogue 1, Section 5, Paragraph 6/10). Evidently, information has been an object of philosophical desire for some time, well before the computer revolution, Internet or the dot.com pandemonium (see for example Dunn (2001) and Adams (2003)). Yet what does Euphranor love, exactly? What is information? The question has received many answers in different fields. Unsurprisingly, several surveys do not even converge on a single, unified definition of information (see for example Braman 1989, Losee (1997), Machlup and Mansfield (1983), Debons and Cameron (1975), Larson and Debons (1983).;Philosophical Conceptions of Information;Floridi, L.;2009;Floridi, L.;Ethics and Philosophy of Information
"The Philosophy of Information is a new area of research at the intersection of phi- losophy and computer science. It concerns (a) the critical investigation of the conceptual nature and basic principles of information, including its dynamics (es- pecially computation), utilization (especially computer ethics) and sciences; and (b) the elaboration and application of computational and information-theoretic methodologies to philosophical problems. Past work by members of our group has concentrated on (a), and in this paper we explore (b). In a nutshell, we ask what computer science can do for philosophy, rather than what the latter can do for the former. Applications of computational methods to philosophical issues may be ap- proached in three main ways: 1. Conceptual experiments in silico, or the externalization of the mental the- ater. As Patrick Grim has remarked “since the eighties, philosophers too have begun to apply computational modeling to questions in logic, epistemology, philosophy of science, philosophy of mind, philosophy of language, philoso- phy of biology, ethics, and social and political philosophy. [...] A number of authors portray computer experimentation in general as a technological extension of an ancient tradition of thought experiment”. Pancomputationalism, or the fallacy of a powerful metaphor. According to this view, computational and informational concepts are so powerful that, given the right Level of Abstraction, anything could be pre- sented as a computational system, from a building to a volcano, from a forest to a dinner, from a brain to a company, and any process could be simulated computationally heating, flying and knitting. Even non-computable func- tions would be representable, although by abstracting them to such a high level that they would no longer count as a system (one would have to abstract output and even termination and the existence of output, but a system has to be allowed to terminate or not, even if one does not observe the output). But then pancomputationalists have the hard task of providing a credible answers to the following two questions: how can one avoid blurring all differences among systems, thus transforming pancomputationalism into a night in which all cows are black, to paraphrase Hegel? And what would it mean for the system under investigation not to be an informational sys- tem (or a computational system, if computation = information processing)? Pancomputationalism does not seem vulnerable to a refutation, in the form of a possible counterexample in a world nomically identical to the one to which pancomputationalism is applied. 3. Regulae ad directionem ingenii, or the Cartesian-Kantian approach. Are there specific methods in computer science that can help us to approach philosophical problems computationally? In the following sections we answer this last question by introducing three main methods: Minimalism, the Method of Abstraction and Constructionism. Each one is discussed in a separate section. ";The Philosophy of Information: A Methodological Point of View ;Greco, G., Paronitti, G. & Turilli, M. et al.;2005;Floridi, L.;Ethics and Philosophy of Information
"On September 28, 2020, at the Next Generation Internet Summit, Helsinki and Amsterdam announced the launch of their open AI registers. They are the first cities to offer such a service in the world (City of Helsinki 2020). The AI registers describe what, where, and how AI applications are being used in the two municipalities; which datasets were used for training purposes; how algorithms were assessed for potential bias or risks; and how humans use the AI services. The registers also offer a feedback channel, which is meant to enable more participation, with information about the city department and the person responsible for the AI service. The goal is to make the use of urban AI solutions as responsible, transparent, and secure as other local government activities, to improve services and citizens’ experiences. The AI registers are currently being populated. Anyone can check them. At the time of writing, there are 5 AI services available in the Helsinki AI Register and 3 in the Amsterdam AI Register. The plan is eventually to have all the cities’ AI services listed in the registers. At the moment, eight services are not many, but, despite their still limited number, the overall project is extremely interesting for several reasons, and one can learn a few lessons from it. Let us see them.";Artificial Intelligence as a Public Service: Learning from Amsterdam and Helsinki;Floridi, L.;2020;Floridi, L.;Ethics and Philosophy of Information
Digital sovereignty seems to be something very important, given the popularity of the topic these days. True. But it also sounds like a technical issue, which concerns only specialists. False. Digital sovereignty, and the fight for it, touch everyone, even those who do not have a mobile phone or have never used an online service. To understand why, let me start with four episodes. I shall add a fifth shortly.;The Fight for Digital Sovereignty: What It Is, and Why It Matters, Especially for the EU;Floridi, L.;2020;Floridi, L.;Ethics and Philosophy of Information
"This article presents a mapping review of the literature concerning the ethics of artificial intelligence (AI) in health care. The goal of this review is to summarise current debates and identify open questions for future research. Five literature databases were searched to support the following research question: how can the primary ethical risks presented by AI-health be categorised, and what issues must policymakers, regulators and developers consider in order to be ‘ethically mindful? A series of screening stages were carried out—for example, removing articles that focused on digital health in general (e.g. data sharing, data access, data privacy, surveillance/nudging, consent, ownership of health data, evidence of efficacy)—yielding a total of 156 papers that were included in the review. We find that ethical issues can be (a) epistemic, related to misguided, inconclusive or inscrutable evidence; (b) normative, related to unfair outcomes and transformative effectives; or (c) related to traceability. We further find that these ethical issues arise at six levels of abstraction: individual, interpersonal, group, institutional, and societal or sectoral. Finally, we outline a number of considerations for policymakers and regulators, mapping these to existing literature, and categorising each as epistemic, normative or traceability-related and at the relevant level of abstraction. Our goal is to inform policymakers, regulators and developers of what they must consider if they are to enable health and care systems to capitalise on the dual advantage of ethical AI; maximising the opportunities to cut costs, improve care, and improve the efficiency of health and care systems, whilst proactively avoiding the potential harms. We argue that if action is not swiftly taken in this regard, a new ‘AI winter’ could occur due to chilling effects related to a loss of public trust in the benefits of AI for health care.";The ethics of AI in health care: A mapping review;Morley, J., Machado, C. & Burr, C. et al.;2020;Floridi, L.;Ethics and Philosophy of Information
"Since 2016, social media companies and news providers have come under pressure to tackle the spread of political mis- and disinformation (MDI) online. However, despite evidence that online health MDI (on the web, on social media, and within mobile apps) also has negative real-world effects, there has been a lack of comparable action by either online service providers or state-sponsored public health bodies. We argue that this is problematic and seek to answer three questions: why has so little been done to control the flow of, and exposure to, health MDI online; how might more robust action be justified; and what specific, newly justified actions are needed to curb the flow of, and exposure to, online health MDI? In answering these questions, we show that four ethical concerns—related to paternalism, autonomy, freedom of speech, and pluralism—are partly responsible for the lack of intervention. We then suggest that these concerns can be overcome by relying on four arguments: (1) education is necessary but insufficient to curb the circulation of health MDI, (2) there is precedent for state control of internet content in other domains, (3) network dynamics adversely affect the spread of accurate health information, and (4) justice is best served by protecting those susceptible to inaccurate health information. These arguments provide a strong case for classifying the quality of the infosphere as a social determinant of health, thus making its protection a public health responsibility. In addition, they offer a strong justification for working to overcome the ethical concerns associated with state-led intervention in the infosphere to protect public health.";Public Health in the Information Age: Recognizing the Infosphere as a Social Determinant of Health;Morley, J., Cowls, J. & Taddeo, M. et al.;2020;Floridi, L.;Ethics and Philosophy of Information
"Technologies to rapidly alert people when they have been in contact with someone carrying the coronavirus SARS-CoV-2 are part of a strategy to bring the pandemic under control. Currently, at least 47 contact-tracing apps are available globally (see go.nature.com/2zc1qhk). They are already in use in Australia, South Korea and Singapore, for instance. And many other governments are testing or considering them. Here we set out 16 questions to assess whether — and to what extent — a contact-tracing app is ethically justifiable. These questions could assist governments, public-health agencies and providers to develop ethical apps — they have already informed developments in France, Italy and the United Kingdom. They will also help watchdogs and others to scrutinize such technologies. What do COVID-19 contact-tracing apps do? Running on a mobile phone, they inform people that they have spent time near someone with the virus. The contacts should then respond according to local rules, for example by isolating themselves. Prompt alerts are key because the incubation time of the virus is up to two weeks. These digital interventions come at a price. Collecting sensitive personal data potentially threatens privacy, equality and fairness. Even if COVID-19 apps are temporary, rapidly rolling out tracing technologies runs the risk of creating permanent, vulnerable records of people’s health, movements and social interactions, over which they have little control. More ethical oversight is essential. So far, such concerns have focused on rights to privacy (see go.nature.com/3e7jntx). Some governments have pledged to protect data privacy (see go.nature.com/3grwfe8). Apple and Google are developing a common interface to support apps that do not require central data storage (see Naturehttp://doi.org/dwc6; 2020). However, other ethical and social considerations must not be cast aside in the rush to quell the pandemic. For instance, contact-tracing apps should be available and accessible to anyone, irrespective of the technology needed or their level of digital literacy. Yet many apps work only with certain phones. Australia, for example, has no plans to make its app work with phones that use software older than Apple’s iOS 10 or Android 6.0. In the United Kingdom, around one-fifth of adults do not use a smartphone, and so might be excluded from a digital contact-tracing programme. Rolling out an app without considering its wide ethical and social implications can be dangerous, costly and useless. For example, Bluetooth signals that show the proximity of two individuals’ mobile phones are not a certain indicator of infection risk — two people might be in the same space but physically separated, for example, by a wall. A high level of false positives from such an app (for instance, as a result of self-reporting) could lead to unjustified panic. And minimal protections against false negatives (people not using the app to report that they are unwell) could spur a false sense of safety in others and increase the risk of infection.";Ethical guidelines for COVID-19 tracing apps;Morley, J., Cowls, J. & Taddeo, M. et al.;2020;Floridi, L.;Ethics and Philosophy of Information
The World Health Organisation declared COVID-19 a global pandemic on 11th March 2020, recognising that the underlying SARS-CoV-2 has caused the greatest global crisis since World War II. In this article, we present a framework to evaluate whether and to what extent the use of digital systems that track and/or trace potentially infected individuals is not only legal but also ethical. Digital tracking and tracing (DTT) systems may severely limit fundamental rights and freedoms, but they ought not to be deployed in a vacuum of guidance, to ensure that they are ethically justifiable, i.e. coherent with society’s expectations and values. Interventions must be necessary to achieve a specific public health objective, proportional to the seriousness of the public health threat, scientifically sound to support their effectiveness, and time-bounded (1,2). However, this is insufficient. This is why in this article we present a more inclusive framework also comprising twelve enabling factors to guide the design and development of ethical DTT systems.;Ethical Guidelines for SARS-CoV-2 Digital Tracking and Tracing Systems;Morley, J., Cowls, J. & Taddeo, M. et al.;2020;Floridi, L.;Ethics and Philosophy of Information
"Artificial intelligence (AI) has dominated recent headlines, with its promises, challenges, risks, successes, and failures. What is its foreseeable future? Of course, the most accurate forecasts are made with hindsight. But if some cheating is not acceptable, then smart people bet on the uncontroversial or the untestable. On the uncontroversial side, one may mention the increased pressure that will come from law-makers to ensure that AI applications align with socially acceptable expectations. For example, everybody expects some regulatory move from the EU, sooner or later. On the untestable side, some people will keep selling catastrophic forecasts, with dystopian scenarios taking place in some future that is sufficiently distant to ensure that the Jeremiahs will not be around to be proven wrong. Fear always sells well, like vampire or zombie movies. Expect more. What is difficult, and may be quite embarrassing later on, is to try to “look into the seeds of time, and say which grain will grow and which will not” (Macbeth, Act I, Scene III), that is, to try to understand where AI is more likely to go and hence where it may not be going. This is what I will attempt to do in the following pages, where I shall be cautious in identifying the paths of least resistance, but not so cautious as to avoid any risk of being proven wrong. Part of the difficulty is to get the level of abstraction right (Floridi 2008a, 2008b), i.e. to identify the set of relevant observables (“the seeds of time”) on which to focus because those are the ones that will make the real, significant difference. In our case, I shall argue that the best observables are provided by an analysis of the nature of the data used by AI to achieve its performance, and of the nature of the problems that AI may be expected to solve. So, my forecast will be divided into two, complementary parts. In Section 2, I will discuss the nature of the data needed by AI; and in Section 3, I will discuss the scope of the problems AI is more likely to tackle successfully. I will conclude with some more general remarks about tackling the related ethical challenges. But first, let me be clear about what I mean by AI.";What the Near Future of Artificial Intelligence Could Be;Floridi, L.;2019;Floridi, L.;Ethics and Philosophy of Information
"To address the rising concern that algorithmic decision-making may reinforce discriminatory biases, researchers have proposed many notions of fairness and corresponding mathematical formalizations. Each of these notions is often presented as a one-size-fits-all, absolute condition; however, in reality, the practical and ethical trade-offs are unavoidable and more complex. We introduce a new approach that considers fairness—not as a binary, absolute mathematical condition—but rather, as a relational notion in comparison to alternative decisionmaking processes. Using US mortgage lending as an example use case, we discuss the ethical foundations of each definition of fairness and demonstrate that our proposed methodology more closely captures the ethical trade-offs of the decision-maker, as well as forcing a more explicit representation of which values and objectives are prioritised.";Algorithmic Fairness in Mortgage Lending: from Absolute Conditions to Relational Trade-offs;Lee, M. & Floridi, L.;2020;Floridi, L.;Ethics and Philosophy of Information
"The trouble with seasonal metaphors is that they are cyclical. If you say that artificial intelligence (AI) got through a bad winter, you must also remember that winter will return, and you better be ready. An AI winter is that stage when technology, business, and the media get out of their warm and comfortable bubble, cool down, temper their sci-fi speculations and unreasonable hypes, and come to terms with what AI can or cannot really do as a technology (Floridi 2019), without exaggeration. Investments become more discerning, and journalists stop writing about AI, to chase some other fashionable topics and fuel the next fad. AI has had several winters. Among the most significant, there was one in the late 1970s, and another at the turn of the 1980s and 1990s. Today, we are talking about another predictable winter (Nield 2019; Walch 2019; Schuchmann 2019). AI is subject to these hype cycles because it is a hope or fear that we have entertained since we were thrown out of paradise: something that does everything for us, instead of us, better than us, with all the dreamy advantages (we shall be on holiday forever) and the nightmarish risks (we are going to be enslaved) that this entails. For some people, speculating about all this is irresistible. It is the wild west of “what if” scenarios. But I hope the reader will forgive me for a “I told you so” moment. For some time, I have been warning against commentators and “experts”, who were competing to see who could tell the tallest tale (Floridi 2016). A web of myths ensued. They spoke of AI as if it were the ultimate panacea, which would solve everything and overcome everything; or as the final catastrophe, a superintelligence that would destroy millions of jobs, replacing lawyers and doctors, journalists and researchers, truckers and taxi drivers, and ending by dominating human beings as if they were pets at best. Many followed Elon Musk in declaring the development of AI the greatest existential risk run by humanity. As if most of humanity did not live in misery and suffering. As if wars, famine, pollution, global warming, social injustice, and fundamentalism were science fiction, or just negligible nuisances, unworthy of their considerations. They insisted that law and regulations were always going to be too late and never catch up with AI, when in fact norms are not about the speed but about the direction of innovation, for they should steer the proper development of a society (if we like where we are heading, we cannot go there quickly enough). Today, we know that legislation is coming, at least in the EU. They claimed AI was a magic black box we could never explain, when in fact it is a matter of the correct level of abstraction at which to interpret the complex interactions engineered—even car traffic downtown becomes a black box if you wish to know why every single individual is there at that moment. Today there is a growing development of adequate tools to monitor and understand how machine learning systems reach their outcomes (Watson and Floridi forthcoming). They spread scepticism about the possibility of an ethical framework that would synthesize what we mean by socially good AI, when in fact the EU, the OECD, and China have converged on very similar principles that offer a common platform for further agreements (Floridi and Cowls 2019). Sophists in search of headlines. They should be ashamed and apologize. Not only for their untenable comments, but also for the great irresponsibility and alarmism, which have misled public opinion both about a potentially useful technology—that could provide helpful solutions, from medicine to security and monitoring systems (Taddeo and Floridi 2018)—and about the real risks—which we know are concrete but so much less fancy, from everyday manipulation of choices (Milano et al. 2019) to increased pressure on individual and group privacy (Floridi 2014), from cyberconflicts to the use of AI by organized crime for money laundering and identity theft (King et al. 2020). The risk of every AI summer is that over-inflated expectations turn into a mass distraction. The risk of every AI winter is that the backlash is excessive, the disappointment too negative, and potentially valuable solutions are thrown out with the water of the illusions. Managing the world is an increasingly complex task: megacities and their “smartification” offer a good example. And we have planetary problems—such as global warming, social injustice, and migration—which require ever higher degrees of coordination to be solved. It seems obvious that we need all the good technology that we can design, develop, and deploy to cope with these challenges, and all human intelligence we can exercise to put this technology in the service of a better future. AI can play an important role in all this because we need increasingly smarter ways of processing immense quantities of data, sustainably and efficiently. But AI must be treated as a normal technology, neither as a miracle nor as a plague, and as one of the many solutions that human ingenuity has managed to devise. This is also why the ethical debate about AI’s remains for ever an entirely human question. Now that a new AI winter is coming, we may try to learn some lessons, and avoid this yo-yo of unreasonable illusions and exaggerated disillusions. Let us not forget that the winter of AI should not be the winter of its opportunities. It certainly will not be the winter of its risks or challenges. We need to ask ourselves whether AI solutions are really going to replace previous solutions—as the automobile has done with the carriage—diversify them—as did the motorcycle with the bicycle—or complement and expand them—as the digital smart watch has done with the analog one. What will the level of social acceptability or preferability be of whatever AI will survive the new winter? Are we really going to be wearing some kind of strange glasses to live in a virtual or augmented world created by AI? Consider that today many people are reluctant to wear glasses even when they seriously need them, just for aesthetic reasons. And then, are there feasible AI solutions in everyday life? Are the necessary skills, datasets, infrastructure, and business models in place to make an AI application successful? The futurologists find these questions boring. They like a single, simple idea, which interprets and changes everything, that can be spread thinly across an easy book that makes the reader feel intelligent, a book to be read by everyone today and ignored by all tomorrow. It is the bad diet of junk fast-food for thoughts and the curse of the airport bestseller. We need to resist oversimplification. This time let us think more deeply and extensively on what we are doing and planning with AI. The exercise is called philosophy, not futurology.";AI and Its New Winter: from Myths to Realities;Floridi, L.;2020;Floridi, L.;Ethics and Philosophy of Information
Health-care systems worldwide face increasing demand, a rise in chronic disease, and resource constraints. At the same time, the use of digital health technologies in all care settings has led to an expansion of data. These data, if harnessed appropriately, could enable health-care providers to target the causes of ill-health and monitor the effectiveness of preventions and interventions. For this reason, policy makers, politicians, clinical entrepreneurs, and computer and data scientists argue that a key part of health-care solutions will be artificial Intelligence (AI), particularly machine learning. AI forms a key part of the National Health Service (NHS) Long-Term Plan (2019) in England, the US National Institutes of Health Strategic Plan for Data Science (2018), and China's Healthy China 2030 strategy (2016).;An ethically mindful approach to AI for health care;Morley, J. & Floridi, L.;2020;Floridi, L.;Ethics and Philosophy of Information
In this commentary, we discuss the nature of reversible and irreversible questions, that is, questions that may enable one to identify the nature of the source of their answers. We then introduce GPT-3, a third-generation, autoregressive language model that uses deep learning to produce human-like texts, and use the previous distinction to analyse it. We expand the analysis to present three tests based on mathematical, semantic (that is, the Turing Test), and ethical questions and show that GPT-3 is not designed to pass any of them. This is a reminder that GPT-3 does not do what it is not supposed to do, and that any interpretation of GPT-3 as the beginning of the emergence of a general form of artificial intelligence is merely uninformed science fiction. We conclude by outlining some of the significant consequences of the industrialisation of automatic and cheap production of good, semantic artefacts.;GPT-3: Its Nature, Scope, Limits, and Consequences;Floridi, L. & Chiriatti, M.;2020;Floridi, L.;Ethics and Philosophy of Information
The idea of artificial intelligence for social good (henceforth AI4SG) is gaining traction within information societies in general and the AI community in particular. It has the potential to tackle social problems through the development of AI-based solutions. Yet, to date, there is only limited understanding of what makes AI socially good in theory, what counts as AI4SG in practice, and how to reproduce its initial successes in terms of policies. This article addresses this gap by identifying seven ethical factors that are essential for future AI4SG initiatives. The analysis is supported by 27 case examples of AI4SG projects. Some of these factors are almost entirely novel to AI, while the significance of other factors is heightened by the use of AI. From each of these factors, corresponding best practices are formulated which, subject to context and balance, may serve as preliminary guidelines to ensure that well-designed AI is more likely to serve the social good.;How to Design AI for Social Good: Seven Essential Factors;Floridi, L., Cowls, J. & King, T. et al.;2020;Floridi, L.;Ethics and Philosophy of Information
An increasing number of technology firms are implementing processes to identify and evaluate the ethical risks of their systems and products. A key part of these review processes is to foresee potential impacts of these technologies on different groups of users. In this article, we use the expression Ethical Foresight Analysis (EFA) to refer to a variety of analytical strategies for anticipating or predicting the ethical issues that new technological artefacts, services, and applications may raise. This article examines several existing EFA methodologies currently in use. It identifies the purposes of ethical foresight, the kinds of methods that current methodologies employ, and the strengths and weaknesses of each of these current approaches. The conclusion is that a new kind of foresight analysis on the ethics of emerging technologies is both feasible and urgently needed.;Ethical Foresight Analysis: What it is and Why it is Needed?;Floridi, L. & Strait, A.;2020;Floridi, L.;Ethics and Philosophy of Information
These apps are the product of inadequate evaluation and regulation. Over the past year, technology companies have made headlines claiming that their artificially intelligent (AI) products can outperform clinicians at diagnosing breast cancer, brain tumours, and diabetic retinopathy. Claims such as these have influenced policy makers, and AI now forms a key component of the national health strategies in England, the United States, and China. It is positive to see healthcare systems embracing data analytics and machine learning. However, there are reasonable concerns about the efficacy, ethics, and safety of some commercial, AI health solutions. Trust in AI applications (or apps) heavily relies on the myth of the objective and omniscient algorithm, and our systems for generating and implementing evidence have not yet met the new specific challenges of AI. They may even have failed on the basics. In a linked article, Freeman and colleagues throw these general concerns into stark relief with a close examination of the evidence on diagnostic apps for skin cancer.;The poor performance of apps assessing skin cancer risk;Morley, J. & Floridi, L.;2020;Floridi, L.;Ethics and Philosophy of Information
This article highlights the limitations of the tendency to frame health- and wellbeing-related digital tools (mHealth technologies) as empowering devices, especially as they play an increasingly important role in the National Health Service (NHS) in the UK. It argues that mHealth technologies should instead be framed as digital companions. This shift from empowerment to companionship is advocated by showing the conceptual, ethical, and methodological issues challenging the narrative of empowerment, and by arguing that such challenges, as well as the risk of medical paternalism, can be overcome by focusing on the potential for mHealth tools to mediate the relationship between recipients of clinical advice and givers of clinical advice, in ways that allow for contextual flexibility in the balance between patiency and agency. The article concludes by stressing that reframing the narrative cannot be the only means for avoiding harm caused to the NHS as a healthcare system by the introduction of mHealth tools. Future discussion will be needed on the overarching role of responsible design.;The Limits of Empowerment: How to Reframe the Role of mHealth Tools in the Healthcare Ecosystem;Morley, J. & Floridi, L.;2020;Floridi, L.;Ethics and Philosophy of Information
"The debate about the ethical implications of Artificial Intelligence dates from the 1960s (Samuel in Science, 132(3429):741–742, 1960. https://doi.org/10.1126/science.132.3429.741; Wiener in Cybernetics: or control and communication in the animal and the machine, MIT Press, New York, 1961). However, in recent years symbolic AI has been complemented and sometimes replaced by (Deep) Neural Networks and Machine Learning (ML) techniques. This has vastly increased its potential utility and impact on society, with the consequence that the ethical debate has gone mainstream. Such a debate has primarily focused on principles—the ‘what’ of AI ethics (beneficence, non-maleficence, autonomy, justice and explicability)—rather than on practices, the ‘how.’ Awareness of the potential issues is increasing at a fast rate, but the AI community’s ability to take action to mitigate the associated risks is still at its infancy. Our intention in presenting this research is to contribute to closing the gap between principles and practices by constructing a typology that may help practically-minded developers apply ethics at each stage of the Machine Learning development pipeline, and to signal to researchers where further work is needed. The focus is exclusively on Machine Learning, but it is hoped that the results of this research may be easily applicable to other branches of AI. The article outlines the research method for creating this typology, the initial findings, and provides a summary of future research needs.";From What to How: An Initial Review of Publicly Available AI Ethics Tools, Methods and Research to Translate Principles into Practices;Morley, J., Floridi, L. & Kinsey, L. et al.;2020;Floridi, L.;Ethics and Philosophy of Information
We propose a formal framework for interpretable machine learning. Combining elements from statistical learning, causal interventionism, and decision theory, we design an idealised explanation game in which players collaborate to find the best explanation(s) for a given algorithmic prediction. Through an iterative procedure of questions and answers, the players establish a three-dimensional Pareto frontier that describes the optimal trade-offs between explanatory accuracy, simplicity, and relevance. Multiple rounds are played at different levels of abstraction, allowing the players to explore overlapping causal patterns of variable granularity and scope. We characterise the conditions under which such a game is almost surely guaranteed to converge on a (conditionally) optimal explanation surface in polynomial time, and highlight obstacles that will tend to prevent the players from advancing beyond certain explanatory thresholds. The game serves a descriptive and a normative function, establishing a conceptual space in which to analyse and compare existing proposals, as well as design new and improved solutions.;The explanation game: a formal framework for interpretable machine learning;Watson, D. & Floridi, L.;2020;Floridi, L.;Ethics and Philosophy of Information
When Death of a Salesman by Arthur Miller premiered in 1949, the job of selling merchandise by peddling wares in a designated area was common. It was a very different age—intermodal containers were being standardised, and consumerism was becoming rampant. In the play, the salesman, Willy Loman, is unhappy about all his travelling, and rightly so. The disappearance of his kind of job reminds us that, until recently, we thought that digital technologies were just going to decrease the need to move around. Today, we shop online, and the new selling agents are recommender systems (Milano et al. 2019), which canvass the space of information, or infosphere. It is true that we move a lot of bytes and not just atoms around, yet this is not the whole story. It is really too simplistic to conclude that the only impact that the digital revolution has, and is still having, on mobility has been that of reducing it. One still hears some driverless car evangelists arguing this (sometimes hijacking a green rhetoric) but they are clearly mistaken, because more people who cannot drive today will be able to do so in the future, thanks to increased levels of automation. It is far more accurate to say that digital technologies are changing the very essence of mobility (they are - re-ontologising it), in four different ways.;Autonomous Vehicles: from Whether and When to Where and How;Floridi, L.;2019;Floridi, L.;Ethics and Philosophy of Information
On September 18, 2019, DeepMind's health teams—responsible for artificial intelligence (AI) health research—joined Google Health. DeepMind Health was created with the motivation of seeing the UK National Health Service (NHS) thrive, so a similar ambition can be assumed of the UK branch of Google Health. Certainly, the aims of the NHS long-term plan will be more achievable with Google Health technology. However, unlocking these and other opportunities will not be easy, because of a fundamental problem: a deficit of trust. Public trust in Google Health could have declined following the DeepMind Health deal with the Royal Free NHS Foundation Trust, because the UK Information Commissioner's Office ruled the Trust not to comply with data protection law. Furthermore, AI health solutions have yet to make a significant difference to clinicians on the frontline. This ineffectiveness, combined with so-called Dr Google allegedly facilitating a rise in what has been termed cyberchondria, indicates that clinicians' trust in AI health solutions could also be low. Fortunately, measures to gain trust need not be complicated. Google Health and the NHS could and should build on existing frameworks and work together to gain and maintain users' trust. First, both organisations should engage regularly with those who could be affected by the introduction of Google Health technologies, and affected individuals should participate in the shaping of the system. Second, we recommend that Google Health re-establish the independent review board that was in place at DeepMind. The board should operate transparently and be responsible for monitoring, analysing, and addressing the normative and overarching issues that arise at the individual, interpersonal, group, institutional, and societal levels in AI. Third, this board should push Google Health and the NHS to publish the aims and scope of all partnerships, and to publicly commit to open evaluation, feedback, and reproducibility of results. Capitalising on the opportunities of Google Health–NHS partnerships will be challenging, but it is possible. If the two organisations follow these steps while policy makers remain mindful of the risks, ready to redress errors, and wary of vendor lock-in, then hypothetical benefits could become reality. The alternative is to risk losing a significant portion of trust—not only in Google Health, but also in AI health solutions—resulting in a stifling of innovation and considerable opportunity costs. Avoiding this risk must be a key priority. JM is employeed by the UK National Health Service X, which had no input into or influence on the submitted work. All other authors declare no competing interests.;Google Health and the NHS: overcoming the trust deficit;Morley, J., Taddeo, M. & Floridi, L.;2019;Floridi, L.;Ethics and Philosophy of Information
Applications of artificial intelligence (AI) for cybersecurity tasks are attracting greater attention from the private and the public sectors. Estimates indicate that the market for AI in cybersecurity will grow from US$1 billion in 2016 to a US$34.8 billion net worth by 2025. The latest national cybersecurity and defence strategies of several governments explicitly mention AI capabilities. At the same time, initiatives to define new standards and certification procedures to elicit users’ trust in AI are emerging on a global scale. However, trust in AI (both machine learning and neural networks) to deliver cybersecurity tasks is a double-edged sword: it can improve substantially cybersecurity practices, but can also facilitate new forms of attacks to the AI applications themselves, which may pose severe security threats. We argue that trust in AI for cybersecurity is unwarranted and that, to reduce security risks, some form of control to ensure the deployment of ‘reliable AI’ for cybersecurity is necessary. To this end, we offer three recommendations focusing on the design, development and deployment of AI for cybersecurity.;Trusting artificial intelligence in cybersecurity is a double-edged sword;Taddeo, M., McCutcheon, T. & Floridi, L.;2019;Floridi, L.;Ethics and Philosophy of Information
In this article we analyze the ethical aspects of multistakeholder recommendation systems (RSs). Following the most common approach in the literature, we assume a consequentialist framework to introduce the main concepts of multistakeholder recommendation. We then consider three research questions: Who are the stakeholders in a RS? How are their interests taken into account when formulating a recommendation? And, what is the scientific paradigm underlying RSs? Our main finding is that multistakeholder RSs (MRSs) are designed and theorized, methodologically, according to neoclassical welfare economics. We consider and reply to some methodological objections to MRSs on this basis, concluding that the multistakeholder approach offers the resources to understand the normative social dimension of RSs.;Ethical aspects of multi-stakeholder recommendation systems;Milano, S., Taddeo, M. & Floridi, L.;2020;Floridi, L.;Ethics and Philosophy of Information
"Healthcare systems across the globe are struggling with increasing costs and worsening outcomes. This presents those responsible for overseeing healthcare with a challenge. Increasingly, policymakers, politicians, clinical entrepreneurs and computer and data scientists argue that a key part of the solution will be ‘Artificial Intelligence’ (AI) – particularly Machine Learning (ML). This argument stems not from the belief that all healthcare needs will soon be taken care of by “robot doctors.” Instead, it is an argument that rests on the classic counterfactual definition of AI as an umbrella term for a range of techniques that can be used to make machines complete tasks in a way that would be considered intelligent were they to be completed by a human. Automation of this nature could offer great opportunities for the improvement of healthcare services and ultimately patients’ health by significantly improving human clinical capabilities in diagnosis, drug discovery, epidemiology, personalised medicine, and operational efficiency. However, if these AI solutions are to be embedded in clinical practice, then at least three issues need to be considered: the technical possibilities and limitations; the ethical, regulatory and legal framework; and the governance framework. In this article, we report on the results of a systematic analysis designed to provide a clear overview of the second of these elements: the ethical, regulatory and legal framework. We find that ethical issues arise at six levels of abstraction (individual, interpersonal, group, institutional, sectoral, and societal) and can be categorised as epistemic, normative, or overarching. We conclude by stressing how important it is that the ethical challenges raised by implementing AI in healthcare settings are tackled proactively rather than reactively and map the key considerations for policymakers to each of the ethical concerns highlighted.";The Debate on the Ethics of AI in Health Care: A Reconstruction and Critical Review;Morley, J., Machado, C. & Burr, C. et al.;2019;Floridi, L.;Ethics and Philosophy of Information
Common mental health disorders are rising globally, creating a strain on public healthcare systems. This has led to a renewed interest in the role that digital technologies may have for improving mental health outcomes. One result of this interest is the development and use of artificial intelligence for assessing, diagnosing, and treating mental health issues, which we refer to as ‘digital psychiatry’. This article focuses on the increasing use of digital psychiatry outside of clinical settings, in the following sectors: education, employment, financial services, social media, and the digital well-being industry. We analyse the ethical risks of deploying digital psychiatry in these sectors, emphasising key problems and opportunities for public health, and offer recommendations for protecting and promoting public health and well-being in information societies.;Digital Psychiatry: Ethical Risks and Opportunities for Public Health and Well-Being;Burr, C., Morley, J. & Taddeo, M. et al.;2019;Floridi, L.;Ethics and Philosophy of Information
In July 2017, China’s State Council released the country’s strategy for developing artificial intelligence (AI), entitled ‘New Generation Artificial Intelligence Development Plan’. This strategy outlined China’s aims to become the world leader in AI by 2030, to monetise AI into a trillion-yuan ($150 billion) industry, and to emerge as the driving force in defining ethical norms and standards for AI. Several reports have analysed specific aspects of China’s AI policies or have assessed the country’s technical capabilities. Instead, in this article, we focus on the socio-political background and policy debates that are shaping China’s AI strategy. In particular, we analyse the main strategic areas in which China is investing in AI and the concurrent ethical debates that are delimiting its use. Through focusing on the policy backdrop, we seek to provide a more comprehensive understanding of China’s AI policy by bringing together debates and analyses of a wide array of policy documents.;The Chinese Approach to Artificial Intelligence: An Analysis of Policy and Regulation;Roberts, H., Cowls, J. & Morley, J. et al.;2019;Floridi, L.;Ethics and Philosophy of Information
"There is an endless number of ways to describe anything, including ourselves. In a more formal context, this is made clear and precise by speaking of levels of abstraction (Floridi 2008).Social scientists prefer to speak of “lenses”, but the idea is the same. The point is not whether a description is accurate in absolute terms (is John a featherless biped? Or the details on his passport? Or 60% water and 40% something else? Or…) but whether it fulfils its purpose, and how well it does so, when compared with other descriptions. When we think that a description is the only one available, this is usually because we take its purpose for granted. Thus, we may describe an artefact as a “vehicle” because we take for granted a specific function or purpose of that artefact, but that description could easily change, if, for example, it is the SS-100-X presidential limousine in which Kennedy was a passenger when he was assassinated. All of a sudden “vehicle” is not wrong, but no longer satisfactory. This is the linguistic function of “just”: it is no longer “just” a vehicle. John is no longer “just” a featherless biped or the details on his passport, because he is John Fitzgerald Kennedy, the 35th president of the USA assassinated in 1963. Back to us, today, it makes a lot of sense to describe ourselves as informational organisms, or inforgs for short (Floridi 2014), in order to understand why and how we interact, flourish, or suffer depending on the flows of information in which we partake. This is related to what has been interestingly argued by Vilem Flusser: The human being can no longer be seen as an individual but rather as the opposite, as a dense scattering of parts; he is calculable. The notorious Self is seen as a knot in which different fields cross, as in the way the many physical fields cross with the ecological, psychic, and cultural. The notorious Self shows itself not as a kernel but as a shell. It holds the scattered parts together, contains them. It is a mask. (Flusser 2005, p. 324) I agree (Floridi 2011), but contrary to Flusser, I am not making an ontological, absolute point. I am making an epistemological and relational one. It should now be clear that I am not arguing that we are “just” inforgs (the self is not just a mask). Rather, I am suggesting that an informational level of abstraction to understand our identities, roles, behaviours, and interactions, is helpful and possibly the most adequate approach to interpret our human predicament today. Describing ourselves as (also) inforgs, who forage for, produce, cultivate, curate, process, and consume information, inhabiting an environment also made of data and computational processes, means adopting an ecological perspective (Floridi 2013). When this is interpreted in social, economic, and political terms, then I would argue that we can upgrade our level of abstraction and understand ourselves not only as inforgs but also, and preferably, as interfaces. It is still an informational level of abstraction, but now we are focusing on a crucial, functional aspect that otherwise remains hidden. By describing ourselves as interfaces, we can understand more insightfully several crucial phenomena that characterise our digital age, including marketing and the “marketisation” of political communication, and what we can do about them. The rest of the article offers a series of clarifications in support of this statement and looks at some of its implications in political theory.";Marketing as Control of Human Interfaces and Its Political Exploitation;Floridi, L.;2019;Floridi, L.;Ethics and Philosophy of Information
"The European Medical Information Framework (EMIF) project, funded through the IMI programme (Innovative Medicines Initiative Joint Undertaking under Grant Agreement No. 115372), has designed and implemented a federated platform to connect health data from a variety of sources across Europe, to facilitate large scale clinical and life sciences research. It enables approved users to analyse securely multiple, diverse, data via a single portal, thereby mediating research opportunities across a large quantity of research data. EMIF developed a code of practice (ECoP) to ensure the privacy protection of data subjects, protect the interests of data sharing parties, comply with legislation and various organisational policies on data protection, uphold best practices in the protection of personal privacy and information governance, and eventually promote these best practices more widely. EMIF convened an Ethics Advisory Board (EAB), to provide feedback on its approach, platform, and the EcoP. The most important challenges the ECoP team faced were: how to define, control and monitor the purposes (kinds of research) for which federated health data are used; the kinds of organisation that should be permitted to conduct permitted research; and how to monitor this. This manuscript explores those issues, offering the combined insights of the EAB and EMIF core ECoP team. For some issues, a consensus on how to approach them is proposed. For other issues, a singular approach may be premature but the challenges are summarised to help the community to debate the topic further. Arguably, the issues and their analyses have application beyond EMIF, to many research infrastructures connected to health data sources.";Key Ethical Challenges in the European Medical Information Framework;Floridi, L., Luetge, C. & Pagallo, U.;2018;Floridi, L.;Ethics and Philosophy of Information
On 8th August 2019, Secretary of State for Health and Social Care, Matt Hancock, announced the creation of a £250 million NHS AI Lab. This significant investment is justified on the belief that transforming the UK’s National Health Service (NHS) into a more informationally mature and heterogeneous organisation, reliant on data-based and algorithmically-driven interactions, will offer significant benefit to patients, clinicians, and the overall system. These opportunities are realistic and should not be wasted. However, they may be missed (one may recall the troubled Care.data programme) if the ethical challenges posed by this transformation are not carefully considered from the start, and then addressed thoroughly, systematically, and in a socially participatory way. To deal with this serious risk, the NHS AI Lab should create an Ethics Advisory Board and monitor, analyse, and address the normative and overarching ethical issues that arise at the individual, interpersonal, group, institutional and societal levels in AI for healthcare. ;NHS AI Lab: why we need to be ethically mindful about AI for healthcare;Morley, J. & Floridi, L.;2019;Floridi, L.;Ethics and Philosophy of Information
"In June 2019, WHO and the Organisation for Economic Co-operation and Development convened a meeting to discuss how best to implement digital health for the purpose of transforming health systems, empowering individuals, and improving the delivery of high-quality health care. This meeting followed the April 2018 communication from the European Commission on “enabling the digital transformation of health and care in the Digital Single Market; empowering citizens and building a healthier society” (appendix). Such international calls to action have been effective at spreading the message that digital health will bring patient empowerment to health-care policy makers across the globe. Consequently, empowerment plays a prominent role in many national-level policy documents, including National Health Service England's Empower the Person strategy (appendix), the eHealth Strategy for Ireland, and the National eHealth Strategy of Australia. The issues with this empowerment narrative are varied and covered in more detail elsewhere. We are primarily concerned with the fact that because these strategies largely fail to detail how digital health tools (DHTs) empower citizens or patients, governments risk using this rhetoric in a potentially deceptive manner. The aforementioned strategies seek to encourage the adoption of technologies that might make individuals responsible for self-surveilling all aspects of their life through the digital medical gaze (appendix), instead of focusing on how data derived from DHTs can enable better care at the level of systems, population, group, or individuals. This risk of self-surveilling is ethically worrying because, as part of this process (the lifestylisation of health care; appendix), individuals are encouraged to reflect on how they might be performing against established baselines for health, but not told how these baselines were established2 and whether or how far they may apply to them. For example, the individual user does not known whether their default optimum heart rate is actually optimum for a person like them, or only for individuals like those included in the design trial (ie, it is not clear whether the specific individual fits the profile associated with the DHT). The advice provided by DHTs promotes conformity rather than autonomy and risks undermining individuals' integrity of self.";Enabling digital health companionship is better than empowerment;Morley, J. & Floridi, L.;2019;Floridi, L.;Ethics and Philosophy of Information
It has been suggested that to overcome the challenges facing the UK’s National Health Service (NHS) of an ageing population and reduced available funding, the NHS should be transformed into a more informationally mature and heterogeneous organisation, reliant on data-based and algorithmically-driven interactions between human, artificial, and hybrid (semi-artificial) agents. This transformation process would offer significant benefit to patients, clinicians, and the overall system, but it would also rely on a fundamental transformation of the healthcare system in a way that poses significant governance challenges. In this article, we argue that a fruitful way to overcome these challenges is by adopting a pro-ethical approach to design that analyses the system as a whole, keeps society-in-the-loop throughout the process, and distributes responsibility evenly across all nodes in the system.;How to design a governable digital health ecosystem;Morley, J. & Floridi, L.;2019;Floridi, L.;Ethics and Philosophy of Information
"It has taken a very long time, but today, the debate on the ethical impact and implications of digital technologies has reached the front pages of newspapers. This is understandable: digital technologies—from web-based services to Artificial Intelligence (AI) solutions—increasingly affect the daily lives of billions of people, so there are many hopes but also concerns about their design, development, and deployment (Cath et al. 2018). After more than half a century of academic research, the recent public reaction has been a flourishing of initiatives to establish what principles, guidelines, codes, or frameworks can ethically guide digital innovation, particularly in AI, to benefit humanity and the whole environment. This is a positive development that shows awareness of the importance of the topic and interest in tackling it systematically. Yet, it is time that debate evolves from the whatto the how: not just what ethics is needed but also how ethics can be effectively and successfully applied and implemented in order to make a positive difference. For example, the European Ethics Guidelines for Trustworthy AI establish a benchmark for what may or may not qualify as ethically good AI in the EU. Their publication is currently being followed by practical efforts of testing, application, and implementation. The move from a first, more theoretical what chapter, to a second, more practical howchapter, so to speak, is reasonable and commendable. However, in translating principles into practices, even the best efforts may be undermined by some unethical risks. In this article, I wish to highlight five of them. We shall see that they are more clusters than individual risks, and there may be other clusters as well, but these five are the ones already encountered or foreseeable in the international debate about digital ethics.Footnote 4 Here is the list: (1) ethics shopping; (2) ethics bluewashing; (3) ethics lobbying; (4) ethics dumping; and (5) ethics shirking. They are the five “ethics gerunds”, to borrow Josh Cowls’ apt label, who also suggested to consider the first three more “distractive” and the last two more “destructive” problems. Let us consider each of them in some detail.";Translating Principles into Practices of Digital Ethics: Five Risks of Being Unethical;Floridi, L.;2019;Floridi, L.;Ethics and Philosophy of Information
The European Commission’s report ‘Ethics guidelines for trustworthy AI’ provides a clear benchmark to evaluate the responsible development of AI systems, and facilitates international support for AI solutions that are good for humanity and the environment, says Luciano Floridi.;Establishing the rules for building trustworthy AI;Floridi, L.;2019;Floridi, L.;Ethics and Philosophy of Information
This article presents the first, systematic analysis of the ethical challenges posed by recommender systems through a literature review. The article identifies six areas of concern, and maps them onto a proposed taxonomy of different kinds of ethical impact. The analysis uncovers a gap in the literature: currently user-centred approaches do not consider the interests of a variety of other stakeholders—as opposed to just the receivers of a recommendation—in assessing the ethical impacts of a recommender system.;Recommender systems and their ethical challenges;Milano, S., Taddeo, M. & Floridi, L.;2020;Floridi, L.;Ethics and Philosophy of Information
To maximise the clinical benefits of machine learning algorithms, we need to rethink our approach to explanation, argue David Watson and colleagues. Machine learning algorithms are an application of artificial intelligence designed to automatically detect patterns in data without being explicitly programmed. They promise to change the way we detect and treat disease and will likely have a major impact on clinical decision making. The long term success of these powerful new methods hinges on the ability of both patients and doctors to understand and explain their predictions, especially in complicated cases with major healthcare consequences. This will promote greater trust in computational techniques and ensure informed consent to algorithmically designed treatment plans. Unfortunately, many popular machine learning algorithms are essentially black boxes—oracular inference engines that render verdicts without any accompanying justification. This problem has become especially pressing with passage of the European Union’s latest General Data Protection Regulation (GDPR), which some scholars argue provides citizens with a “right to explanation.” Now, any institution engaged in algorithmic decision making is legally required to justify those decisions to any person whose data they hold on request, a challenge that most are ill equipped to meet. We urge clinicians to link with patients, data scientists, and policy makers to ensure the successful clinical implementation of machine learning (fig 1). We outline important goals and limitations that we hope will inform future research.;Clinical applications of machine learning algorithms: beyond the black box;Watson, D., Krutzinna, J. & Bruce, I. et al.;2019;Floridi, L.;Ethics and Philosophy of Information
This article presents the first thematic review of the literature on the ethical issues concerning digital well-being. The term ‘digital well-being’ is used to refer to the impact of digital technologies on what it means to live a life that is good for a human being. The review explores the existing literature on the ethics of digital well-being, with the goal of mapping the current debate and identifying open questions for future research. The review identifies major issues related to several key social domains: healthcare, education, governance and social development, and media and entertainment. It also highlights three broader themes: positive computing, personalised human–computer interaction, and autonomy and self-determination. The review argues that three themes will be central to ongoing discussions and research by showing how they can be used to identify open questions related to the ethics of digital well-being.;The Ethics of Digital Well-Being: A Thematic Review;Burr, C., Taddeo, M. & Floridi, L.;2020;Floridi, L.;Ethics and Philosophy of Information
The automation of online social life is an urgent issue for researchers and the public alike. However, one of the most significant uses of such technologies seems to have gone largely unnoticed by the research community: religion. Focusing on Islamic Prayer Apps, which automatically post prayers from its users’ accounts, we show that even one such service is already responsible for millions of tweets daily, constituting a significant portion of Arabic-language Twitter traffic. We argue that the fact that a phenomenon of these proportions has gone unnoticed by researchers reveals an opportunity to broaden the scope of the current research agenda on online automation.;Prayer-Bots and Religious Worship on Twitter: A Call for a Wider Research Agenda;Öhman, C., Gorwa, R. & Floridi, L.;2019;Floridi, L.;Ethics and Philosophy of Information
Allow me to be trivial and vague to begin with. There is a wealth of resources—including ideas, insights, discoveries, inventions, traditions, cultures, languages, arts, religions, sciences, narratives, stories, poems, customs and norms, music and songs, games and personal experiences, and advertisements—that we produce, curate, consume, transmit, and inherit as humans. We use this wealth—which I shall define more precisely as semantic capital in the next section—in order to give meaning to, and make sense of, our own existence and the world surrounding us, to define who we are, and to develop an individual and social life. Given its crucial importance, one would expect the concept of semantic capital and its related phenomena to be well-known and extensively theorised. Much to my surprise, this is not the case. In this section, I shall offer an explanation of why there is such a gap. I will return to the definition of what semantic capital is in the next.;Semantic Capital: Its Nature, Value, and Curation;Floridi, L.;2018;Floridi, L.;Ethics and Philosophy of Information
"This article reports the findings of AI4People, an Atomium—EISMD initiative designed to lay the foundations for a “Good AI Society”. We introduce the core opportunities and risks of AI for society; present a synthesis of five ethical principles that should undergird its development and adoption; and offer 20 concrete recommendations—to assess, to develop, to incentivise, and to support good AI—which in some cases may be undertaken directly by national or supranational policy makers, while in others may be led by other stakeholders. If adopted, these recommendations would serve as a firm foundation for the establishment of a Good AI Society.";AI4People—An Ethical Framework for a Good AI Society: Opportunities, Risks, Principles, and Recommendations;Floridi, L., Cowls, J. & Beltrametti, M.;2018;Floridi, L.;Ethics and Philosophy of Information
The article discusses the governance of the digital as the new challenge posed by technological innovation. It then introduces a new distinction between soft ethics, which applies after legal compliance with legislation, such as the General Data Protection Regulation in the European Union, and hard ethics, which precedes and contributes to shape legislation. It concludes by developing an analysis of the role of digital ethics with respect to digital regulation and digital governance. This article is part of the theme issue ‘Governing artificial intelligence: ethical, legal, and technical opportunities and challenges’.;Soft ethics, the governance of the digital and the General Data Protection Regulation;Floridi, L.;2018;Floridi, L.;Ethics and Philosophy of Information
This article addresses the question of how ‘Country of Origin Information’ (COI) reports—that is, research developed and used to support decision-making in the asylum process—can be published in an ethical manner. The article focuses on the risk that published COI reports could be misused and thereby harm the subjects of the reports and/or those involved in their development. It supports a situational approach to assessing data ethics when publishing COI reports, whereby COI service providers must weigh up the benefits and harms of publication based, inter alia, on the foreseeability and probability of harm due to potential misuse of the research, the public good nature of the research, and the need to balance the rights and duties of the various actors in the asylum process, including asylum seekers themselves. Although this article focuses on the specific question of ‘how to publish COI reports in an ethical manner’, it also intends to promote further research on data ethics in the asylum process, particularly in relation to refugees, where more foundational issues should be considered.;Towards the Ethical Publication of Country of Origin Information (COI) in the Asylum Process;Aggarwal, N. & Floridi, L.;2020;Floridi, L.;Ethics and Philosophy of Information
Although the employment situation of disabled people has widely been identified as in need of improvement, progress in this area remains slow. While some progress has been made in including the physically or sensory disabled in the workplace, other types of disability have been largely neglected. This applies particularly to disabled workers in atypical employment, such as those whose workplace is the Digital Economy. In this article, we discuss the case of disabled app developers as a significant example of how the current regulatory framework fails to be inclusive in its attempts to protect the rights of disabled Digital Economy workers. We identify two problems that are at the heart of this: first, a continuing failure to collect relevant and comprehensive data on disability, and second, a lack of accountability towards disabled workers in atypical employment. Consequently, we call for better data collection and argue for urgent policy changes to close the existing accountability gap.;Atypical employment and disability in the digital economy: accountability gap leaves disabled app developers’ rights unprotected;Krutzinna, J. & Floridi, L.;2018;Floridi, L.;Ethics and Philosophy of Information
The art world is full of reproductions. Some are plain replicas, for example the Mona Lisa. Others are fakes or forgeries, like the “Vermeers” painted by Han van Meegeren that sold for $60 million (Kreuger and van Meegeren 2010). The distinction between a replica and a fake is based on the concept of authenticity. Is this artefact what it claims to be?Footnote 1 The answer seems simple but, in reality, things are complicated. Today, the paintings of the forger John Myatt are so famous that they are valued at up to $40,000 each, as “genuine fakes” (Furlong 1986). They are not what they say they are, but they are authentically painted by him and not by another forger. And they are beautiful. A bit as if one were to utter a beautiful lie, not any ordinary lie. And an artist like Magritte seems to have painted not only false Picassos and Renoirs during the Nazi occupation of Belgium (Mariën 1983), but also faked his own work, so to speak, in the famous case of the two copies of the painting “The Flavour of Tears” (1948), both by Magritte, but one of which he passed off as false—partly as a surrealist act and partly to make money. In this mess, and as if things were not confusing enough, digital technologies further reshuffle what is possible and our understanding of it.;Artificial Intelligence, Deepfakes and a Future of Ectypes;Floridi, L.;2018;Floridi, L.;Ethics and Philosophy of Information
Artificial intelligence (AI) is not just a new technology that requires regulation. It is a powerful force that is reshaping daily practices, personal and professional interactions, and environments. For the well-being of humanity it is crucial that this power is used as a force of good. Ethics plays a key role in this process by ensuring that regulations of AI harness its potential while mitigating its risks. AI may be defined in many ways. Get its definition wrong, and any assessment of the ethical challenges of AI becomes science fiction at best or an irresponsible distraction at worst, as in the case of the singularity debate. A scientifically sound approach is to draw on its classic definition as a growing resource of interactive, autonomous, self-learning agency, which enables computational artifacts to perform tasks that otherwise would require human intelligence to be executed successfully. AI can then be further defined in terms of features such as the computational models on which it relies or the architecture of the technology. But when it comes to ethical and policy-related issues, the latter distinctions are unnecessary. On the one hand, AI is fueled by data and therefore faces ethical challenges related to data governance, including consent, ownership, and privacy. These data-related challenges may be exacerbated by AI, but would occur even without AI. On the other hand, AI is a distinct form of autonomous and self-learning agency and thus raises unique ethical challenges. The latter are the focus of this article. The ethical debate on AI as a new form of agency dates to the 1960s. Since then, many of the relevant problems have concerned delegation and responsibility. As AI is used in ever more contexts, from recruitment to health care, understanding which tasks and decisions to entrust (delegate) to AI and how to ascribe responsibility for its performance are pressing ethical problems. At the same time, as AI becomes invisibly ubiquitous, new ethical challenges emerge. The protection of human self-determination is one of the most relevant and must be addressed urgently. The application of AI to profile users for targeted advertising, as in the case of online service providers, and in political campaigns, as unveiled by the Cambridge Analytica case, offer clear examples of the potential of AI to capture users' preferences and characteristics and hence shape their goals and nudge their behavior to an extent that may undermine their self-determination.;How AI can be a force for good;Taddeo, M. & Floridi, L.;2018;Floridi, L.;Ethics and Philosophy of Information
That AI will have a major impact on society is no longer in question. Current debate turns instead on how far this impact will be positive or negative, for whom, in which ways, in which places, and on what timescale. In order to frame these questions in a more substantive way, in this prolegomena we introduce what we consider the four core opportunities for society offered by the use of AI, four associated risks which could emerge from its overuse or misuse, and the opportunity costs associated with its under use. We then offer a high-level view of the emerging advantages for organisations of taking an ethical approach to developing and deploying AI. Finally, we introduce a set of five principles which should guide the development and deployment of AI technologies. The development of laws, policies and best practices for seizing the opportunities and minimizing the risks posed by AI technologies would benefit from building on ethical frameworks such as the one offered here.;Prolegomena to a White Paper on an Ethical Framework for a Good AI Society;Cowls, J. & Floridi, L.;2018;Floridi, L.;Ethics and Philosophy of Information
In a previous article (Floridi 2018), I introduced the distinction between hard and soft ethics. Since the reader may not be familiar with it, let me quickly summarise it here. I will then be able to use it to clarify two issues: the application of soft ethics to the General Data Protection Regulation (henceforth GDPR) and the idea that soft ethics has a dual advantage.;Soft Ethics: Its Application to the General Data Protection Regulation and Its Dual Advantage;Floridi, L.;2018;Floridi, L.;Ethics and Philosophy of Information
Artificial intelligence (AI) research and regulation seek to balance the benefits of innovation against any potential harms and disruption. However, one unintended consequence of the recent surge in AI research is the potential re-orientation of AI technologies to facilitate criminal acts, term in this article AI-Crime (AIC). AIC is theoretically feasible thanks to published experiments in automating fraud targeted at social media users, as well as demonstrations of AI-driven manipulation of simulated markets. However, because AIC is still a relatively young and inherently interdisciplinary area—spanning socio-legal studies to formal science—there is little certainty of what an AIC future might look like. This article offers the first systematic, interdisciplinary literature analysis of the foreseeable threats of AIC, providing ethicists, policy-makers, and law enforcement organisations with a synthesis of the current problems, and a possible solution space.;Artificial Intelligence Crime: An Interdisciplinary Analysis of Foreseeable Threats and Solutions;King, T., Aggarwal, N. & Taddeo, M. et al.;2019;Floridi, L.;Ethics and Philosophy of Information
In an open letter, 156 artificial-intelligence experts from 14 European countries (go.nature.com/2t5mgov) have rejected the European Parliament’s recommendation that robots should have legal status as electronic persons. This would make robots responsible for repairing any damage they might cause (go.nature.com/2wxlwg6). We are not signatories to the open letter, but endorse it nonetheless. In our view, the parliament’s recommendation is flawed. Its rationale seems to be that robots can be electronic juridical persons in the same way as companies are. But companies are constituted and run by real people. That is why they can be meaningfully attributed with intentions, plans, goals, legal rights and duties, and why they can be taught, praised or punished. Hence, they are considered to be responsible, accountable or liable for their actions. Attributing electronic personhood to robots risks misplacing moral responsibility, causal accountability and legal liability regarding their mistakes and misuses. Robots could be blamed and punished instead of humans. And irresponsible people would dismiss the need for care in the engineering, marketing and use of robots. Even the Romans knew better: the owner of an enslaved person was fully responsible for any damage caused by that person (known as vicarious liability).;Romans would have denied robots legal personhood;Floridi, L.;2018;Floridi, L.;Ethics and Philosophy of Information
Define an international doctrine for cyberspace skirmishes before they escalate into conventional warfare, urge Mariarosaria Taddeo and Luciano Floridi. Cyberattacks are becoming more frequent, sophisticated and destructive. Each day in 2017, the United States suffered, on average, more than 4,000 ransomware attacks, which encrypt computer files until the owner pays to release them1. In 2015, the daily average was just 1,000. In May last year, when the WannaCry virus crippled hundreds of IT systems across the UK National Health Service, more than 19,000 appointments were cancelled. A month later, the NotPetya ransomware cost pharmaceutical giant Merck, shipping firm Maersk and logistics company FedEx around US$300 million each. Global damages from cyberattacks totalled $5 billion in 2017 and may reach $6 trillion a year by 2021 (see go.nature.com/2gncsyg).;Regulate artificial intelligence to avert cyber arms race;Taddeo, M. & Floridi, L.;2018;Floridi, L.;Ethics and Philosophy of Information
This article argues that personal medical data should be made available for scientific research, by enabling and encouraging individuals to donate their medical records once deceased, similar to the way in which they can already donate organs or bodies. This research is part of a project on posthumous medical data donation developed by the Digital Ethics Lab at the Oxford Internet Institute at the University of Oxford. Ten arguments are provided to support the need to foster posthumous medical data donation. Two major risks are also identified—harm to others, and lack of control over the use of data—which could follow from unregulated donation of medical data. The argument that record-based medical research should proceed without the need to secure informed consent is rejected, and instead a voluntary and participatory approach to using personal medical data should be followed. The analysis concludes by stressing the need to develop an ethical code for data donation to minimise the risks, and offers five foundational principles for ethical medical data donation suggested as a draft code.;Enabling Posthumous Medical Data Donation: An Appeal for the Ethical Utilisation of Personal Health Data;Krutzinna, J., Taddeo, M. & Floridi, L.;2019;Floridi, L.;Ethics and Philosophy of Information
The environment can elicit biological responses such as oxidative stress (OS) and inflammation as a consequence of chemical, physical, or psychological changes. As population studies are essential for establishing these environment-organism interactions, biomarkers of OS or inflammation are critical in formulating mechanistic hypotheses. By using examples of stress induced by various mechanisms, we focus on the biomarkers that have been used to assess OS and inflammation in these conditions. We discuss the difference between biomarkers that are the result of a chemical reaction (such as lipid peroxides or oxidized proteins that are a result of the reaction of molecules with reactive oxygen species) and those that represent the biological response to stress, such as the transcription factor NRF2 or inflammation and inflammatory cytokines. The high-throughput and holistic approaches to biomarker discovery used extensively in large-scale molecular epidemiological exposome are also discussed in the context of human exposure to environmental stressors. We propose to consider the role of biomarkers as signs and to distinguish between signs that are just indicators of biological processes and proxies that one can interact with and modify the disease process. Antioxid. Redox Signal. 28, 852–872.;Oxidative Stress and Inflammation Induced by Environmental and Psychological Stressors: A Biomarker Perspective;Ghezzi, P., Floridi, L. & Boraschi, D. et al.;2018;Floridi, L.;Ethics and Philosophy of Information
Biomarkers are widely used not only as prognostic or diagnostic indicators, or as surrogate markers of disease in clinical trials, but also to formulate theories of pathogenesis. We identify two problems in the use of biomarkers in mechanistic studies. The first problem arises in the case of multifactorial diseases, where different combinations of multiple causes result in patient heterogeneity. The second problem arises when a pathogenic mediator is difficult to measure. This is the case of the oxidative stress (OS) theory of disease, where the causal components are reactive oxygen species (ROS) that have very short half-lives. In this case, it is usual to measure the traces left by the reaction of ROS with biological molecules, rather than the ROS themselves. Borrowing from the philosophical theories of signs, we look at the different facets of biomarkers and discuss their different value and meaning in multifactorial diseases and system medicine to inform their use in patient stratification in personalized medicine.;Theory of signs and statistical approach to big data in assessing the relevance of clinical biomarkers of inflammation and oxidative stress;Ghezzi, P., Davies, K. & Delaney, A.;2018;Floridi, L.;Ethics and Philosophy of Information
The web is increasingly inhabited by the remains of its departed users, a phenomenon that has given rise to a burgeoning digital afterlife industry. This industry requires a framework for dealing with its ethical implications. The regulatory conventions guiding archaeological exhibitions could provide the basis for such a framework.;An ethical framework for the digital afterlife industry;Öhman, C. & Floridi, L.;2018;Floridi, L.;Ethics and Philosophy of Information
Today, in any mature information society (Floridi 2016), we no longer live online or offline but onlife, that is, we increasingly live in that special space, or infosphere, that is seamlessly analogue and digital, offline and online. If this seems confusing, perhaps an analogy may help to convey the point. Imagine someone asks whether the water is sweet or salty in the estuary where the river meets the sea. Clearly, that someone has not understood the special nature of the place. Our mature information societies are growing in such a new, liminal place, like mangroves flourishing in brackish water. And in these ‘mangrove societies’, machine-readable data, new forms of smart agency and onlife interactions are constantly evolving, because our technologies are perfectly fit to take advantage of such a new environment, often as the only real natives. As a result, the pace of their evolution can be mind-blowing. And this in turn justifies some apprehension. However, we should not be distracted by the scope, depth and pace of technological innovation. True, it does disrupt some deeply ingrained assumptions of the old, exclusively analogue society, e.g. about production, logistics, customization, competition, education, work, health, entertainment, politics and security, just to mention some crucial topics. Yet that is not the most consequential challenge we are facing. It is rather how we are going to design the infosphere and the mature information societies developing within it that matters most. Because the digital revolution transforms our views about values and their priorities, good behaviour, and what sort of innovation is socially preferable—and this is the fundamental issue, let me explain.;Soft Ethics and the Governance of the Digital;Floridi, L.;2018;Floridi, L.;Ethics and Philosophy of Information
One of the ambitions of Science Robotics is to deeply root robotics research in science while developing novel robotic platforms that will enable new scientific discoveries. Of our 10 grand challenges, the first 7 represent underpinning technologies that have a wider impact on all application areas of robotics. For the next two challenges, we have included social robotics and medical robotics as application-specific areas of development to highlight the substantial societal and health impacts that they will bring. Finally, the last challenge is related to responsible innovation and how ethics and security should be carefully considered as we develop the technology further.;The grand challenges of Science Robotics;Yang, G-Z., Bellingham, J. & Dupont, P. et al.;2018;Floridi, L.;Ethics and Philosophy of Information
"Three classic distinctions specify that truths can be necessary versus contingent,analyticversus synthetic, and a priori versus a posteriori. The philosopher reading this article knows very well both how useful and ordinary such distinctions are in our conceptual work and that they have been subject to many and detailed debates, especially the last two. In the following pages, I do not wish to discuss how far they may be tenable. I shall assume that, if they are reasonable and non problematic in some ordinary cases, then they can be used in order to understand what kind of knowledge the maker’s knowledge is. By this I mean the sort of knowledge that Alice enjoys when she holds the information (true content) that Bob’s coffee is sweetened because she just put two spoons of sugar in it herself. The maker’s knowledgetradition is quite important but it is not mainstream in modern and analytic epistemology and lacks grounding in terms of exactly what sort of knowledge one is talking about. My suggestion is that this grounding can be provided by a minimalist approach, based on an information-theoretical analysis. In the article, I argue that (a) we need to decouple a fourth distinction, namely informative versus uninformative, from the previous three and, in particular, from its implicit association with analytic versus synthetic and a priori versus a posteriori; (b) such a decoupling facilitates, and is facilitated by, moving from a monoagent to a multiagent approach: the distinctions qualify a proposition, a message, or some information not just in themselves but relationally, with respect to an informational agent; (c) the decoupling and the multiagent approach enable a re-mapping of currently available positions in epistemology (Classic, Innatist, Kant’s and Kripke’s) on these four dichotomies; (d) within such a re-mapping, two positions, capturing the nature of a witness’ knowledge and of a maker’s knowledge, can best be described as contingent, synthetic, a posteriori, and uninformativeand as contingent, synthetic, weakly a priori (ab anteriori), and uninformative respectively. In the conclusion, I indicate why the analysis of the maker’s knowledge has important consequences in all those cases in which the poietic (constructive) intervention on a system determines the truth of the model of that system.";What a maker’s knowledge could be;Floridi, L.;2018;Floridi, L.;Ethics and Philosophy of Information
It is a sign of our times that, when politicians speak of infrastructure nowadays, they often have in mind information and communication technologies (ICTs). They are not wrong. From success in business to cyber-conflicts, what makes contemporary societies work depends increasingly on bits rather than atoms. Depending on their digital infrastructures, societies may grow and prosper. And it is their ICTs that often represent one of their weakest sides, in terms of cyber security. We know all this. What is less obvious, and philosophically more interesting, is that ICTs also seem to have unveiled a new sort of equation. Consider the unprecedented emphasis that ICTs place on crucial phenomena such as accountability, intellectual property right, neutrality, openness, privacy, transparency, and trust. These are probably better understood in terms of a platform or infrastructure of social norms, expectations and rules that is there to facilitate or hinder the moral or immoral behaviour of the agents involved. By placing at the core of our life our informational interactions so significantly, ICTs have uncovered something that, of course, has always been there, but less visibly so in the past: the fact that moral behaviour is also a matter of “ethical infrastructure”, or what I have simply called infraethics.;Infraethics–on the Conditions of Possibility of Morality;Floridi, L.;2017;Floridi, L.;Ethics and Philosophy of Information
We investigate the causal uncertainty surrounding the flash crash in the U.S. Treasury bond market on October 15, 2014, and the unresolved concern that no clear link has been identified between the start of the flash crash at 9:33 and the opening of the U.S. equity market at 9:30. We consider the contributory effect of mini flash crashes in equity markets, and find that the number of equity mini flash crashes in the three-minute window between market open and the Treasury Flash Crash was 2.6 times larger than the number experienced in any other three-minute window in the prior ten weekdays. We argue that (a) this statistically significant finding suggests that mini flash crashes in equity markets both predicted and contributed to the October 2014 U.S. Treasury Bond Flash Crash, and (b) mini-flash crashes are important phenomena with negative externalities that deserve much greater scholarly attention.;The October 2014 United States Treasury bond flash crash and the contributory effect of mini flash crashes;Levine, Z., Hale, S. & Floridi, L.;2017;Floridi, L.;"Ethics and Philosophy of Information; Digital Politics and Government"
"Saturday, 27th of May 2017. An IT meltdown plunged into chaos British Airways’ flights at Heathrow and Gatwick airports. More than 75,000 passengers, including your philosopher, were grounded, during the busy May Bank Holiday weekend. Apparently, the cause was a mere inadvertent cutting of power, followed by an unauthorised and incorrect restoration. It took three days to get back to normal. Discomfort and damage to passengers were incalculable. Costs were significant: £58 million in compensation. The loss of reputation was serious, especially in a highly competitive environment, with limited margins, such as that of airlines. The system crashed again on the 2nd of August, this time only temporarily. Sunday, 28th of May 2017. First assessment of the damage caused by WannaCry. The malware hit Microsoft Windows systems, encrypting infected computer files and then asking for a ransom to decrypt them. This time your philosopher was safe, probably because he uses Apple systems. It is estimated that, at that date, WannaCry had already infected more than 230,000 computers (they will soon become 300,000) in 150 countries. It was one of the largest computer infections of all time. Among the affected organisations were Deutsche Bahn, FedEx, Telefónica, Renault, the Russian Ministry of Interior, and many others. In England, the National Health System (NHS) was forced to cancel appointments and send patients home. For a few days, we went back to paper and pen. Luckily, there were no casualties. Two disasters. But also two good examples of how fragile the digital is, how wide and risky our dependence on it is becoming, and how systemic the problems caused by digital failures can be. Fragility is not necessarily a bad feature all the time. It can also be a positive, even precious, quality, if, for example, we are talking about the delicacy of a crystal vase. But it becomes a problem when it denotes the unreliability of a system, and the more so the more we depend on it. So when the system in question is that digital sofa on which our information society is increasingly couchpotatoing, we should really worry about it. We live in increasingly complex environments, which work only thanks to digital technologies. We fight complexity with even more complexity. Suffice it to think that, in 2025, 70% of the world’s population will be urbanised. Cities may not become very smart very soon, but they are growing really fast and certainly becoming reliant on the digital to work, safely and securely. Digital fragility is a serious issue. The question is whether anything can be done to make the digital less fragile and more reliable. Resignation is not a strategy. On the contrary, much can and should be done to strengthen the digital and ensure that the damage is at least limited, if not entirely avoided, whenever the “digital sofa” collapses under our bottoms. It is not too early to act on the fragility of the digital, and not just for individuals, but also for companies and institutions. Because the cost of doing nothing is now unsustainable. Let’s start with a classic solution: if the system collapses, there should be a backup that kicks in. It is called resilience through redundancy. Add one leg to the sofa, to use the previous analogy. Or make sure that there is at least one spare wheel, like in my old Vespa. The trouble with British Airways was to cut costs so radically that when the spare wheel was needed, it was not there. Unfortunately, just like in my new Vespa. In the case of personal systems, individuals have long had the experience of making a full and updated backup. If your computer is blocked by a malware, it is tedious, but you can format and reinstall a clean copy. If only it were so simple for the whole information society and the analogue world. Another solution is “reflexivity”. In digital systems, everything is zero and one: the operating system, the programs managed by the operating system, and the data manipulated by the programs. This is so obvious today that we take it for granted. Yet, it is quite amazing if you think of it for a moment. It is as if the driving system, the engine and the petrol of the car were all made of the same interchangeable substance. Or as if bottles and glasses were made of solid ice, and you could use them to pour and drink water. Thanks to such reflexivity, the digital is able to work with itself, on itself, for itself. WannaCry infected so many computers because too many did not have the necessary updates to block it, forgetting that reflexivity does indeed make the digital so fragile, since malware too is made of zeros and one, but it is also what allows the digital to defend itself, with an antivirus for example. The future will increasingly need the positive feature of digital reflexivity, without which we will not be able to protect the software of our driverless cars, for example. The next malware might be called WannaDrive and force us to walk. More generally, one solution to cope with the fragility of the digital is more digital, not less. The way forward is to have software that monitors, repairs, and improves other software. And talking about cars, there is always a third, old-fashioned solution: insurance. According to a Financial Times article,Footnote 1 the cyber insurance market grew significantly after WannaCry. Unfortunately, we often shut the stable door after the horse has bolted. Today, the cyber insurance sector has a value of about $3 to $4 billion a year but, according to Allianz, it could easily reach $20 billion in 2025, becoming one of the fastest-growing segments in the insurance industry. Finally, we should not underestimate the usefulness of fragility itself. We constantly use it to our own advantage in the artefacts we build, by introducing fuses, valves, safety glasses, emergency break glasses, and lifesaving devices of all kinds. If something goes wrong, the system degrades in a calculated and limited way, and we know where and how to intervene. We should do the same thing with the digital. Accidentally, that is what happened with WannaCry. It is unclear why, but the malware had a peculiar feature: before infecting a computer, it checked whether a dark website had been registered. Following a negative response—the site did not exist—the malware infected the files. So a young British researcher registered the website. The result was surprising: the malware, finding that the URL had been registered, stopped infecting new computers. In other words, the creation of the website played the role of a kill switch. Similar measures involve the creation of so-called honeypots, virtual baits that provide apparently legitimate data, such as a website, that are safely isolated and monitored to detect and block attackers. WannaCry caused huge economic losses. According to one estimate, they could reach up to $4 billion.Footnote 2 But in another sense, we got lucky. There were no human tragedies. Yet the alarm should be loud and clear. It is time that we all accept our responsibilities and act on them. First, the organisations. For example, the National Security Agency was aware of the vulnerability in Windows that was exploited by WannaCry. They should have warned everyone as early as possible, and not kept it secret, in order to exploit it as a possible cyber weapon in the future. They launched the alarm too late, after the information had been stolen. How many other vulnerabilities are the spooks still hiding from us? Then there are companies like Microsoft, which will have to become more accountable as the first line of defence. It is not acceptable to abandon old operating systems as no longer supported or patchable; because these are still used anywhere in digital environments and of course a chain is only as robust as its weakest ring. And finally, there is ourselves, the users. We need to be more responsible because, just like in health care, a small percentage of unvaccinated people is at risk of endangering a whole population. The digital can take care of itself, but like in all systemic contexts, the solutions work only if everyone collaborates. No matter how robust the other legs of the couch may be, if one is fragile, we will always end up getting hurt. And it will be just our fault.";The Unsustainable Fragility of the Digital, and What to Do About It;Floridi, L.;2017;Floridi, L.;Ethics and Philosophy of Information
Since approval of the European Union General Data Protection Regulation (GDPR) in 2016, it has been widely and repeatedly claimed that a ‘right to explanation’ of all decisions made by automated or artificially intelligent algorithmic systems will be legally mandated by the GDPR once it is in force, in 2018. However, there are several reasons to doubt both the legal existence and the feasibility of such a right. In contrast to the right to explanation of specific automated decisions claimed elsewhere, the GDPR only mandates that data subjects receive meaningful, but properly limited, information (Articles 13–15) about the logic involved, as well as the significance and the envisaged consequences of automated decision-making systems, what we term a ‘right to be informed’. The ambiguity and limited scope of the ‘right not to be subject to automated decision-making’ contained in Article 22 (from which the alleged ‘right to explanation’ stems) raises questions over the protection actually afforded to data subjects. These problems show that the GDPR lacks precise language as well as explicit and well-defined rights and safeguards against automated decision-making, and therefore runs the risk of being toothless. We propose a number of legislative steps that, if implemented, may improve the transparency and accountability of automated decision-making when the GDPR comes into force in 2018.;Why a Right to Explanation of Automated Decision-Making Does Not Exist in the General Data Protection Regulation;Wachter, S., Mittelstadt, B. & Floridi, L.;2017;Floridi, L.;"Ethics and Philosophy of Information; Digital Politics and Government; Information Governance and Security"
Online technologies enable vast amounts of data to outlive their producers online, thereby giving rise to a new, digital form of afterlife presence. Although researchers have begun investigating the nature of such presence, academic literature has until now failed to acknowledge the role of commercial interests in shaping it. The goal of this paper is to analyse what those interests are and what ethical consequences they may have. This goal is pursued in three steps. First, we introduce the concept of the Digital Afterlife Industry (DAI), and define it as an object of study. Second, we identify the politico-economic interests of the DAI. For this purpose, we develop an analytical approach based on an informational interpretation of Marxian economics. Third, we explain the practical manifestations of the interests using four real life cases. The findings expose the incentives of the DAI to alter what is referred to as the “informational bodies” of the dead, which in turn is to be seen as a violation of the principle of human dignity. To prevent such consequences, we argue that the ethical conventions that guide trade with remains of organic bodies may serve as a good model for future regulation of DAI.;The Political Economy of Death in the Age of Information: A Critical Approach to the Digital Afterlife Industry;Öhman, C. & Floridi, L.;2017;Floridi, L.;Ethics and Philosophy of Information
The digital is deeply transforming reality. This is much obvious and uncontroversial. The real questions are why, how, and so what. In each case, the answer is far from trivial and definitely open to debate. To explain the ones I find most convincing, let me start in medias res, that is, from the “how”. It will then be easier to move backwards, to understand “why”, and then forward, to deal with the “so what”. The digital “cuts and pastes” reality, in the sense that it couples, decouples, or recouples features of the world—and therefore our corresponding assumptions about them—which we never thought could be anything but indivisible and unchangeable. It splits apart and fuses the “atoms” of our experience and culture, so to speak. It changes the bed of the river, to use a Wittgensteinian metaphor. A few stark examples bring home the point vividly.;Digital’s Cleaving Power and Its Consequences;Floridi, L.;2017;Floridi, L.;Ethics and Philosophy of Information
To create fair and accountable AI and robotics, we need precise regulation and better methods to certify, explain, and audit inscrutable systems.;Transparent, explainable, and accountable AI for robotics;Wachter, S., Mittelstadt, B. & Floridi, L.;2017;Floridi, L.;"Ethics and Philosophy of Information; Digital Politics and Government; Information Governance and Security"
"AI and robots continue to make news. Alarmist headlines used to be about some kind of Terminator developing in the future to dominate and enslave us, like an inferior species. They are now about tireless machines that, like enslaved persons, will make us redundant, replacing and outperforming us more efficiently and cheaply than we can ever be. This master-slave dialectics is not science fiction. On the 16th of February 2017, the plenary session of the European Parliament voted in favour of a resolution. to create a new ethical-legal framework according to which robots may qualify as “electronic persons”. The Commission does not have to follow the Parliament’s recommendations but, if it refuses, it will have to explain why. The following day, on the 17th of February, in an interview with Quartz, Bill Gates, Microsoft co-founder, suggested that there should be a tax on robots. Regulating robots is a very reasonable idea. Today, we live onlife, spending increasing amount of time inside the infosphere. In this digital ocean, robots are the real natives: we scuba dive, they are like fish. So robots of all kinds are going to multiply and proliferate, making the infosphere even more their own space. These smart, autonomous, and social agents perform an increasing number of tasks better than we can. Some of them are already among us. Others are discernible on the horizon, while later generations are still unforeseeable. The solutions that have already arrived come in soft forms, such as apps, webbots, algorithms, and software of all kinds; and hard forms, such as robots, house appliances, personal assistants, smart watches, and other gadgets. In health care, for example, robots and AI solutions are joining nurses, doctors, social workers, technicians, and experts, such as radiologists, by helping perform functions that, just a few years ago, were considered off-limits for technological disruption: cataloguing images, suggesting diagnosis, monitoring and even moving patients, interpreting radiographies, controlling insulin pumps, extracting new medical information from huge data sets, and so forth. Many trivial, routine tasks will be performed automatically either by AI or by people aided by AI. This is good news. We need AI to deal with increasing levels of complexity and difficulty. With an analogy, we need to remember that the best chess player is neither a human nor a computer, but a human using a computer.  While we can only guess at the scale of the coming disruption, everybody expects it to be profound. Any job in which people serve as menial interfaces—e.g. adjusting the dose of a medication for a patient—is now at risk. Yet new jobs will appear because we will need to manage and coordinate AI solutions. For example, someone will need to ensure that the data collected by insulin pumps and by smart apps are properly combined in order to improve the health care provided and the technologies of the future. What is more, many tasks will not be cost-effective for AI applications. The world never changes at the same pace. In some places, nurses will be irreplaceable for many routine tasks while in others they may coordinate and direct semi-autonomous robots through smart tablets and apps. And some old jobs will survive, even when a machine is doing most of the work: a doctor who delegates some routine tasks to a smart digital assistant will simply have more time to focus on other things, such as prevention. Jobs that were economically not viable until yesterday will become available. Finally, other tasks will be delegated back to us—the patients—to perform them as users, such as testing for blood pressure, something trivial and routine in many countries but still impossible in others. Another source of uncertainty concerns the point at which AI will no longer be controlled only by a guild of scientists, technicians, and managers. Still relying on the health care example, what will happen when AI becomes “democratised” and a “digital doctor” is available to millions of people on their smartphones or some other device? As Elena Bonfiglioli and Mathias Ekman recently wroteFootnote 5: “As you think innovation in health, you want to think about how to scale the adoption of systems of intelligence making them accessible in more intuitive ways. The vision of AI as “conversations” will empower intelligent health experiences that mirror the way people collaborate and interact with one another, and the way machines proactively understand our intent. […] Systems of intelligence will endemically transform the way we innovate for improved cancer outcomes, the way we optimise clinical and operational processes, and the way we think and do prevention. So, what if people across the healthcare continuum could collaborate and use machine learning to come up with ways to catch cancer earlier and improve outcomes for patients?”. We should investigate how we are going to socialise such systems of intelligence and how we shall best adopt them and adapt to them, from an ethical perspective, because many solutions are far from inevitable, and some may be preferable to others and should be privileged. There is no dystopian science-fiction scenario. Brave New World is not coming to life, and the Terminator is not lurking just beyond the horizon, either. There is a good chance that Satya Nadella, Microsoft CEO, may be right when he remarked: “humans and machines will work together – not against one another. Computers may win at games, but imagine what’s possible when human and machine work together to solve society’s greatest challenges like beating disease, ignorance, and poverty.”Footnote 6 But there are of course risks and challenges in how we shall develop and socialise AI systems and we should tackle them now, to ensure that individual and social benefits are maximised. Quoting Nadella once more: “The most critical next step in our pursuit of A.I. is to agree on an ethical and empathic framework for its design”. Add machine learning to artificial intelligence and robotics, mix these ingredients with the Internet, the Web, smart phones and apps, cloud computing, big data, and the Internet of Things, and it becomes obvious that there is no time to waste. We are laying down the foundations of the mature information societies of the near future, so we need new ethical solutions for the infosphere, to determine which forms of artificial agency and interactions we like to see flourishing in it. Against this background, one can look at the normative initiative taken by the European Parliament or the debate that has followed Gate’s suggestion with a mixed sense of excitement for the aspiration but disappointment for the implementation. For there is too much fantasy about speculative scenarios and too little imagination in designing realistic solutions that could work well. Consider two key issues: jobs and responsibilities. Robots replace human workers. Retraining unemployed people was never easy, but it is more challenging now that technological disruption is spreading so rapidly, widely, and unpredictably. Today, a bus driver replaced by a driverless bus is unlikely to become a web master, not least because even that job is at risk of automation. There will be many new forms of employment in other corners of the infosphere. Think of how many people have opened virtual shops on eBay. But these will require new and different skills. So more education and a universal basic income may be needed to mitigate the impact of robotics on the labour market, while ensuring a more equitable redistribution of its economic benefits. This means that society will need more resources. Unfortunately, robots do not pay taxes. And it is unlikely that more profitable companies may pay enough more taxes to compensate for the loss of revenues. So robots cause a higher demand for taxpayers’ money but also a lower supply of it. The problem is exacerbated by the fact that people with low income purchase cheap goods, those produced more efficiently by increasingly roboticised processes. How can one get out of this tailspin? The report correctly identifies the problem. But its original recommendation of a robotax on companies that employ robots may be unfeasible—for what exactly counts as a robot, if you need to pay a tax on it?—and counterproductive, for a robotax would disincentive innovation. The final text approved by the European Parliament shuns the recommendation but does not offer an alternative solution to the revenue problem. Consider next the allocation of responsibilities. If a robot breaks the window of my neighbour, who is responsible? The company who produced it, the shop who sold it, I the owner, or the robot itself, if the robot has become completely autonomous through a learning process and is now capable of intelligent-looking actions? In this case too, the report identifies the issue. It rightly recommends forms of risk management (insurance and compensation). But it also suggests the creation of a “specific legal status” for more advanced robots, as “electronic persons responsible for making good any damage they may cause”. This has been approved in the final document. As a result, we may see a future in which companies do not pay a robotax and are not even liable for some kinds of robots. This is probably a mistake. There is no need to adopt science fiction solutions to solve practical problems of legal liability with which jurisprudence has been dealing successfully for a long time. If robots become one day as good as human agents—think of Droids in Star Wars—we may adapt rules as old as Roman law, according to which the owner of an enslaved person was responsible for any damage caused by that person (respondeat superior). As the Romans already knew, attributing some kind of legal personality to robots would deresponsabilise those who should control them. Not to speak of the counterintuitive attribution of rights. For example, do robots as “electronic persons” have the right to own the data they produce (machine-generate data)? Should they be “liberated”? It may be fun to speculate about such questions, but it is also distracting and irresponsible, given the pressing issues we have at hand. The point is not to decide whether robots will qualify some day as a kind of persons, but to realise that we are stuck within the wrong conceptual framework. The digital is forcing us to rethink new solutions for new forms of agency. While doing so we must keep in mind that the debate is not about robots but about us, who will have to live with them, and about the kind of infosphere and societies we want to create. We need less science fiction and more philosophy.";Robots, Jobs, Taxes, and Responsibilities;Floridi, L.;2017;Floridi, L.;Ethics and Philosophy of Information
In recent years, there has been a huge increase in the number of bots online, varying from Web crawlers for search engines, to chatbots for online customer service, spambots on social media, and content-editing bots in online collaboration communities. The online world has turned into an ecosystem of bots. However, our knowledge of how these automated agents are interacting with each other is rather poor. Bots are predictable automatons that do not have the capacity for emotions, meaning-making, creativity, and sociality and it is hence natural to expect interactions between bots to be relatively predictable and uneventful. In this article, we analyze the interactions between bots that edit articles on Wikipedia. We track the extent to which bots undid each other’s edits over the period 2001–2010, model how pairs of bots interact over time, and identify different types of interaction trajectories. We find that, although Wikipedia bots are intended to support the encyclopedia, they often undo each other’s edits and these sterile “fights” may sometimes continue for years. Unlike humans on Wikipedia, bots’ interactions tend to occur over longer periods of time and to be more reciprocated. Yet, just like humans, bots in different cultural environments may behave differently. Our research suggests that even relatively “dumb” bots may give rise to complex interactions, and this carries important implications for Artificial Intelligence research. Understanding what affects bot-bot interactions is crucial for managing social media well, providing adequate cyber-security, and designing well functioning autonomous vehicles.;Even good bots fight: The case of Wikipedia;Tsvetkova, M., Garcia-Gavilanes, R. & Floridi, L. et al.;2017;Floridi, L.;Ethics and Philosophy of Information
Contemporary science seems to be caught in a strange predicament. On the one hand, it holds a firm and reasonable commitment to a healthy naturalistic methodology, according to which explanations of natural phenomena should never overstep the limits of the natural itself. On the other hand, contemporary science is also inextricably and now inevitably dependent on ever more complex technologies, especially Information and Communication Technologies, which it exploits as well as fosters. Yet such technologies are increasingly “artificialising” or “denaturalising” the world, human experiences and interactions, as well as what qualifies as real. So the search for the ultimate explanation of the natural seems to rely upon, and promote, the development of the artificial, seen here as an instantiation of the non-natural. In this article, I would like to try and find a way out of this apparently strange predicament. I shall argue that the naturalisation of our knowledge of the world is either philosophically trivial (naturalism as anti-supernaturalism and anti-preternaturalism), or mistaken (naturalism as anti-constructionism). First, I shall distinguish between different kinds of naturalism. Second, I shall remind the reader that the kinds of naturalism that are justified today need to be protected and supported pragmatically, but they are no longer very interesting conceptually. We know how to win the argument. We just have to keep winning it. Whereas the kind of naturalism that is still interesting today is now in need of revision in order to remain acceptable. Such a kind of naturalism may be revised on the basis of a realistic philosophy of information, according to which knowing is a constructive activity, through which we do not merely represent the phenomena we investigate passively, but create more or less correct informational models (semantic artefacts) of them, proactively and interactively. I shall conclude that the natural is in itself artefactual (a semantic construction), and that the information revolution is disclosing a tension not between the natural and the non-natural, but a deeper one between a user’s and a producer’s interpretation of knowledge. The outcome is a philosophical view of knowledge and science in the information age that may be called constructionist and a revival of philosophy as a classic, foundationalist enterprise.;A Plea for Non-naturalism as Constructionism;Floridi, L.;2017;Floridi, L.;Ethics and Philosophy of Information
In this article, I outline a logic of design of a system as a specific kind of conceptual logic of the design of the model of a system, that is, the blueprint that provides information about the system to be created. In section two, I introduce the method of levels of abstraction as a modelling tool borrowed from computer science. In section three, I use this method to clarify two main conceptual logics of information (i.e., modelling systems) inherited from modernity: Kant’s transcendental logic of conditions of possibility of a system, and Hegel’s dialectical logic of conditions of in/stability of a system. Both conceptual logics of information analyse structural properties of given systems. Strictly speaking, neither is a conceptual logic of information about (or modelling) the conditions of feasibility of a system, that is, neither is a logic of information as a logic of design. So, in section four, I outline this third conceptual logic of information and then interpret the conceptual logic of design as a logic of requirements, by introducing the relation of “sufficientisation”. In the conclusion, I argue that the logic of requirements is exactly what we need in order to make sense of, and buttress, a constructionist (poietic) approach to knowledge.;The Logic of Design as a Conceptual Logic of Information;Floridi, L.;2017;Floridi, L.;Ethics and Philosophy of Information
Cloud computing is rapidly gaining traction in business. It offers businesses online services on demand (such as Gmail, iCloud and Salesforce) and allows them to cut costs on hardware and IT support. This is the first paper in business ethics dealing with this new technology. It analyzes the informational duties of hosting companies that own and operate cloud computing datacentres (e.g., Amazon). It considers the cloud services providers leasing ‘space in the cloud’ from hosting companies (e.g., Dropbox, Salesforce). And it examines the business and private ‘clouders’ using these services. The first part of the paper argues that hosting companies, services providers and clouders have mutual informational (epistemic) obligations to provide and seek information about relevant issues such as consumer privacy, reliability of services, data mining and data ownership. The concept of interlucency is developed as an epistemic virtue governing ethically effective communication. The second part considers potential forms of government restrictions on or proscriptions against the development and use of cloud computing technology. Referring to the concept of technology neutrality, it argues that interference with hosting companies and cloud services providers is hardly ever necessary or justified. It is argued, too, however, that businesses using cloud services (e.g., banks, law firms, hospitals etc. storing client data in the cloud) will have to follow rather more stringent regulations.;The Ethics of Cloud Computing;de Bruin, B. & Floridi, L.;2017;Floridi, L.;Ethics and Philosophy of Information
The concept of distributed moral responsibility (DMR) has a long history. When it is understood as being entirely reducible to the sum of (some) human, individual and already morally loaded actions, then the allocation of DMR, and hence of praise and reward or blame and punishment, may be pragmatically difficult, but not conceptually problematic. However, in distributed environments, it is increasingly possible that a network of agents, some human, some artificial (e.g. a program) and some hybrid (e.g. a group of people working as a team thanks to a software platform), may cause distributed moral actions (DMAs). These are morally good or evil (i.e. morally loaded) actions caused by local interactions that are in themselves neither good nor evil (morally neutral). In this article, I analyse DMRs that are due to DMAs, and argue in favour of the allocation, by default and overridably, of full moral responsibility (faultless responsibility) to all the nodes/agents in the network causally relevant for bringing about the DMA in question, independently of intentionality. The mechanism proposed is inspired by, and adapts, three concepts: back propagation from network theory, strict liability from jurisprudence and common knowledge from epistemic logic. This article is part of the themed issue ‘The ethical impact of data science’.;Faultless responsibility: on the nature and allocation of moral responsibility for distributed moral actions;Floridi, L.;2016;Floridi, L.;Ethics and Philosophy of Information
"On 14 April 2016, the European Parliament approved the General Data Protection Regulation (GDPR). The expression “human dignity” appears in Article 88, which indicates that rules “shall include suitable and specific measures to safeguard the data subject’s human dignity [my italics], legitimate interests and fundamental rights, with particular regard to the transparency of processing, the transfer of personal data within a group of undertakings, or a group of enterprises engaged in a joint economic activity and monitoring systems at the work place” Council of the European Union (2016). The text just quoted contains two assumptions: that the data subject must be a human person, whose dignity is safeguarded (a legal person could not enjoy human dignity); and that human dignity is different from “legitimate interests and fundamental rights”. Both are correct, and the second assumption is indicative. Despite its almost invisible presence in the GDPR, human dignity is the fundamental concept that provides the framework within which one needs to interpret what the GDPR—and more generally European culture and jurisdiction (Lynskey (2015))—understand by informational privacy (henceforth only privacy). This is coherent with the role played by the concept both in the 1948 Universal Declaration of Human Rights (Preamble and Article 1) and in the EU Charter of Fundamental Rights.";On Human Dignity as a Foundation for the Right to Privacy;Floridi, L.;2016;Floridi, L.;Ethics and Philosophy of Information
This theme issue has the founding ambition of landscaping data ethics as a new branch of ethics that studies and evaluates moral problems related to data (including generation, recording, curation, processing, dissemination, sharing and use), algorithms (including artificial intelligence, artificial agents, machine learning and robots) and corresponding practices (including responsible innovation, programming, hacking and professional codes), in order to formulate and support morally good solutions (e.g. right conducts or right values). Data ethics builds on the foundation provided by computer and information ethics but, at the same time, it refines the approach endorsed so far in this research field, by shifting the level of abstraction of ethical enquiries, from being information-centric to being data-centric. This shift brings into focus the different moral dimensions of all kinds of data, even data that never translate directly into information but can be used to support actions or generate behaviours, for example. It highlights the need for ethical analyses to concentrate on the content and nature of computational operations—the interactions among hardware, software and data—rather than on the variety of digital technologies that enable them. And it emphasizes the complexity of the ethical challenges posed by data science. Because of such complexity, data ethics should be developed from the start as a macroethics, that is, as an overall framework that avoids narrow, ad hoc approaches and addresses the ethical impact and implications of data science and its applications within a consistent, holistic and inclusive framework. Only as a macroethics will data ethics provide solutions that can maximize the value of data science for our societies, for all of us and for our environments. This article is part of the themed issue ‘The ethical impact of data science’.;What is data ethics?;Floridi, L. & Taddeo, M.;2016;Floridi, L.;Ethics and Philosophy of Information
In information societies, operations, decisions and choices previously left to humans are increasingly delegated to algorithms, which may advise, if not decide, about how data should be interpreted and what actions should be taken as a result. More and more often, algorithms mediate social processes, business transactions, governmental decisions, and how we perceive, understand, and interact among ourselves and with the environment. Gaps between the design and operation of algorithms and our understanding of their ethical implications can have severe consequences affecting individuals as well as groups and whole societies. This paper makes three contributions to clarify the ethical importance of algorithmic mediation. It provides a prescriptive map to organise the debate. It reviews the current discussion of ethical aspects of algorithms. And it assesses the available literature in order to identify areas requiring further work to develop the ethics of algorithms.;The ethics of algorithms: Mapping the debate;Mittelstadt, B., Allo, P. & Taddeo, M. et al.;2016;Floridi, L.;Ethics and Philosophy of Information
Recent years have seen a surge in online collaboration between experts and amateurs on scientific research. In this article, we analyse the epistemological implications of these crowdsourced projects, with a focus on Zooniverse, the world’s largest citizen science web portal. We use quantitative methods to evaluate the platform’s success in producing large volumes of observation statements and high impact scientific discoveries relative to more conventional means of data processing. Through empirical evidence, Bayesian reasoning, and conceptual analysis, we show how information and communication technologies enhance the reliability, scalability, and connectivity of crowdsourced e-research, giving online citizen science projects powerful epistemic advantages over more traditional modes of scientific investigation. These results highlight the essential role played by technologically mediated social interaction in contemporary knowledge production. We conclude by calling for an explicitly sociotechnical turn in the philosophy of science that combines insights from statistics and logic to analyse the latest developments in scientific research.;Crowdsourced science: sociotechnical epistemology in the e-research paradigm;Watson, D. & Floridi, L.;2016;Floridi, L.;Ethics and Philosophy of Information
The British referendum in favour of the exit from the European Union, known as Brexit, has been described as “probably the most disastrous single event in British history since the second world war” (Wolf, 2016). I agree. But, as the phrase goes, never waste a good crisis, especially in terms of understanding what went wrong, and what could be avoided in the future. There are many lessons that can be learnt from Brexit. Three seem to be particularly relevant when it comes to the relationship between democracy and digital technologies.;Technology and Democracy: Three Lessons from Brexit;Floridi, L.;2016;Floridi, L.;Ethics and Philosophy of Information
The information revolution has changed the world profoundly and irreversibly at a breath-taking pace. Given its unprecedented scope, vital issues have emerged concerning the creation, management, and utilisation of information. The information revolution has brought enormous benefits and opportunities. However, it has outpaced our understanding of its foundations and consequences. It has raised conceptual issues that are rapidly expanding, evolving, and becoming increasingly serious. Today, philosophy faces the challenge of providing a foundational treatment of the concepts and phenomena underlying the information revolution, in order to foster our understanding and guide the responsible construction of our information society. Philosophy of information meets this challenge. It is a thriving new area of research that investigates the conceptual nature and basic principles of information, including its ethical consequences.;Introduction: The Philosophy of Information;Baumgaertner, B. & Floridi, L.;2016;Floridi, L.;Ethics and Philosophy of Information
We are so familiar with talk of “the information society” that we sometimes forget there is no such thing, but rather a multitude of societies, unalike from each other, some of which may qualify as information ones in different ways and degrees. So we should really speak of “information societies” without a “the” but with an “s”, and ensure that our generalizations are not so generic as to apply to all of them, while obliterating any salient distinction. Just to be clear, there is always a level of abstraction at which something is like anything else: the moon is like your umbrella, which is like a pizza, because they are all singular objects that exist and look round, for example. The point is not being smug about one’s own acrobatic equations (x is like y which is like z) but being critical in checking whether the level of abstraction at which the equation is drawn is the fruitful one to fulfil the purpose that one is pursuing. All this should clarify why, once we have many information societies that are all different from one another, it still makes sense to compare them in terms of relevant criteria and why, more specifically, it is important to understand what it means for an information society to be more or less mature than others.;Mature Information Societies—a Matter of Expectations;Floridi, L.;2016;Floridi, L.;Ethics and Philosophy of Information
Online service providers (OSPs)—such as AOL, Facebook, Google, Microsoft, and Twitter—significantly shape the informational environment (infosphere) and influence users’ experiences and interactions within it. There is a general agreement on the centrality of OSPs in information societies, but little consensus about what principles should shape their moral responsibilities and practices. In this article, we analyse the main contributions to the debate on the moral responsibilities of OSPs. By endorsing the method of the levels of abstract (LoAs), we first analyse the moral responsibilities of OSPs in the web (LoAIN). These concern the management of online information, which includes information filtering, Internet censorship, the circulation of harmful content, and the implementation and fostering of human rights (including privacy). We then consider the moral responsibilities ascribed to OSPs on the web (LoAON) and focus on the existing legal regulation of access to users’ data. The overall analysis provides an overview of the current state of the debate and highlights two main results. First, topics related to OSPs’ public role—especially their gatekeeping function, their corporate social responsibilities, and their role in implementing and fostering human rights—have acquired increasing relevance in the specialised literature. Second, there is a lack of an ethical framework that can (a) define OSPs’ responsibilities, and (b) provide the fundamental sharable principles necessary to guide OSPs’ conduct within the multicultural and international context in which they operate. This article contributes to the ethical framework necessary to deal with (a) and (b) by endorsing a LoA enabling the definition of the responsibilities of OSPs with respect to the well-being of the infosphere and of the entities inhabiting it (LoAFor).;The Debate on the Moral Responsibilities of Online Service Providers;Taddeo, M. & Floridi, L.;2016;Floridi, L.;Ethics and Philosophy of Information
Toleration is one of the fundamental principles that inform the design of a democratic and liberal society. Unfortunately, its adoption seems inconsistent with the adoption of paternalistically benevolent policies, which represent a valuable mechanism to improve individuals’ well-being. In this paper, I refer to this tension as the dilemma of toleration. The dilemma is not new. It arises when an agent A would like to be tolerant and respectful towards another agent B’s choices but, at the same time, A is altruistically concerned that a particular course of action would harm, or at least not improve, B’s well-being, so A would also like to be helpful and seeks to ensure that B does not pursue such course of action, for B’s sake and even against B’s consent. In the article, I clarify the specific nature of the dilemma and show that several forms of paternalism, including those based on ethics by design and structural nudging, may not be suitable to resolve it. I then argue that one form of paternalism, based on pro-ethical design, can be compatible with toleration and hence with the respect for B’s choices, by operating only at the informational and not at the structural level of a choice architecture. This provides a successful resolution of the dilemma, showing that tolerant paternalism is not an oxymoron but a viable approach to the design of a democratic and liberal society.;Tolerant Paternalism: Pro-ethical Design as a Resolution of the Dilemma of Toleration;Floridi, L.;2016;Floridi, L.;Ethics and Philosophy of Information
"The capacity to collect and analyse data is growing exponentially. Referred to as ‘Big Data’, this scientific, social and technological trend has helped create destabilising amounts of information, which can challenge accepted social and ethical norms. Big Data remains a fuzzy idea, emerging across social, scientific, and business contexts sometimes seemingly related only by the gigantic size of the datasets being considered. As is often the case with the cutting edge of scientific and technological progress, understanding of the ethical implications of Big Data lags behind. In order to bridge such a gap, this article systematically and comprehensively analyses academic literature concerning the ethical implications of Big Data, providing a watershed for future ethical investigations and regulations. Particular attention is paid to biomedical Big Data due to the inherent sensitivity of medical information. By means of a meta-analysis of the literature, a thematic narrative is provided to guide ethicists, data scientists, regulators and other stakeholders through what is already known or hypothesised about the ethical risks of this emerging and innovative phenomenon. Five key areas of concern are identified: (1) informed consent, (2) privacy (including anonymisation and data protection), (3) ownership, (4) epistemology and objectivity, and (5) ‘Big Data Divides’ created between those who have or lack the necessary resources to analyse increasingly large datasets. Critical gaps in the treatment of these themes are identified with suggestions for future research. Six additional areas of concern are then suggested which, although related have not yet attracted extensive debate in the existing literature. It is argued that they will require much closer scrutiny in the immediate future: (6) the dangers of ignoring group-level ethical harms; (7) the importance of epistemology in assessing the ethics of Big Data; (8) the changing nature of fiduciary relationships that become increasingly data saturated; (9) the need to distinguish between ‘academic’ and ‘commercial’ Big Data practices in terms of potential harm to data subjects; (10) future problems with ownership of intellectual property generated from analysis of aggregated datasets; and (11) the difficulty of providing meaningful access rights to individual data subjects that lack necessary resources. Considered together, these eleven themes provide a thorough critical framework to guide ethical assessment and governance of emerging Big Data practices.";The Ethics of Big Data: Current and Foreseeable Issues in Biomedical Contexts;Mittelstadt, B., & Floridi, L.;2016;Floridi, L.;Ethics and Philosophy of Information
"We do not check out a hotel personally; we rely on TripAdvisor. We may have never met a person, yet we are ‘friends’ on Facebook. We may press ‘like’ but engage only in some kind of slacktivism. It does not matter whether we haven’t got a clue about how to reach a place downtown, as long as we have access to Google Maps and follow the instructions. Five stars on Amazon may be sufficient to convince us of the quality of a product, even if we have never tried it ourselves. Being a ‘best seller’ in The New York Times Best Seller list is often a self-fulfilling prophecy. In all these cases, something (the signifier) signifies something else (the signified). Such ‘signifying’ is at the heart of every semantic and semiotic process. It is the immensely important relation of ‘standing for’. There is no sense, reference, or meaning without it. So, we have always helped ourselves to different kinds of signifying means, in order to interact with each other and the world, and make sense of both. We are the symbolic species after all, and twentieth-century philosophy—whether hermeneutically oriented or based on a philosophy of language—can easily be read in terms of a theory of signification. All this is clear, if complicated. The point here is that only our own culture, the culture that characterises mature information societies, is now evolving from being a culture of signs and signification into a culture of proxies and interaction. What is the difference? Why is this happening today? And what are the implications of such a major transformation? In order to answer these questions, one needs to understand better what a proxy is and what ‘degenerate’ proxies may be.";A Proxy Culture;Floridi, L.;2015;Floridi, L.;Ethics and Philosophy of Information
"One of the pressing challenges we face today—in a post-Westphalian order (emergence of the state as the modern, political information agent) and post-Bretton Woods world (emergence of non-state multiagent systems or MASs as “hyperhistorical” players in the global economy and politics)—is how to design the right kind of MAS that can take full advantage of the socio-economic and political progress made so far, while dealing successfully with the new global challenges that are undermining the best legacy of that very progress. This is the topic of the article. In it, I argue that (i) in order to design the right kind of MAS, we need to design the right kind of norms that constitute them; (ii) in order to design the right kind of constitutive norms, we need to identify and adopt the right kind of principles of normative design; (iii) toleration is one of those principles; (iv) unfortunately, its role as a foundation for the design of norms has been undermined by the “paradox of toleration”; (v) however, the paradox can be solved; (vi) so toleration can be re-instated as the right kind of foundational principle for the design of the right kind of norms that can constitute the right kind of MAS that can operate across cultures, societies and states, to help us to tackle the new global challenges facing us.";Toleration and the Design of Norms;Floridi, L.;2015;Floridi, L.;Ethics and Philosophy of Information
"In 1941, Aldous Huxley published Grey Eminence: A Study in Religion and Politics. It was the biography of François Leclerc du Tremblay. This French Capuchin friar was also known as l’éminence grise because of a robe he used to wear and because, although he was not a cardinal, he was as influential as one, in his role as advisor to His Eminence the Cardinal de Richelieu. Twice removed from the official source of power—Richelieu was in his turn King Louis XIII’s chief minister—du Tremblay profoundly shaped French and European politics and the course of the Thirty Years’ War. This was one of the longest and most destructive conflicts in European history, World War Zero really. It is such an ability to control events and people’s behaviour by influencing the influencers, behind the scenes, that I have in mind when coining the expression “grey power”. There is grey power in every society, and the two change together, as concauses. The whole process may sometimes be dramatic, even revolutionary, but it is never linear or regularly paced. Think of how European societies and their grey powers slowly changed because of the complex interactions between mercantilism, colonialism and the emergence of the so-called Westphalian system of sovereign states. Or how quickly the USA was transformed during the Gilded Age, from 1870s to about 1900, and the grey power exercised during that time by wealthy industrialists and financiers such as Andrew Carnegie, Andrew W. Mellon, J. P. Morgan or John D. Rockefeller. Changes in society and in grey power within it do not follow a domino-effect pattern; they are more like a complicated waltz in history’s ballroom, where society and grey power dance together, sometimes revisiting some corners, moving at varied pace and alternating between who leads whom. This long premise is needed to clarify that asking how grey power has evolved in order to adapt to our current societies is both a pressing question and a potential trap. It is pressing because grey power is not the same within mature information societies as it is in industrial, mass-media or ecclesiocratic ones. Unless we gain a better understanding of how the nature and exercise of grey power—in short, its morphology—has altered, we shall find developing a better society more challenging than it should be. We need to know that which we want to improve. A scholarly history of grey power would be a very interesting reading. Yet the question may become a trap if we are not wary of superficial simplifications. Recall that grey power is like ivy: it grows on the wall of official power and flourishes in full shade. At a time of great societal transformations and widespread conflicts, it is tempting to single out some headline-grabbing news as the factor driving the transformations in the morphology of grey power today. Immigration and terrorism, globalisation and the financial markets, the housing bubble and the reform of the banking system, inflation and deflation, hacktivism and slacktivism, cyberwar and the Second Cold War, the Euro and the European Union, multinationals and American cultural colonialism, the Arab Spring and Colour Revolutions, China’s GDP and the Greek crisis… the list is long but it may be distracting because it focuses on contingent historical phenomena that fail to identify the deeper shift in the means through which events and people’s behaviour are controlled or influenced, and hence are primed and prompted to relate with such historical phenomena within mature information societies. Using a different analogy, these are the waves on history’s surface. No matter how gigantic and even threatening, we need to focus on the underling currents that will still be there when the storm is over. We need to dive deeper, if we wish to understand the new morphology of grey power. So let me take the plunge.";The New Grey Power;Floridi, L.;2015;Floridi, L.;Ethics and Philosophy of Information
The anti-counterfeiting trade agreement (ACTA) was originally meant to harmonise and enforce intellectual property rights (IPR) provisions in existing trade agreements within a wider group of countries. This was commendable in itself, so ACTA’s failure was all the more disappointing. In this article, I wish to contribute to the post-ACTA debate by proposing a specific analysis of the ethical reasons why ACTA failed, and what we can learn from them. I argue that five kinds of objections—namely, secret negotiations, lack of consultation, vagueness of formulation, negotiations outside any international body, and the creation of a new governing body outside already existing forums—had only indirect ethical implications. This takes nothing away from their seriousness but it does make them less compelling, because agreements should be evaluated, ethically, for what they are, rather than for the alleged reasons why they are being proposed. I then argue that ACTA would have caused three ethical problems: an excessive and misplaced kind of responsibility, a radical decrease in freedom of expression, and a severe reduction in information privacy. I conclude by indicating three lessons that can help us in shaping a potential ACTA 2.0. First, we should acknowledge the increasingly vital importance of the framework of implicit expectations, attitudes, and practices that can facilitate and promote morally good decisions and actions. ACTA failed to perceive that it would have undermined the very framework that it was supposed to foster, namely one promoting some of the best and most successful aspects of our information society. Second, we should realise that, in advanced information societies, any regulation affecting how people deal with information is now bound to influence the whole ‘onlife’ habitat within which they live. So enforcing IPR becomes an environmental problem. Third, since legal documents, such as ACTA, emerge from within the infosphere that they affect, we should apply to the process itself, which one day may lead to a post-ACTA treaty, the very framework and ethical values that we would like to see promoted by it.;The anti-counterfeiting trade agreement: the ethical analysis of a failure, and its lessons;Floridi, L.;2015;Floridi, L.;Ethics and Philosophy of Information
It is Christmas and your aunt has knitted a scarf for you. It is beautiful, useful, and you needed it. In fact, you love it. You feel that she cares about you and understands your wishes. You are most grateful for such a thoughtful gift. It seems that everybody is happy. How can there be anything wrong with such a win-win scenario? This is the same rhetorical question asked by many defenders of free online services. The answer is supposed to be stronger than a mere “nothing”. It is supposed to be a much more positive “there is everything right with this!”. Indeed, the new rhetorical question becomes “what’s wrong with you, killjoy?”. For the digital gifts from Baidu, Expedia, Facebook, Flickr (Yahoo!), Google, Instagram (Facebook), LinkedIn, Microsoft, Tencent, TripAdvisor, Tumblr (Yahoo!), Twitter, Yahoo!, YouTube (Google), WhatsApp (Facebook) and all the other thousands of digital aunts, we seem to have online ensure that we, the giftees, are part of the information society, that we live on the good side of the digital divide and that we enjoy all the amazing fruits of our technological developments. And all these, free of charge. They make every day feel like Christmas. End of story? Not quite. Think more carefully and critically and you will realise that “timeo Danaos et dona ferentes” may still apply. The Latin phrase means “I fear the Greeks even when they bear gifts”. In the Aeneid, Virgil makes the Trojan priest Laocoön utter this now proverbial line, in his attempt to warn the Trojans not to accept the famous wooden horse left by the Greeks as an apparent gift. We know how it ends. And yet, we seem to be falling into the same trap. Let me explain.;Free Online Services: Enabling, Disenfranchising, Disempowering;Floridi, L.;2015;Floridi, L.;Ethics and Philosophy of Information
Artefacts do not always do what they are supposed to, due to a variety of reasons, including manufacturing problems, poor maintenance, and normal wear-and-tear. Since software is an artefact, it should be subject to malfunctioning in the same sense in which other artefacts can malfunction. Yet, whether software is on a par with other artefacts when it comes to malfunctioning crucially depends on the abstraction used in the analysis. We distinguish between “negative” and “positive” notions of malfunction. A negative malfunction, or dysfunction, occurs when an artefact token either does not (sometimes) or cannot (ever) do what it is supposed to. A positive malfunction, or misfunction, occurs when an artefact token may do what is supposed to but, at least occasionally, it also yields some unintended and undesirable effects. We argue that software, understood as type, may misfunction in some limited sense, but cannot dysfunction. Accordingly, one should distinguish software from other technical artefacts, in view of their design that makes dysfunction impossible for the former, while possible for the latter.;On malfunctioning software;Floridi, L., Fresco, N. & Primiero, G.;2015;Floridi, L.;Ethics and Philosophy of Information
What is uncertainty? There are of course several possible definitions, offered by different fields, from epistemology to statistics, but, in the background, one usually finds some kind of relation with the lack of information, in the following sense. Suppose we define semantic or factual information as the combination of a question plus the relevant, correct answer. If one has both the question and the correct answer, one is informed: “was Berlin the capital of Germany in 2010? Yes”. If one has the question but the incorrect answer, one is insipient. If one has neither, one is ignorant. And if one has only the question but not the answer, then one is uncertain. Uncertainty is what a correct answer to a relevant question erases. This is why, in information theory, the value of information is often discussed in terms of the amount of uncertainty that it decreases. And this is also why there are many things in life that we value, but uncertainty is not usually one of them. At first sight, this may seem to be unproblematic, indeed obvious. What we actually value is information, understandable now as the appropriate combination of relevant questions and correct answers, the Qs and the As. We value information because it is power: power to understand what happened, forecast what will happen and, hence, choose now among the things that could happen between the past and the future. Marx and the past two centuries thought that power, understood as the sociopolitical ability to control or influence people’s behaviour, was exercised through the creation or control of (the means of production of) things, i.e. goods and services. But it is equally clear that power is also exercised through the creation or control of (the means of production of) information about things, e.g. laws, statistics, news or technoscience. To use a trivial example, if you wish to buy a second-hand car, you value information about its past (was it involved in any accident? yes), its future (is it expensive to run? yes) and its present (should I haggle over the price? yes). The more information you have, the better you may shape your environment and control its development and the more advantage you may enjoy against competitors who lack such a resource.;The Politics of Uncertainty;Floridi, L.;2015;Floridi, L.;Ethics and Philosophy of Information
While the existence of a fundamental relationship between logic and information seems unquestionable, its precise nature has so far proved to be rather elusive and somewhat puzzling. The received view on this issue is epitomized by the traditional tenet that logical inference is ‘tautological’ (literally, repetitive and trivially true), namely that a valid inference is one in which the information carried by the conclusion is (in a sense variously specified) contained in the information carried by the (conjunction of the) premises. At the mid of the 20th century, Bar-Hillel and Carnap's controversial theory of ‘semantic information’ provided what is, to date, the strongest theoretical justification for this tenet. However, as remarked by a number of authors who have made the history of logic, including Frege, Dummett and Hintikka, the received view clashes with the intuitive idea that deductive arguments are useful just because, by their means, we obtain information that we did not possess before. Moreover, it also clashes with two 20th-century milestones of the theory of computation: the undecidability of first-order logic and the NP-hardness of propositional logic. How can logic be informationally trivial and, yet, computationally hard? Can we obtain an informational characterization of logical consequence that is more in tune with our intuition and with the negative results on the computational complexity of logical reasoning? Besides and beyond this well-known and puzzling difficulty—which has recently seen a revival of attention both for its philosophical and for its computational aspects—a deep interest in an informational view of logic has been pivotal in research on non-classical logics at least since Kripke and Urquhart provided informational semantics for intuitionistic logic (Kripke, 1965) and relevant logics (Urquhart, 1972). It has been pursued by authors such as Barwise and Perry, Ono and Komori, Girard, Van Benthem, Wansing and many others. Furthermore, the connection between logic and information has been the subject of extensive research in epistemic logics. From this point of view, logic allows for a clarification of the nature of different kinds of information and for deeper investigations into their formal properties. Over the last few years, a fresh view of these fundamental issues has been put forward by the development of philosophy of information as a specific, interdisciplinary and fast-expanding area of research. This new subject is rapidly attracting a large community of researchers from a variety of disciplines and is leading to a growing number of conferences and workshops all over the world. However, there is still no dedicated venue that focuses explicitly and uniquely on its contributions to the field of logic and computation. This corner welcomes research or survey papers on all aspects of the relation between logic and information, in broad areas such as: and in closely related areas, provided that they have a logico-mathematical or computational content.;The logic and philosophy of information corner: Presentation and call for papers;D'Agostino, M. &  Floridi, L.;2015;Floridi, L.;Ethics and Philosophy of Information
On October 6 and 7, the European Commission, with the participation of Portuguese authorities and the support of the Champalimaud Foundation, organised in Lisbon a high-level conference on “The Future of Europe is Science”. Mr. Barroso, President of the European Commission, opened the meeting. I had the honour of giving one of the keynote addresses. The explicit goal of the conference was twofold. On the one hand, we tried to take stock of European achievements in science, engineering, technology and innovation (SETI) during the last 10 years. On the other hand, we looked into potential future opportunities that SETI may bring to Europe, both in economic terms (growth, jobs, new business opportunities) and in terms of wellbeing (individual welfare and higher social standards). One of the most interesting aspects of the meeting was the presentation of the latest report on “The Future of Europe is Science” by the President’s Science and Technology Advisory Council (STAC). The report addresses some very big questions: How will we keep healthy? How will we live, learn, work and interact in the future? How will we produce and consume and how will we manage resources? It also seeks to outline some key challenges that will be faced by Europe over the next 15 years. It is well written, clear, evidence-based and convincing. I recommend reading it. In what follows, I wish to highlight three of its features that I find particularly significant.;Technoscience and Ethics Foresight;Floridi, L.;2014;Floridi, L.;Ethics and Philosophy of Information
"Information has always been at the core of conflicts. When Napoleon planned to invade Italy, he duly upgraded the first telegraph network in the world, the French “semaphore”. He famously remarked that “an army marches on its stomach,” but he also knew that the same army acted on information. As Von Clausewitz once stated “by the word ‘information’ we denote all the knowledge which we have of the enemy and his country; therefore, in fact, the foundation of all our ideas of actions [in war].” This is why the radar, the computer, the satellite, the GPS system, and the Internet were initially developed as military technologies, while unmanned vehicles are becoming a reality thanks to DARPA. The difference between then and now is that information warfare is acquiring kinetic aspects unknown to past generations. Information has become a weapon because the targets too have become informational. The phenomenon is well known. Today, those who live by the digit may die by the digit. This much is clear. The question is how we should understand such a macroscopic transformation. A popular interpretation is rooted in the eighties. Simulacra and Simulation, Jean Baudrillard’s book on the relationship between the real and the symbolic, was published in 1981. It was required reading for the actors in The Matrix, where it appears in the first episode. WarGames, the American Cold War science-fiction film, became a box office success in 1983. Both Simulacra and Simulation and WarGames suggested looking at the role of information in conflicts in terms of an increasing process of simulation, virtualization, and gamification. This was understandable but, today, it has also become misleading. To understand why, consider your bed in your house. This may be the actual object in which you sleep. Let’s define that the system. Or it may be an idea of your bed, say what you have in mind when you are at work. Let’s define that the model. When we treat something as real, we expect the system and the model to be correctly related. When this relation does not work correctly, we make mistakes, e.g., we bump into the bed (the system) at night because we think it (the model) is elsewhere. The virtual is not real, but it is not a mistake either. It is rather a model without a corresponding system. The engineer designing a bed is working with a model (blueprint) to which nothing yet in the world corresponds, it is a “virtual” bed, like the shadow of an object without its corresponding object. You know it is virtual because you cannot sleep in it, no more than you could sleep in Plato’s idea of the bed. One of the problems with Kant is that, when you lie in bed, you lie in the noumenal not just in the phenomenal bed, whatever that may be in itself. To change example a little: when you eat an apple, you eat both the system (whatever the noumenal apple in itself may be, in Kant’s terminology) and the model (whatever the phenomenal apple is perceived to be by you when eating it). Notice now that the decoupling between system and model cuts both ways. There is the opposite case in which, instead of there being a model without its system (the virtual), there is a system without its model. This is the case in which the representable remains unrepresented. Imagine an object without its shadow, like Peter Pan. There is no technical word for this, so let me use latent, in the original Latin sense of concealed or unknown. What there is in the world is either real (system + model), virtual (only the model), or latent (only the system). The world is full of latent things, systems that remain or are meant to remain below the threshold of the observable. The operations of the NSA were latent until Snowden disclosed them. Only then did they become real for all of us. Let us return to the nature of information warfare. In the past, war has always and only been real, in the system + model sense, like the bed in which you sleep and the apple you eat. The hard facts of war were inevitably accompanied by their informational shadows: the human shouting, the smell of horses, the sounds of trumpets in battles, the rhythm of machineguns, the pitched whistles of bombs falling from the sky, the smell of napalm, the marks left by the tanks’ tracks. For a short time, in the eighties, passive mass media and digital consumerism made us mistakenly think that war could be experienced by the public as virtual: a televised or computerized game, involving only representations to which nothing corresponded, like shadows without objects, simulacra in Baudrillard’s terminology. Thus, in 1991, Baudrillard argued in The Gulf War Did Not Take Place that the hi-tech fighting on the American side during the first Gulf War had transformed a conflict into propaganda and mass-mediated experience. The analysis was correct both in perceiving a difference and in identifying that difference in the decoupling between the system and the model. But it was wrong in selecting models as the new battlefields. Global information warfare is not virtual. It is mostly latent, that is, it is in the world but not experienced as part of the world. It is a war without shadows. You cannot see it, and cannot hear it, it silently happens everyday, can hit anyone anywhere, and we can all be its unaware victims. Take for instance distributed denial-of-service attacks. According to Arbor Networks, more than 2,000 of DDoS occur worldwide every day.Footnote 5 Their number is increasing and more and more countries are involved that are not officially at war with each other. Similar attacks are very cheap. According to TrendMicro Research a week-long DDoS attack, capable of taking a small organization offline, can cost as little as $150 in the underground market. This is just an example. Conflicts in the infosphere—not just DDoS attacks, but also trade wars, currency wars, patent wars, marketing wars, and other silent forms of informational battles to win hearts, minds, and wallets—are increasingly neither real nor virtual, but latent to most of their victims. They are nonetheless dangerous and wasteful. They require special interfaces to be perceived. They will require a special sensitivity to be eradicated.";The Latent Nature of Global Information Warfare;Floridi, L.;2014;Floridi, L.;Ethics and Philosophy of Information
In 1930, John Maynard Keynes published a masterpiece that should be a compulsory reading for any educated person, a short essay entitled Economic Possibilities for our Grandchildren(Keynes 1930, 1972).Footnote 1 It was an attempt to see what life would be like if peace, prosperity and techno-scientific developments were increasingly part of humanity’s future. Of course, things went otherwise. The Great Depression begun in the same year, and World War II soon followed. In the subsequent decades, other disasters, conflicts and crises awaited humanity. The essay became a philosophical exercise that could collect dust in the libraries. Yet the fact that history took such terrible and tragic steps back does not in any way detract from Keynes’ brilliant insights. And to a generation that never saw enemy tanks in the streets of Paris or Rome, and will celebrate the hundredth anniversary of the end of World War II, the essay has plenty of lessons to teach, especially about what we want to achieve in the future, our human project.;Technological Unemployment, Leisure Occupation, and the Human Project;Floridi, L.;2014;Floridi, L.;Ethics and Philosophy of Information
In this article, I define and then defend the principle of information closure (PIC) against a sceptical objection similar to the one discussed by Dretske in relation to the principle of epistemic closure. If I am successful, given that PIC is equivalent to the axiom of distribution and that the latter is one of the conditions that discriminate between normal and non-normal modal logics, a main result of such a defence is that one potentially good reason to look for a formalization of the logic of “𝑆S is informed that 𝑝p” among the non-normal modal logics, which reject the axiom, is also removed. This is not to argue that the logic of “𝑆S is informed that 𝑝p” should be a normal modal logic, but that it could still be insofar as the objection that it could not be, based on the sceptical objection against PIC, has been removed. In other word, I shall argue that the sceptical objection against PIC fails, so such an objection provides no ground to abandon the normal modal logic B (also known as KTB) as a formalization of “𝑆S is informed that 𝑝p”, which remains plausible insofar as this specific obstacle is concerned.;Information closure and the sceptical objection;Floridi, L.;2014;Floridi, L.;Ethics and Philosophy of Information
"The debate on Open Data and Data Protection focuses on individual privacy. How can the latter be protected while taking advantage of the enormous potentialities offered by ever-bigger Open Data and ever-smarter algorithms and applications? The tension is sometimes presented as being asymmetric: between the ethics of privacy and the politics of security. In fact, it is ultimately ethical. Two moral duties need to be reconciled: fostering human rights and improving human welfare. The tension is obvious if one considers medical contexts and biomedical Big Data, for example, where protection of patients’ records and cure or prevention of diseases need to go hand in hand (Howe et al. 2008; Groves et al. 2013). Currently, the balance between these two moral duties is implicitly understood within a classic ontological framework. The beneficiaries of the exercise of the two moral duties are the individual vs. the society to which the individual belongs. At first sight, this may seem unproblematic. We work on the assumption that these are the only two “weights” on the two sides of the scale. Such a framework is not mistaken, but it is dangerously reductive, and it should be expanded urgently. For there is a third “weight” that must be taken into account by Data Protection, that of groups and their privacy. Privacy as a group right is a right held by a group as a group rather than by its members severally. It is the group, not its members, that is correctly identified as the right-holder. A typical example is the right of self-determination, which is held by a nation as a whole.";Open Data, Data Protection, and Group Privacy;Floridi, L.;2014;Floridi, L.;Ethics and Philosophy of Information
Even if we protect personal data, the protection of groups won't take care of itself, argues Luciano Floridi. The debate on Open Data and Data Protection focuses on individual privacy. How can the latter be protected while taking advantage of the enormous potentialities offered by ever-bigger Open Data and even-smarter algorithms and applications? The tension is sometimes presented as being symmetric: between the ethics of privacy and the politics of security. In fact, it is ultimately ethical. Two moral duties need to be reconciled: fostering human rights and improving human welfare. The tension is obvious if one considers medical contexts and biomedical Big Data, for example, where protection of patients' records and cure or prevention of diseases need to go hand in hand.;Group privacy;Floridi, L.;2014;Floridi, L.;Ethics and Philosophy of Information
"Assuming one could momentarily step aside from the current pandemonium generated by big data, social networks, and smart phones; there is an obvious question that comes to one's mind: what will be the next wave of innovations? Of course, the only safe prediction, about forecasting the future, is that it is very easy to get it wrong. Who would have thought that, 20 years after the flop of the Newton (http://www.youtube.com/watch?v=MiNKMmyRiw4), we would have been queuing to buy an iPad? Sometimes, you just have to wait for the right Apple to fall on your head. Still, there are a couple of low-hanging fruits that seem to be sufficiently ripe to be worth monitoring. Some pundits have been talking about “the internet of things” for some years now (Atzori et al. (2010)). According to this view, the next revolution will not be the vertical development of some unchartered new technology, but horizontal. For it will be about connecting anything to anything, or a2a, not just humans to humans. They have a point. One day, you-name-it 2.0 will be passé, and we might be thrilled by a2a technologies. Even now, the fact that the Newton was advertised as being able to connect to a printer sounds quite amazing. Imagine a world in which your car autonomously checks your electronic diary and reminds you, through your digital TV, that you need to get some petrol tomorrow, before your long-distance commuting. All this and more is already feasible. The greatest obstacles are a lack of shared standards, limited protocols, and hardware that is not designed to be fully modular with the rest of the infosphere. It is a problem of integration and de-fragmentation, which we routinely solve by forcing humans to work like interfaces. We connect the printer to the computer, we translate the GPS's instructions into driving manoeuvres, and we make the fridge talk to the grocery supermarket. Essentially, the internet of things is about getting rid of us, the cumbersome humans in the loop. In a defragmented and fully integrated infosphere, the invisible coordination between gadgets will be as seamless as the way in which your iPhone interacts with your iMac.";Things;Floridi, L.;2013;Floridi, L.;Ethics and Philosophy of Information
Philosophers have relied on visual metaphors to analyse ideas and explain their theories at least since Plato. Descartes is famous for his system of axes, and Wittgenstein for his first design of truth table diagrams. Today, visualisation is a form of ‘computer-aided seeing’ information in data. Hence, information is the fundamental ‘currency’ exchanged through a visualisation pipeline. In this article, we examine the types of information that may occur at different stages of a general visualization pipeline. We do so from a quantitative and a qualitative perspective. The quantitative analysis is developed on the basis of Shannon’s information theory. The qualitative analysis is developed on the basis of Floridi’s taxonomy in the philosophy of information. We then discuss in detail how the condition of the ‘data processing inequality’ can be broken in a visualisation pipeline. This theoretic finding underlines the usefulness and importance of visualisation in dealing with the increasing problem of data deluge. We show that the subject of visualisation should be studied using both qualitative and quantitative approaches, preferably in an interdisciplinary synergy between information theory and the philosophy of information.;An analysis of information visualisation;Chen, M. & Floridi, L.;2013;Floridi, L.;Ethics and Philosophy of Information
"‘Civilised’, ‘cultured’, and ‘educated’: perhaps there were times when these words could rightly be treated as synonymous. Thucydides and Cicero may come to mind. Some characters in Jane Austin, Henry James, or Edith Wharton seem to draw little distinction between the three corresponding concepts. Yet today they hardly overlap at all. ‘Civilised’ refers to a person’s manners and behaviours; ‘cultured’ qualifies someone who is engaged with arts, letters, and other intellectual pursuits; and ‘educated’ is usually applied to people who have successfully attended learning or training courses offered by primary, secondary, or tertiary (higher) institutions. One could be any of the three without being either of the remaining two. Globalisation has greatly contributed to this differentiation, even if it has been pushing it in opposite directions, local and global. Montaigne already knew that ‘civilised’ and ‘cultured’ had local interpretations. The difference is that today, we feel increasingly less justified in prioritising one ‘locality’ over the others, be this Rio de Janeiro, New Delhi, Beijing, or Tokyo. We know that it is a matter of civilised manners both to take off one’s shoes and to keep them on, it depends on where we are or whom we are visiting. We accept that Alice may be cultured even if she has no clue about Bossa Nova music, Sattriya dance, Sichuanese opera, or Noh plays. And we acknowledge that education is not about any of this. Compulsory schooling, the institutionalisation of teaching and learning, pedagogical universal principles, and the globalisation of the job market began detaching education from upbringing a long time ago. Today, an avionics engineer, a scholar of Mexican literature, a developmental psychologist, or a macro-economist is increasingly evaluated on global, international standards.";E-ducation and the Languages of Information;Floridi, L.;2013;Floridi, L.;Ethics and Philosophy of Information
One of the most obvious features that characterises any technology is its in-betweeness. Suppose you live in Rio de Janeiro, not in Oxford. A hat is a technology between you and the sunshine. A pair of sandals is a technology between you and the beach on which you are walking. And a pair of sunglasses is between you and the bright light that surrounds you. The point may be phrased slightly differently, in terms of what exactly a specific technology relates. Perhaps a pair of sandals relates not you, but just your feet, and not to the beach, but just to some of its sandy surface. Yet this is hair-splitting and, in its essence, the idea of such an in-betweeness seems clear and uncontroversial. However, it soon gets complicated.;Technology’s In-Betweeness;Floridi, L.;2013;Floridi, L.;Ethics and Philosophy of Information
There are many ways of understanding the nature of philosophical questions. One may consider their morphology, semantics, relevance, or scope. This article introduces a different approach, based on the kind of informational resources required to answer them. The result is a definition of philosophical questions as questions whose answers are in principle open to informed, rational, and honest disagreement, ultimate but not absolute, closed under further questioning, possibly constrained by empirical and logico‐mathematical resources, but requiring noetic resources to be answered. The article concludes with a discussion of some of the consequences of this definition for a conception of philosophy as the study (or “science”) of open questions, which uses conceptual design to analyse and answer them.;What is A Philosophical Question?;Floridi, L.;2013;Floridi, L.;Ethics and Philosophy of Information
The most developed post-industrial societies live by information, and information and communication technologies keep them oxygenated (English 2009). So, the better the quality of the information exchanged, the more likely such societies and their members may prosper. But what is information quality (IQ) exactly? The question has become increasingly pressing in recent years. Yet, our answers have been less than satisfactory so far. In the USA, the Information Quality Act, also known as the Data Quality Act, enacted in 2000, left undefined virtually every key concept in the text. So, it required the Office of Management and Budget “to promulgate guidance to agencies ensuring the quality, objectivity, utility, and integrity of information (including statistical information) disseminated by Federal agencies”. Unsurprisingly, the guidelines have received much criticism and have been under review ever since.;Information Quality;Floridi, L.;2013;Floridi, L.;Ethics and Philosophy of Information
Whenever a visualization researcher is asked about the purpose of visualization, the phrase “gaining insight” by and large pops out instinctively. However, it is not absolutely factual that all uses of visualization are for gaining a deep understanding, unless the term insight is broadened to encompass all types of thought. Even when insight is the focus of a visualization task, it is rather difficult to know what insight is gained, how much, or how accurate. In this paper, we propose that “saving time” in accomplishing a user’s task is the most fundamental objective. By giving emphasis to “saving time”, we can establish a concrete metric, alleviate unnecessary contention caused by different interpretations of insight, and stimulate new research efforts in some aspects of visualization, such as empirical studies, design optimization and theories of visualization.;What Is Visualization Really For?;Chen, M. & Floridi, L.;2014;Floridi, L.;Ethics and Philosophy of Information
The phenomenon of distributed knowledge is well-known in epistemic logic. In this paper, a similar phenomenon in ethics, somewhat neglected so far, is investigated, namely distributed morality. The article explains the nature of distributed morality, as a feature of moral agency, and explores the implications of its occurrence in advanced information societies. In the course of the analysis, the concept of infraethics is introduced, in order to refer to the ensemble of moral enablers, which, although morally neutral per se, can significantly facilitate or hinder both positive and negative moral behaviours.;Distributed Morality in an Information Society;Floridi, L.;2013;Floridi, L.;Ethics and Philosophy of Information
This chapter addresses two questions. First, if knowledge is accounted information, how are we supposed (to apply this analysis in order) to understand perceptual knowledge and knowledge by testimony? In the first part of the chapter, I articulate an answer in terms of a re-interpretation of perception and testimony as data providers rather than full-blown cases of knowledge. Second, if perception and testimony are correctly understood as data providers, how are we supposed (to apply this analysis in order) to understand the semantic value of the data provided by such processes? In the second part of the chapter, I argue in favour of a constructionist hypothesis about how data may become meaningful for human cognitive agents through a process of repurposing of natural data/signals. The conclusion of the chapter is that human agents are natural-born data hackers.;Perception and Testimony as Data Providers;Floridi, L.;2013;Floridi, L.;Ethics and Philosophy of Information
"It is estimated that humanity accumulated 180 EB of data between the invention of writing and 2006. Between 2006 and 2011, the total grew ten times and reached 1,600 EB. This figure is now expected to grow fourfold approximately every 3 years. Every day, enough new data are being generated to fill all US libraries eight times over. As a result, there is much talk about “big data”. This special issue on “Evolution, Genetic Engineering and Human Enhancement”, for example, would have been inconceivable in an age of “small data”, simply because genetics is one of the data-greediest sciences around. This is why, in the USA, the National Institutes of Health (NIH) and the National Science Foundation (NSF) have identified big data as a programme focus. One of the main NSF–NIH interagency initiatives addresses the need for core techniques and technologies for advancing big data science and engineering (see NSF-12-499). Despite the importance of the phenomenon, it is unclear what exactly the term “big data” means and hence refers to. The aforementioned document specifies that: “The phrase ‘big data’ in this solicitation refers to large, diverse, complex, longitudinal, and/or distributed data sets generated from instruments, sensors, Internet transactions, email, video, click streams, and/or all other digital sources available today and in the future.” You do not need to be an analytic philosopher to find this both obscure and vague. Wikipedia, for once, is also unhelpful. Not because the relevant entry is unreliable, but because it reports the common definition, which is unsatisfactory: “data sets so large and complex that they become awkward to work with using on-hand database management tools”. Apart from the circular problem of defining “big” with “large”, the definition suggests that data are too big or large only in relation to our current computational power. This is misleading. Of course, “big”, as many other terms, is a relational predicate: a pair of shoes is too big for you, but fine for me. It is also trivial to acknowledge that we tend to evaluate things non-relationally, in this case as absolutely big, whenever the frame of reference is obvious enough to be left implicit. A horse is a big animal, no matter what whales may think. Yet, these two simple points may give the impression that there is no real trouble with “big data” being a loosely defined term referring to the fact that our current computers cannot handle so many gazillions of data efficiently. And this is where two confusions seem to creep in. First, that the epistemological problem with big data is that there is too much of them (the ethical problem concerns how we use them; see below). And second, that the technological solution to the epistemological problem is more and better techniques and technologies, which will “shrink” big data back to a manageable size. The epistemological problem is different, and it requires an equally epistemological solution.";Big Data and Their Epistemological Challenge;Floridi, L.;2012;Floridi, L.;Ethics and Philosophy of Information
Some time ago, I met a very bright and lively graduate, who registered with Facebook during the academic year 2003–2004, when she was a student at Harvard. Her Facebook ID number was 246. Impressive. A bit like being the 246th person to land on a new continent. Such Facebook ID numbers no longer exist. In a few years, that continent has become rather crowded, as she has been joined by several hundreds of million users worldwide. Half a billion was reached in July 2010. It is a good reminder of how more and more people spend an increasing amount of time ‘onlife’, interacting with and within an infosphere that is neither entirely virtual nor only physical. It is also a good reminder of how influential information and communication technologies are becoming in shaping our personal identities, as technologies of the self. In the philosophy of mind, there is a well-honed distinction between personal identity and self-conception or more simply between who we are (call it our ontological self) and who we think we are (call it our epistemological self). Like many other handy distinctions, this too seems to work at its best once you drop it. Like a Wittgensteinian ladder, it helps you to reach a better perspective, as long as you do not get stuck on it. Of course, there is a difference between being and believing to be. However, it is equally obvious that, in healthy individuals, the ontological and the epistemological selves flourish only if they support each other in a symbiotic relationship. Not only our self-conceptions should be close to who we really are. Our ontological selves are also sufficiently malleable to be significantly influenced by who we think we are or would like to be. And such epistemological selves in turn are sufficiently ductile to be shaped by who we are told to be.;Technologies of the Self;Floridi, L.;2012;Floridi, L.;Ethics and Philosophy of Information
In this article, I outline the three main philosophical lessons that we may learn from Turing's work, and how they lead to a new philosophy of information. After a brief introduction, I discuss his work on the method of levels of abstraction (LoA), and his insistence that questions could be meaningfully asked only by specifying the correct LoA. I then look at his second lesson, about the sort of philosophical questions that seem to be most pressing today. Finally, I focus on the third lesson, concerning the new philosophical anthropology that owes so much to Turing's work. I then show how the lessons are learned by the philosophy of information. In the conclusion, I draw a general synthesis of the points made, in view of the development of the philosophy of information itself as a continuation of Turing's work.;Turing's three philosophical lessons and the philosophy of information;Floridi, L.;2012;Floridi, L.;Ethics and Philosophy of Information
Science, especially physics, has taught us to be very cautious about our naïve certainties (“that's the way it is!”), everyday intuitions (“it must be that way!”), and commonsensical rejections (“that's impossible!”). While reading this issue of Philosophy & Technology, just recall that we are all travelling at about 100,000 km/h around the sun. Indeed, we are getting so used to contemporary science supporting extraordinary claims that abrasively clash with what we would consider plausible, that we might overreact, and be inclined to believe almost anything. If tomorrow some credible source tells us that unicorns have been biologically engineered in some lab, how many of us would be utterly incredulous? So when scientists come up with some incredible results, what should we believe? The problem is exacerbated by the fact that these days, experiments churn gazillions of data. The Large Hadron Collider, currently the largest highest-energy particle accelerator, pumps out approximately 15 petabytes of data per year, which require a dedicated computational grid to be refined, analysed, and put to proper use. The more data and analysis we need, the more likely it is that something might go wrong in the process. Quality standards and safety measures are serious issues in the knowledge industry too. The problem is widely felt, not just by insecure philosophers in search of reassurance. This is why scientists require very high levels of probability when it comes to deciding whether we are witnessing an amazing discovery or just some weird glitch in our systems. Have some neutrinos really travelled faster than light (http://www.bbc.co.uk/news/science-environment-15017484)? Enter statistics.;Degenerate Epistemology;Floridi, L.;2012;Floridi, L.;Ethics and Philosophy of Information
The article addresses the problem of how semantic information can be upgraded to knowledge. The introductory section explains the technical terminology and the relevant background. Section 2 argues that, for semantic information to be upgraded to knowledge, it is necessary and sufficient to be embedded in a network of questions and answers that correctly accounts for it. Section 3 shows that an information flow network of type A fulfils such a requirement, by warranting that the erotetic deficit, characterising the target semantic information t by default, is correctly satisfied by the information flow of correct answers provided by an informational source s. Section 4 illustrates some of the major advantages of such a Network Theory of Account (NTA) and clears the ground of a few potential difficulties. Section 5 clarifies why NTA and an informational analysis of knowledge, according to which knowledge is accounted semantic information, is not subject to Gettier-type counterexamples. A concluding section briefly summarises the results obtained.;Semantic information and the network theory of account;Floridi, L.;2012;Floridi, L.;Ethics and Philosophy of Information
In written and spoken communications, figures of speech (e.g., metaphors and synecdoche) are often used as an aid to help convey abstract or less tangible concepts. However, the benefits of using rhetorical illustrations or embellishments in visualization have so far been inconclusive. In this work, we report an empirical study to evaluate hypotheses that visual embellishments may aid memorization, visual search and concept comprehension. One major departure from related experiments in the literature is that we make use of a dual-task methodology in our experiment. This design offers an abstraction of typical situations where viewers do not have their full attention focused on visualization (e.g., in meetings and lectures). The secondary task introduces “divided attention”, and makes the effects of visual embellishments more observable. In addition, it also serves as additional masking in memory-based trials. The results of this study show that visual embellishments can help participants better remember the information depicted in visualization. On the other hand, visual embellishments can have a negative impact on the speed of visual search. The results show a complex pattern as to the benefits of visual embellishments in helping participants grasp key concepts from visualization.;An Empirical Study on Using Visual Embellishments in Visualization;Borgo, R., Abdul-Rahman, A. & Mohamed, F. et al.;2012;Floridi, L.;Ethics and Philosophy of Information
"As the European Group on Ethics in Science and New Technologies and the UNESCO Observatory on the Information Society have documented, the exponential developments of internet services and resources have brought enormous benefits and opportunities to an increasing number of people. But it has also greatly outpaced our understanding of their conceptual nature and ethical implications, while raising unprecedented moral challenges, whose complexity and global dimensions are rapidly expanding, evolving and becoming increasingly serious. Examples come readily to mind. Consider the ethical issues arising from “the triple A”, namely the availability, accessibility and accuracy of informational resources, independently of their format, kind and physical support; the corresponding questions concerning standards of reliability and trustworthiness of information sources; or e-inclusionand the so-called digital divide. The more we become accustomed to living and working immersed within an informational environment, the easier it becomes to unveil the new moral difficulties that we encounter. Thus, in recent years, information societies have been forced to deal with urgent, ethical problems concerning information privacy and confidentiality, hacking (understood as the unauthorised access to a computerised information system), digital vandalism (e.g. the creation and intentional dissemination of software viruses), security, monitoring and control (including issues related to digital warfare, terrorism and a dystopian “surveillance society”), freedom of expression, censorship, filtering and contents control. Likewise, the debate about information ownership and intellectual property(including copyright and patents legislation), fair use, piracy, and the development and support of open source software affects both users and producers ethically, while morally shaping their informational environment. Inevitably, similar issues constitute a complex and potentially confusing scenario, not least because it is in constant and rapid evolution. The lack of balance is obvious and a matter of daily experience in the life of millions of citizens.";The New Ethical Responsibilities of Internet Service Providers;Floridi, L.;2011;Floridi, L.;Ethics and Philosophy of Information
"It is a well-known fact that artificial intelligence (AI) research seeks both to reproduce the outcome of our intelligent behaviour by non-biological means, and to produce the non-biological equivalent of our intelligence. As a branch of engineering interested in intelligent behaviour reproduction, AI has been astoundingly successful. We increasingly rely on AI-related applications (smart technologies) to perform tasks that would be simply impossible by un-aided or un-augmented human intelligence. But as a branch of cognitive science interested in intelligence production, AI has been a dismal disappointment. Productive AI does not merely underperform with respect to human intelligence; it has not joined the competition yet. The fact that Watson—IBM’s system capable of answering questions asked in natural language—recently won against its human opponents when playing Jeopardy! only shows that artefacts can be smart without being intelligent. The two souls of AI, the engineering and the cognitive one, have often engaged in fratricidal feuds for intellectual predominance, academic power, and financial resources. That is partly because they both claim common ancestors and a single intellectual inheritance: a founding event, the Dartmouth Summer Research Conference on Artificial Intelligence in 1956, and a founding father, Turing, with his machine and its computational limits, and then his famous test. It hardly helps that a simulation might be used in order to check both whether the simulated source has been produced, and whether the targeted source’s behaviour or performance has been reproduced or even surpassed. The misalignment of their goals and results has caused endless and mostly pointless diatribes. Defenders of AI point to the strong results of reproductive, engineering AI, whereas detractors of AI point to the weak results of productive, cognitive AI. Many of the current speculations on the so-called singularity issue have their roots in such confusion. In order to escape the dichotomy just outlined, one needs to realise that AI cannot be reduced to a “science of nature”, or to a “science of culture”, because it is a “science of the artificial”, to put it with Herbert Simon (Simon 1996). As such, AI pursues neither a descriptive nor a prescriptive approach to the world: it investigates the constraining conditions that make possible to build and embed artefacts in the world and interact with it successfully. In other words, it inscribes the world, for such artefacts are new logico-mathematical pieces of code, that is, new texts, written in Galileo’s mathematical book of nature. Until recently, the widespread impression was that such process of adding to the mathematical book of nature (inscription) required the feasibility of productive, cognitive AI. After all, developing even a rudimentary form of non-biological intelligence may seem to be not only the best but perhaps the only way to implement technologies sufficiently adaptive and flexible to deal effectively with a complex, ever-changing and often unpredictable, when not unfriendly, environment. Such impression is not incorrect, but it is distracting because, while we were unsuccessfully pursuing the inscription of productive AI into the world, we were actually re-ontologising the world to fit reproductive, engineering AI. The world is becoming an infosphere increasingly well adapted to AI-bounded capacities. In robotics, an envelope is the three-dimensional space that defines the boundaries that a robot can reach. We have been enveloping the world for decades without fully realising it.";Children of the Fourth Revolution;Floridi, L.;2011;Floridi, L.;Ethics and Philosophy of Information
Technologies lower constraints and expand affordances. By doing so, they continuously redesign the feasibility space of the agents who enjoy them. The more empowering or enabling technologies become, the more likely they are to change the nature and scope of the risks that they may bring, both in terms of undesirable outcomes (possible damages or losses) and in terms of missed desirable outcomes (potential benefits or opportunities). As a consequence, technologies, by their very nature, tend to redesign the corresponding space of risks in which agents operate and interact. Like a string of paper dolls, it seems that technologies cannot shape actual constraints and affordances without also shaping the corresponding risks, both positive and negative. A risk-free technology is therefore an oxymoron, as recent disasters and crises affecting the energy industry have painfully reminded us. Nevertheless, the intrinsically risky nature of technologies should not be reason for despair, for technologies can also reduce the space of risks and make it more manageable, and this is ground for some cautious optimism. Let me explain.;Energy, Risks, and Metatechnology;Floridi, L.;2011;Floridi, L.;Ethics and Philosophy of Information
This article offers an account and defence of constructionism, both as a metaphilosophical approach and as a philosophical methodology, with references to the so‐called maker's knowledge tradition. Its main thesis is that Plato's “user's knowledge” tradition should be complemented, if not replaced, by a constructionist approach to philosophical problems in general and to knowledge in particular. Epistemic agents know something when they are able to build (reproduce, simulate, model, construct, etc.) that something and plug the obtained information into the correct network of relations that account for it. Their epistemic expertise increases with the scope and depth of the questions that they are able to ask and answer. Thus, constructionism deprioritises mimetic, passive, and declarative knowledge that something is the case, in favour of poietic, interactive, and practical knowledge of something being the case. Metaphilosophically, constructionism suggests adding conceptual engineering to conceptual analysis as a fundamental method.;A DEFENCE OF CONSTRUCTIONISM: PHILOSOPHY AS CONCEPTUAL ENGINEERING;Floridi, L.;2011;Floridi, L.;Ethics and Philosophy of Information
"An interesting way of looking at the history of cultures is in terms of the increasing distance of human life from the natural course of events, thanks to an ever-thickening layer of technological mediations. A culture (not necessarily a good culture, let alone a civilization) emerges when a society is able to detach itself from the physical world (physis), and generate sufficient resources to express itself with some stability. From the division of labour to sheer oppression, from the invention of tools to the creation of weapons, there must be at least a fissure between surviving and living, where the seeds of a culture can take root non-ephemerally. A culture therefore can be pre-historical (no recordings) but hardly pre-technological; “hardly” because, exceptionally, such breaking away from physis may be achievable by barehanded individuals in unaided contexts. In theory, nothing prevents extraordinary people from planting some cultural seeds even when life is flattened into survival two-dimensionally, here and now. In practice, however, cultures tend to emerge and flourish only behind the dam provided by some techne. Even embittered stylites need pillars on which to stand, and peasants to bring food. Once cultures are sufficiently advanced to be able to reflect critically on their technological conditions of possibility, they seem to encounter two traps.";Harmonising Physis and Techne: The Mediating Role of Philosophy;Floridi, L.;2011;Floridi, L.;Ethics and Philosophy of Information
"Semantic information is usually supposed to satisfy the veridicality thesis: p qualifies as semantic information only if p is true. However, what it means for semantic information to be true is often left implicit, with correspondentist interpretations representing the most popular, default option. The article develops an alternative approach, namely a correctness theory of truth (CTT) for semantic information. This is meant as a contribution not only to the philosophy of information but also to the philosophical debate on the nature of truth. After the introduction, in Sect. 2, semantic information is shown to be translatable into propositional semantic information (i). In Sect. 3, i is polarised into a query (Q) and a result (R), qualified by a specific context, a level of abstraction and a purpose. This polarization is normalised in Sect. 4, where [Q + R] is transformed into a Boolean question and its relative yes/no answer [Q + A]. This completes the reduction of the truth of i to the correctness of A. In Sects. 5 and 6, it is argued that (1) A is the correct answer to Q if and only if (2) A correctly saturates Q by verifying and validating it (in the computer science’s sense of “verification” and “validation”); that (2) is the case if and only if (3) [Q + A] generates an adequate model (m) of the relevant system (s) identified by Q; that (3) is the case if and only if (4) m is a proxy of s (in the computer science’s sense of “proxy”) and (5) proximal access to m commutes with the distal access to s (in the category theory’s sense of “commutation”); and that (5) is the case if and only if (6) reading/writing (accessing, in the computer science’s technical sense of the term) menables one to read/write (access) s. Sect. 7 provides some further clarifications about CTT, in the light of semantic paradoxes. Section 8 draws a general conclusion about the nature of CTT as a theory for systems designers not just systems users. In the course of the article all technical expressions from computer science are explained.";Semantic Information and the Correctness Theory of Truth;Floridi, L.;2011;Floridi, L.;Ethics and Philosophy of Information
"Trust is generally understood as a relationship in which an agent (the trustor) decides to depend on another agent’s (the trustee) foreseeable behaviour in order to fulfil his expectations. It is a fundamental aspect of social interactions, as it has economical, social, psychological and ethical implications, and as such it is a crucial topic in several research areas (Gambetta 1998; Taddeo 2009). In the past decade, research interest in trust-related concepts and phenomena has escalated following the advent of the information revolution (Floridi 2008). The use of Information and Communication Technologies, of Computer Mediated Communications (CMCs), and the development of artificial agents—such as SatNav systems, drones, and robotic companion—have provided unprecedented opportunities for social interactions in informational environments, involving human as well as artificial and hybrid agents (Ess 2010). In such scenario, one of the most problematic issues is represented by the emergence of e-trust, that is, trust specifically developed in digital contexts and/or involving artificial agents. Like trust, e-trust too has a variety of heterogeneous implications, ranging from the effects on social interactions in digital environment to the behaviour of the involved agents, whether human or artificial (Taddeo 2009; Ess 2010). When e-trust is considered from a philosophical perspective, four problems seem to be more salient: (i) the identification of the fundamental and distinctive aspects of e-trust; (ii) the relation between trust and e-trust, that is, whether e-trust should be considered as an occurrence of trust on-line or as an independent phenomenon in itself; (iii) whether the environment of occurrence has any influence on the emergence of e-trust; and, finally, (iv) the extent to which artificial agents can be involved in an e-trust relationship.";The case for e-trust;Taddeo, M. & Floridi, L.;2011;Floridi, L.;Ethics and Philosophy of Information
Information and communication technologies (ICTs) are building a new habitat (infosphere) in which future generations, living in advanced information societies, will spend an increasing amount of time. In the infosphere, it is progressively more difficult to understand what life was like in pre-digital times and, in the near future, the very distinction between online and offline will become blurred and then disappear. The phenomenon is variously known as “Ubiquitous Computing”, “Ambient Intelligence”, “The Internet of Things” or “Web-augmented things”. GPs are a good example of this convergence: asking whether you are online when driving a car while following some GP’s instructions updated in real-time is becoming progressively less meaningful. We already live mostly onlife.;The Construction of Personal Identities Online;Floridi, L.;2011;Floridi, L.;Ethics and Philosophy of Information
In this paper, I present an informational approach to the nature of personal identity. In “Plato and the problem of the chariot”, I use Plato’s famous metaphor of the chariot to introduce a specific problem regarding the nature of the self as an informational multiagent system: what keeps the self together as a whole and coherent unity? In “Egology and its two branches” and “Egology as synchronic individualisation”, I outline two branches of the theory of the self: one concerning the individualisation of the self as an entity, the other concerning the identification of such entity. I argue that both presuppose an informational approach, defend the view that the individualisation of the self is logically prior to its identification, and suggest that such individualisation can be provided in informational terms. Hence, in “A reconciling hypothesis: the three membranes model”, I offer an informational individualisation of the self, based on a tripartite model, which can help to solve the problem of the chariot. Once this model of the self is outlined, in “ICTs as technologies of the self” I use it to show how ICTs may be interpreted as technologies of the self. In “The logic of realisation”, I introduce the concept of “realization” (Aristotle’s anagnorisis) and support the rather Spinozian view according to which, from the perspective of informational structural realism, selves are the final stage in the development of informational structures. The final “Conclusion: from the egology to the ecology of the self” briefly concludes the article with a reference to the purposeful shaping of the self, in a shift from egology to ecology.;The Informational Nature of Personal Identity;Floridi, L.;2011;Floridi, L.;Ethics and Philosophy of Information
"The article investigates the sceptical challenge from an information-theoretic perspective. Its main goal is to articulate and defend the view that either informational scepticism is radical, but then it is epistemologically innocuous because redundant; or it is moderate, but then epistemologically beneficial because useful. In order to pursue this cooptation strategy, the article is divided into seven sections. Section 1 sets up the problem. Section 2 introduces Borel numbers as a convenient way to refer uniformly to (the data that individuate) different possible worlds. Section 3 adopts the Hamming distance between Borel numbers as a metric to calculate the distance between possible worlds. In Sects. 4 and 5, radical and moderate informational scepticism are analysed using Borel numbers and Hamming distances, and shown to be either harmless (extreme form) or actually fruitful (moderate form). Section 6 further clarifies the approach by replying to some potential objections. In the conclusion, the Peircean nature of the overall approach is briefly discussed.";Information, possible worlds and the cooptation of scepticism;Floridi, L.;2010;Floridi, L.;Ethics and Philosophy of Information
The article contains the replies to the collection of contributions discussing my research on the philosophy of information.;The Philosophy of Information as a Conceptual Framework;Floridi, L.;2010;Floridi, L.;Ethics and Philosophy of Information
This article provides replies to, and comments on, the contributions to the special issue on the philosophy of information. It seeks to highlight con‐vergences and points of potential agreement, while offering clarifications and further details. It also answers some criticisms and replies to some objections articulated in the special issue.;THE PHILOSOPHY OF INFORMATION: TEN YEARS LATER;Floridi, L.;2010;Floridi, L.;Ethics and Philosophy of Information
"Information: A Very Short Introduction explores the concept of information, central to modern science and society, from thermodynamics and DNA to our use of the mobile phone and the Internet. It moves from a brief look at the mathematical roots of information — its definition and measurement in ‘bits’ — to its role in genetics, and its social meaning and value, before considering the ethics of information, including issues of ownership, privacy, and accessibility; copyright and open source. This VSI also considers concepts such as ‘Infoglut’ (too much information to process) and the emergence of an information society.";Information: A Very Short Introduction;Floridi, L.;2010;Floridi, L.;Ethics and Philosophy of Information
Information and its cognate concepts are frequently used in increasingly varied areas of scientific and scholarly investigations, from computing and engineering to phi- losophy and the social sciences. As a consequence, a great deal of interesting and exciting research is taking place in a wide range of fields, which do not always com- municate with each other. So the second workshop1 of the IEG (the interdepartmental research group in philosophy of information at the University of Oxford2), took the shape of a series of seminars, hosted in Oxford by the Department of Engineering Science, during the academic year 2008–2009. The project aimed to bring together leading experts in the broadly construed area of information research, with the goal of offering a stimulating series of talks and discussions on “the nature and scope of information”. The series explored the links and commonalities between various disciplines with an interest in the nature of information. The focus was on conceptual and theoretical approaches as well as practical implications, with an emphasis on future direc- tions and programmes of research about information and the open questions currently facing us. ;Introduction to the special issue on the nature and scope of information;Black, E., Floridi, L. & Third, A.;2010;Floridi, L.;Ethics and Philosophy of Information
An important lesson that philosophy can learn from the Turing test and computer science more generally concerns the careful use of the method of levels of abstraction (LoAs). The purpose of this paper is to summarize the method and apply it to the paper, modelling and analysis of phenomenological and conceptual systems showing its principal features and main advantages. The constituents of the method are “observables”, collected together and moderated by predicates restraining their “behaviour”. The resulting collection of sets of observables is called a “gradient of abstractions” (GoAs) and it formalises the minimum consistency conditions that the chosen abstractions must satisfy. Two useful kinds of GoA – disjoint and nested – are identified. It is then argued that in any discrete (as distinct from analogue) domain of discourse, a complex phenomenon may be explicated in terms of simple approximations organised together in a GoAs. Thus, the method replaces, for discrete disciplines, the differential and integral calculus, which form the basis for understanding the complex analogue phenomena of science and engineering. The result formalises an approach that is rather common in computer science but has hitherto found little application in philosophy. So the philosophical value of the method is demonstrated by showing how making the LoA of discourse explicit can be fruitful for phenomenological and conceptual analysis. To this end, the method is applied to the Turing test, the concept of agenthood, the definition of emergence, the notion of artificial life, quantum observation and decidable observation. This paper applies the method of abstraction to the paper, modelling and analysis of phenomenological and conceptual systems showing its principal features and main advantages. It is hoped that this treatment will promote the use of the method in certain areas of the humanities and especially in philosophy.;Levels of abstraction and the Turing test;Floridi, L.;2010;Floridi, L.;Ethics and Philosophy of Information
This article brings together two research fields in applied ethics – namely, information ethics and business ethics– which deal with the ethical impact of information and communication technologies but that, so far, have remained largely independent. Its goal is to articulate and defend an informational approach to the conceptual foundation of business ethics, by using ideas and methods developed in information ethics, in view of the convergence of the two fields in an increasingly networked society.;Network Ethics: Information and Business Ethics in a Networked Society;Floridi, L.;2009;Floridi, L.;Ethics and Philosophy of Information
The paper investigates the ethics of information transparency (henceforth transparency). It argues that transparency is not an ethical principle in itself but a pro-ethical condition for enabling or impairing other ethical practices or principles. A new definition of transparency is offered in order to take into account the dynamics of information production and the differences between data and information. It is then argued that the proposed definition provides a better understanding of what sort of information should be disclosed and what sort of information should be used in order to implement and make effective the ethical practices and principles to which an organisation is committed. The concepts of “heterogeneous organisation” and “autonomous computational artefact” are further defined in order to clarify the ethical implications of the technology used in implementing information transparency. It is argued that explicit ethical designs, which describe how ethical principles are embedded into the practice of software design, would represent valuable information that could be disclosed by organisations in order to support their ethical standing.;The ethics of information transparency;Turilli, M. & Floridi, L.;2009;Floridi, L.;Ethics and Philosophy of Information
The paper develops some of the conclusions, reached in Floridi (2007), concerning the future developments of Information and Communication Technologies (ICTs) and their impact on our lives. The two main theses supported in that article were that, as the information society develops, the threshold between online and offline is becoming increasingly blurred, and that once there won't be any significant difference, we shall gradually re-conceptualise ourselves not as cyborgs but rather as inforgs, i.e. socially connected, informational organisms. In this paper, I look at the development of the so-called Semantic Web and Web 2.0 from this perspective and try to forecast their future. Regarding the Semantic Web, I argue that it is a clear and well-defined project, which, despite some authoritative views to the contrary, is not a promising reality and will probably fail in the same way AI has failed in the past. Regarding Web 2.0, I argue that, although it is a rather ill-defined project, which lacks a clear explanation of its nature and scope, it does have the potentiality of becoming a success (and indeed it is already, as part of the new phenomenon of Cloud Computing) because it leverages the only semantic engines available so far in nature, us. I conclude by suggesting what other changes might be expected in the future of our digital environment.;Web 2.0 vs. the Semantic Web: A Philosophical Assessment;Floridi, L.;2012;Floridi, L.;Ethics and Philosophy of Information
The article introduces the special issue dedicated to “The Philosophy of Information, Its Nature, and Future Developments.” It outlines the origins of the information society and then briefly discusses the definition of the philosophy of information, the possibility of reconciling nature and technology, the informational turn as a fourth revolution (after Copernicus, Darwin, and Freud), and the metaphysics of the infosphere.;The Information Society and Its Philosophy: Introduction to the Special Issue on “The Philosophy of Information, Its Nature, and Future Developments”;Floridi, L.;2009;Floridi, L.;Ethics and Philosophy of Information
The paper argues that the two best known formal logical fallacies, namely denying the antecedent (DA) and affirming the consequent (AC) are not just basic and simple errors, which prove human irrationality, but rather informational shortcuts, which may provide a quick and dirty way of extracting useful information from the environment. DA and AC are shown to be degraded versions of Bayes’ theorem, once this is stripped of some of its probabilities. The less the probabilities count, the closer these fallacies become to a reasoning that is not only informationally useful but also logically valid.;Logical fallacies as informational shortcuts;Floridi, L.;2009;Floridi, L.;Ethics and Philosophy of Information
The paper argues that digital ontology (the ultimate nature of reality is digital, and the universe is a computational system equivalent to a Turing Machine) should be carefully distinguished from informational ontology (the ultimate nature of reality is structural), in order to abandon the former and retain only the latter as a promising line of research. Digital vs. analogue is a Boolean dichotomy typical of our computational paradigm, but digital and analogue are only “modes of presentation” of Being (to paraphrase Kant), that is, ways in which reality is experienced or conceptualised by an epistemic agent at a given level of abstraction. A preferable alternative is provided by an informational approach to structural realism, according to which knowledge of the world is knowledge of its structures. The most reasonable ontological commitment turns out to be in favour of an interpretation of reality as the totality of structures dynamically interacting with each other. The paper is the first part (the pars destruens) of a two-part piece of research. The pars construens, entitled “A Defence of Informational Structural Realism”, is developed in a separate article, also published in this journal.;Against digital ontology;Floridi, L.;2009;Floridi, L.;Ethics and Philosophy of Information
An evaluation of the 2008 Loebner contest;Turing’s Imitation Game: Still an Impossible Challenge for All Machines and Some Judges––An Evaluation of the 2008 Loebner Contest;Floridi, L., Taddeo, M. & Turilli, M.;2009;Floridi, L.;Ethics and Philosophy of Information
"In this article I argue that the best way to understand the information turn is in terms of a fourth revolution in the long process of reassessing humanity's fundamental nature and role in the universe. We are not immobile, at the centre of the universe (Copernicus); we are not unnaturally distinct and different from the rest of the animal world (Darwin); and we are far from being entirely transparent to ourselves (Freud). We are now slowly accepting the idea that we might be informational organisms among many agents (Turing), inforgs not so dramatically different from clever, engineered artefacts, but sharing with them a global environment that is ultimately made of information, the infosphere.";ARTIFICIAL INTELLIGENCE'S NEW FRONTIER: ARTIFICIAL COMPANIONS AND THE FOURTH REVOLUTION;Floridi, L.;2008;Floridi, L.;Ethics and Philosophy of Information
"The use of “levels of abstraction” in philosophical analysis (levelism) has recently come under attack. In this paper, I argue that a refined version of epistemological levelism should be retained as a fundamental method, called the method of levels of abstraction. After a brief introduction, in section “Some Definitions and Preliminary Examples” the nature and applicability of the epistemological method of levels of abstraction is clarified. In section “A Classic Application of the Method of Abstraction”, the philosophical fruitfulness of the new method is shown by using Kant’s classic discussion of the “antinomies of pure reason” as an example. In section “The Philosophy of the Method of Abstraction”, the method is further specified and supported by distinguishing it from three other forms of “levelism”: (i) levels of organisation; (ii) levels of explanation and (iii) conceptual schemes. In that context, the problems of relativism and antirealism are also briefly addressed. The conclusion discusses some of the work that lies ahead, two potential limitations of the method and some results that have already been obtained by applying the method to some long-standing philosophical problems.";The Method of Levels of Abstraction;Floridi, L.;2008;Floridi, L.;Ethics and Philosophy of Information
"What is the ultimate nature of reality? This paper defends an answer in terms of informational realism (IR). It does so in three stages. First, it is shown that, within the debate about structural realism (SR), epistemic (ESR) and ontic (OSR) structural realism are reconcilable by using the methodology of the levels of abstractions. It follows that OSR is defensible from a structuralist-friendly position. Second, it is argued that OSR is also plausible, because not all related objects are logically prior to all relational structures. The relation of difference is more fundamental (because constitutive of) any relata. Third, it is suggested that an ontology of structural objects for OSR can reasonably be developed in terms of informational objects, and that Object Oriented Programming provides a flexible and powerful methodology with which to clarify and make precise the concept of ""informational object"". The outcome is informational realism, the view that the world is the totality of informational objects (structured constraining affordances) dynamically interacting with each other.";Information ethics: a reappraisal;Floridi, L.;2008;Floridi, L.;Ethics and Philosophy of Information
Agents require a constant flow, and a high level of processing, of relevant semantic information, in order to interact successfully among themselves and with the environment in which they are embedded. Standard theories of information, however, are silent on the nature of epistemic relevance. In this paper, a subjectivist interpretation of epistemic relevance is developed and defended. It is based on a counterfactual and metatheoretical analysis of the degree of relevance of some semantic information i to an informee/agent a, as a function of the accuracy of i understood as an answer to a query q, given the probability that q might be asked by a. This interpretation of epistemic relevance vindicates a strongly semantic theory of information, according to which semantic information encapsulates truth. It accounts satisfactorily for several important applications and interpretations of the concept of relevant information in a variety of philosophical areas. And it interfaces successfully with current philosophical interpretations of causal and logical relevance.;Understanding Epistemic Relevance;Floridi, L.;2008;Floridi, L.;Ethics and Philosophy of Information
"This is the revised version of an invited keynote lecture delivered at the 1st Australian Computing and Philosophy Conference (CAP@AU; the Australian National University in Canberra, 31 October–2 November, 2003). The paper is divided into two parts. The first part defends an informational approach to structural realism. It does so in three steps. First, it is shown that, within the debate about structural realism (SR), epistemic (ESR) and ontic (OSR) structural realism are reconcilable. It follows that a version of OSR is defensible from a structuralist-friendly position. Second, it is argued that a version of OSR is also plausible, because not all relata (structured entities) are logically prior to relations (structures). Third, it is shown that a version of OSR is also applicable to both sub-observable (unobservable and instrumentally-only observable) and observable entities, by developing its ontology of structural objects in terms of informational objects. The outcome is informational structural realism, a version of OSR supporting the ontological commitment to a view of the world as the totality of informational objects dynamically interacting with each other. The paper has been discussed by several colleagues and, in the second half, ten objections that have been moved to the proposal are answered in order to clarify it further.";A defence of informational structural realism;Floridi, L.;2008;Floridi, L.;Ethics and Philosophy of Information
This article is the second step in our research into the Symbol Grounding Problem (SGP). In a previous work, we defined the main condition that must be satisfied by any strategy in order to provide a valid solution to the SGP, namely the zero semantic commitment condition (Z condition). We then showed that all the main strategies proposed so far fail to satisfy the Z condition, although they provide several important lessons to be followed by any new proposal. Here, we develop a new solution of the SGP. It is called praxical in order to stress the key role played by the interactions between the agents and their environment. It is based on a new theory of meaning—Action-based Semantics (AbS)—and on a new kind of artificial agents, called two-machine artificial agents (AM²). Thanks to their architecture, AM2s implement AbS, and this allows them to ground their symbols semantically and to develop some fairly advanced semantic abilities, including the development of semantically grounded communication and the elaboration of representations, while still respecting the Z condition.;A Praxical Solution of the Symbol Grounding Problem;Floridi, L.;2007;Floridi, L.;Ethics and Philosophy of Information
This paper may be read as a sequel to a 1995 paper, published in this journal, in which I predicted what sort of transformations and problems were likely to affect the development of the Internet and our system of organized knowledge in the medium term. In this second attempt, I look at the future developments of information and communication technologies and try to estimate what their impact on our lives will be. The forecast is that, in information societies, the threshold between online and offline will soon disappear, and that once there is no difference, we shall become not cyborgs but rather inforgs, that is, connected informational organisms.;A Look into the Future Impact of ICT on Our Lives;Floridi, L.;2006;Floridi, L.;Ethics and Philosophy of Information
Information plays a major role in any moral action. ICT (Information and Communication Technologies) have revolutionized the life of information, from its production and management to its consumption, thus deeply affecting our moral lives. Amid the many issues they have raised, a very serious one, discussed in this paper, is labelled the tragedy of the Good Will. This is represented by the increasing pressure that ICT and their deluge of information are putting on any agent who would like to act morally, when informed about actual or potential evils, but who also lacks the resources to do much about them. In the paper, it is argued that the tragedy may be at least mitigated, if not solved, by seeking to re-establish some equilibrium, through ICT themselves, between what agents know about the world and what they can do to improve it.;Information technologies and the tragedy of the Good Will;Floridi, L.;2006;Floridi, L.;Ethics and Philosophy of Information
Philosophers have recently turned their attention tothe moral issues raised by ICTs (Information andCommunication Technologies) in the informationsociety. In less than a century, humanity has movedfrom a state of ‘‘submission’’ to nature, through astate of power of potential total destruction, to thepresent state in which we have the means and tools toengineer and manage entire new realities, to tailorthem to our needs and to invent the future. Largelyowing to ICTs, our technological power is alreadyenormous and is growing relentlessly. It is already sovast to have overcome the barrier between the naturaland the artiﬁcial. For the ﬁrst time in history,humanity can now create and control almost anyaspect of its environment. Therefore, our moralresponsibilities towards the world and future gener-ations are equally enormous. How can we cope withthese new ethical challenges?;Information ethics: Agents, artefacts and new cultural perspectives;Floridi, L. & Savulescu, J.;2006;Floridi, L.;Ethics and Philosophy of Information
"One of the open problems in the philosophy of information is whether there is an information logic (IL), different from epistemic (EL) and doxastic logic (DL), which formalises the relation ""a is informed--that p"" (Iap) satisfactorily. In this paper, the problem is--solved by arguing that the axiom schemata of the normal modal logic (NML) KTB (also known as B or Br or Brouwer's system) are well suited to formalise the relation of ""being informed"". After having shown that IL can be constructed as an informational reading--of KTB, four consequences of a KTB-based IL are explored: information overload; the veridicality thesis (Iap → p); the relation between IL and EL; and the Kp → Bp principle or entailment property, according to which knowledge implies belief. Although these issues are discussed later in the article, they are the motivations behind the development of IL.";The logic of being informed;Floridi, L.;2006;Floridi, L.;Ethics and Philosophy of Information
"In this article, I summarise the ontological theory of informational privacy (an approach based on information ethics) and then discuss four types of interesting challenges confronting any theory of informational privacy: (1) parochial ontologies and non-Western approaches to informational privacy; (2) individualism and the anthropology of informational privacy; (3) the scope and limits of informational privacy; and (4) public, passive and active informational privacy. I argue that the ontological theory of informational privacy can cope with such challenges fairly successfully. In the conclusion, I discuss some of the work that lies ahead.";Four challenges for a theory of informational privacy;Floridi, L.;2006;Floridi, L.;Ethics and Philosophy of Information
In recent years, «InformationEthics» (IE) has come to mean differentthings to different researchers working in avariety of disciplines. It is certainly unfortu-nate, for it has generated some confusionabout the specific natureand scopeof IE. Inthis article, we will defend an InformationEthics where the agent-related behaviourandthe patient-related statusof informationalobjects quainformational objects can bemorally significant, over and above the ins-trumental function that may be attributed tothem by other ethical approaches, and hencein which they can contribute to determining,normatively, ethical duties and legally enforceable rights.;Ética de la información: su naturaleza y alcance;Floridi, L.;2006;Floridi, L.;Ethics and Philosophy of Information
Luciano Floridi takes over our regular look at the web. Privacy is a Hydra of a problem. You cut off one head and two grow instead. The EU Directive on Data Protection has made informational privacy almost a fundamental human right, since it requires member states to protect the “fundamental rights and freedoms of natural persons, and in particular their right to privacy with respect to the processing of personal data.” (Article 1) Yet, recently, Big Brother has acquired a new head, courtesy of your search engine on the other side of the Atlantic, Google. Nowadays, everyone can google anyone and dig up piles of more or less insignificant information about them. The more powerful the search engine, the wider and deeper it travels the web, the more popular and useful it is bound to become. Many appreciate such dialectical crescendo as virtuoso. The snag is that any bit of data about you that sinks in the web, sooner or later may google up. True, we live in a glassy infosphere, whether we ignore it or even dislike it. This, however, is not the head of the Hydra we are hunting for here. Complaining about cookies would be equally wrong-headed. Cookies have been around for a long while, we are aware that some are innocuous or even useful, and we know how to deal with those that aren't. No, the new privacy problem regarding search engines, and hence the mighty Google in particular, is another. It concerns your profile as defined by your entire search history.;Word of Mouse;Floridi, L.;2006;Floridi, L.;Ethics and Philosophy of Information
The article provides an outline of the ontological interpretation of informational privacy based on information ethics (Floridi, forthcoming-b). It is part of a larger project of research, in which I have developed the foundations of ideas presented here (Floridi, forthcoming-c) and their consequences (Floridi, forthcoming-a).As an outline, it is meant to be self-sufficient and to provide enough information to enable the reader to assess how the approach fares with respect to other alternatives. However, those interested in a more detailed analysis, and especially in the reasons offered in its favour, may wish to consult the other articles as well.;Informational privacy and its ontological interpretation;Floridi, L.;2006;Floridi, L.;Ethics and Philosophy of Information
"""The world of the future will be an ever more demanding struggle against the limitations of our intelligence, not a comfortable hammock in which we can lie down to be waited upon by our robot slaves""---Wiener (1964), p. 69.";Information ethics, its nature and scope;Floridi, L.;2006;Floridi, L.;Ethics and Philosophy of Information
"The paper outlines a new interpretation of informational privacy and of its moral value. The main theses defended are: (a) informational privacy is a function of the ontological friction in the infosphere, that is, of the forces that oppose the information flow within the space of information; (b) digital ICTs (information and communication technologies) affect the ontological friction by changing the nature of the infosphere (re-ontologization); (c) digital ICTs can therefore both decrease and protect informational privacy but, most importantly, they can also alter its nature and hence our understanding and appreciation of it; (d) a change in our ontological perspective, brought about by digital ICTs, suggests considering each person as being constituted by his or her information and hence regarding a breach of one’s informational privacy as a form of aggression towards one’s personal identity.";The Ontological Interpretation of Informational Privacy;Floridi, L.;2005;Floridi, L.;Ethics and Philosophy of Information
This special issue of Ethics and Information Technology focuses on the ethics of new and emerging information technology (IT). The papers have been selected from submissions to the sixth international conference on Computer Ethics: Philosophical Enquiry (CEPE2005), which took place at the University of Twente, the Netherlands,July 17–19, 2005.Ethics of New Information Technology was the central theme of CEPE2005, as reﬂected in many of the 48 papers presented at the conference. The premise of CEPE2005 was that information technologies currently moving beyond the familiar mainframe,PC, laptop and networked computer paradigms, and that these new developments require ethical reﬂection. We are now witnessing the mobile and wireless revolution, the ubiquitous computing revolution, as well as revolutionary new uses of IT in biomedicine, education, the ﬁght against crime and terrorism, entertainment and other areas. We are anticipating a nanotechnology revolution, as well as a convergence between information technology, biotechnology and nanotechnology (‘‘converging technologies’’). These new developments require ethical reﬂection, even before their consequences become visible.;Editorial Introduction – Ethics of New Information Technology;Brey, P., Floridi, L. & Grodzinsky, F.;2005;Floridi, L.;Ethics and Philosophy of Information
There is no consensus yet on the definition of semantic information. This paper contributes to the current debate by criticising and revising the Standard Definition of semantic Information (SDI) as meaningful data, in favour of the Dretske‐Grice approach: meaningful and well‐formed data constitute semantic information only if they also qualify as contingently truthful. After a brief introduction, SDI is criticised for providing necessary but insufficient conditions for the definition of semantic information. SDI is incorrect because truth‐values do not supervene on semantic information, and misinformation (that is, false semantic information) is not a type of semantic information, but pseudo‐information, that is not semantic information at all. This is shown by arguing that none of the reasons for interpreting misinformation as a type of semantic information is convincing, whilst there are compelling reasons to treat it as pseudo‐information. As a consequence, SDI is revised to include a necessary truth‐condition. The last section summarises the main results of the paper and indicates some interesting areas of application of the revised definition.;Is Semantic Information Meaningful Data?;Floridi, L.;2007;Floridi, L.;Ethics and Philosophy of Information
The paper introduces a new model of telepresence. First, it criticizes the standard model of presence as epistemic failure, showing it to be inadequate. It then replaces it with a new model of presence as successful observation. It further provides reasons to distinguish between two types of presence, backward and forward. The new model is then tested against two ethical issues whose nature has been modified by the development of digital information and communication technologies, namely pornography and privacy, and shown to be effective.;The Philosophy of Presence: From Epistemic Failure to Successful Observation;Floridi, L.;2005;Floridi, L.;Ethics and Philosophy of Information
This article reviews eight proposed strategies for solving the symbol grounding problem (SGP), which was given its classic formulation in Harnad (1990). After a concise introduction, the paper provides an analysis of the requirement that must be satisfied by any hypothesis seeking to solve the SGP, the zero semantical commitment condition. It is then used to assess the eight strategies, which are organized into three main approaches: representationalism, semi-representationalism and non-representationalism. The conclusion is that all the strategies are semantically committed and hence that none of them provides a valid solution to the SGP, which remains an open problem.;Solving the symbol grounding problem: a critical review of fifteen years of research;Taddeo, M. & Floridi, L.;2005;Floridi, L.;Ethics and Philosophy of Information
This paper has three goals. The first is to introduce the “knowledge game”, a new, simple and yet powerful tool for analysing some intriguing philosophical questions. The second is to apply the knowledge game as an informative test to discriminate between conscious (human) and conscious-less agents (zombies and robots), depending on which version of the game they can win. And the third is to use a version of the knowledge game to provide an answer to Dretske’s question “how do you know you are not a zombie?”.;Consciousness, Agents and the Knowledge Game;Floridi, L.;2005;Floridi, L.;Ethics and Philosophy of Information
In this paper we introduce three methods to approach philosophical problems informationally: Minimalism, the Method of Abstraction and Constructionism. Minimalism considers the specifications of the starting problems and systems that are tractable for a philosophical analysis. The Method of Abstraction describes the process of making explicit the level of abstraction at which a system is observed and investigated. Constructionism provides a series of principles that the investigation of the problem must fulfil once it has been fully characterised by the previous two methods. For each method, we also provide an application: the problem of visual perception, functionalism, and the Turing Test, respectively.;How to Do Philosophy Informationally;Greco, G., Paronitti, G. & Turilli, M. et al.;2005;Floridi, L.;Ethics and Philosophy of Information
The philosophy of information (PI) is a new area of research with its own field of investigation and methodology. This article, based on the Herbert A. Simon Lecture of Computing and Philosophy I gave at Carnegie Mellon University in 2001, analyses the eighteen principal open problems in PI. Section 1 introduces the analysis by outlining Herbert Simon's approach to PI. Section 2 discusses some methodological considerations about what counts as a good philosophical problem. The discussion centers on Hilbert's famous analysis of the central problems in mathematics. The rest of the article is devoted to the eighteen problems. These are organized into five sections: problems in the analysis of the concept of information, in semantics, in the study of intelligence, in the relation between information and nature, and in the investigation of values.;Open Problems in the Philosophy of Information;Floridi, L.;2004;Floridi, L.;Ethics and Philosophy of Information
"Library information science (LIS) should develop its foundation interms of a philosophy of information (PI). This seems a rather harmlesssuggestion. Where else could information science look for its conceptualfoundations if not in PI? However, accepting this proposal means movingaway from one of the few solid alternatives currently available in the ﬁeld,namely, providing LIS with a foundation in terms of social epistemology (SE). This is no trivial move, so some reasonable reluctance is to be expect-ed. To overcome it, the proposal needs to be more than just acceptable; it must be convincing. In Floridi (2002a), I have articulated some of the rea-sons why I believe that PI can fulﬁll the foundationalist needs better thanSE can. I won’t rehearse them here. I ﬁnd them compelling, but I am ready to change my mind if counterarguments become available. Rather, in thiscontribution, I wish to clarify some aspects of my proposal (Floridi, 2002a)in favor of the interpretation of LIS as applied PI. I won’t try to show youthat I am right in suggesting that PI may provide a foundation for LIS bet-ter than SE. My more modest goal is to remove some ambiguities and pos-sible misunderstandings that might prevent the correct evaluation of my position, so that disagreement can become more constructive.";LIS as Applied Philosophy of Information: A Reappraisal;Floridi, L.;2004;Floridi, L.;Ethics and Philosophy of Information
In the paper it is argued that bridging the digital divide may cause a new ethical and social dilemma. Using Hardin's Tragedy of the Commons, we show that an improper opening and enlargement of the digital environment (Infosphere) is likely to produce a Tragedy of the Digital Commons (TDC). In the course of the analysis, we explain why Adar and Huberman's previous use of Hardin's Tragedy to interpret certain recent phenomena in the Infosphere (especially peer-to-peer communication) may not be entirely satisfactory. We then seek to provide an improved version of the TDC that avoids the possible shortcomings of their model. Next, we analyse some problems encountered by the application of classical ethics in the resolution of the TDC. In the conclusion, we outline the kind of work that will be required to develop an ethical approach that may bridge the digital divide but avoid the TDC.;The tragedy of the digital commons;Greco, G. & Floridi, L.;2004;Floridi, L.;Ethics and Philosophy of Information
The tripartite account of propositional, fallibilist knowledge that p as justified true belief can become adequate only if it can solve the Gettier Problem. However, the latter can be solved only if the problem of a successful coordination of the resources (at least truth and justification) necessary and sufficient to deliver propositional, fallibilist knowledge that p can be solved. In this paper, the coordination problem is proved to be insolvable by showing that it is equivalent to the ''coordinated attack'' problem, which is demonstrably insolvable in epistemic logic. It follows that the tripartite account is not merely inadequate as it stands, as proved by Gettier-type counterexamples, but demonstrably irreparable in principle, so that efforts to improve it can never succeed.;ON THE LOGICAL UNSOLVABILITY OF THE GETTIER PROBLEM;Floridi, L.;2004;Floridi, L.;Ethics and Philosophy of Information
This paper outlines a quantitative theory of strongly semantic information (TSSI) based on truth-values rather than probability distributions. The main hypothesis supported in the paper is that the classic quantitative theory of weakly semantic information (TWSI), based on probability distributions, assumes that truth-values supervene on factual semantic information, yet this principle is too weak and generates a well-known semantic paradox, whereas TSSI, according to which factual semantic information encapsulates truth, can avoid the paradox and is more in line with the standard conception of what generally counts as semantic information. After a brief introduction, section two outlines the semantic paradox implied by TWSI, analysing it in terms of an initial conflict between two requisites of a quantitative theory of semantic information. In section three, three criteria of semantic information equivalence are used to provide a taxonomy of quantitative approaches to semantic information and introduce TSSI. In section four, some further desiderata that should be fulfilled by a quantitative TSSI are explained. From section five to section seven, TSSI is developed on the basis of a calculus of truth-values and semantic discrepancy with respect to a given situation. In section eight, it is shown how TSSI succeeds in solving the paradox. Section nine summarises the main results of the paper and indicates some future developments.;Outline of a Theory of Strongly Semantic Information;Floridi, L.;2004;Floridi, L.;Ethics and Philosophy of Information
There is no consensus yet on the definition of semantic information. This paper contributes to the current debate by criticising and revising the Standard Definition of semantic Information (SDI) as meaningful data, in favour of the Dretske-Grice approach: meaningful and well-formed data constitute semantic information only if they also qualify as contingently truthful. After a brief introduction, SDI is criticised for providing necessary but insufficient conditions for the definition of semantic information. SDI is incorrect because truth-values do not supervene on semantic information, and misinformation (that is, false semantic information) is not a type of semantic information, but pseudo-information, that is not semantic information at all. This is shown by arguing that none of the reasons for interpreting misinformation as a type of semantic information is convincing, whilst there are compelling reasons to treat it as pseudo-information. As a consequence, SDI is revised to include a necessary truth-condition. The last section summarises the main results of the paper and indicates the important implications of the revised definition for the analysis of the deflationary theories of truth, the standard definition of knowledge and the classic, quantitative theory of semantic information.;From Data to Semantic Information;Floridi, L.;2003;Floridi, L.;Ethics and Philosophy of Information
Of our mundane and technical concepts, information is currently one of the most important, most widely used and least understood. So far, philosophers have done comparatively little work on information and its cognate concepts. The paradoxical situation may soon count as one more “scandal of philosophy”. However, the problems are fairly recent, dating back half a century or even less, and work is already in progress— witness this special issue of Minds and Machines and some important collections—so we might still be in time to do a good job. Philosophy, understood as conceptual exploration and analysis, needs to turn its attention to the new world of information. This is a quick and dirty way of introducing the field that, in other contexts, I’ve defined as the philosophy and information (PI). As an introduction, I believe it to be reasonably convincing. There is definitely a reassuring sense of déjà vu about it, and I have seen it becoming increasingly acceptable even among the more sceptical. But if this is the whole story, I must admit that I’m not entirely satisfied. Before explaining why, let me briefly elaborate on it. ;Two Approaches to the Philosophy of Information;Floridi, L.;2003;Floridi, L.;Ethics and Philosophy of Information
As a full expression of techne, the information society has already posed fundamental ethical problems, whose complexity and global dimensions are rapidly evolving. What is the best strategy to construct an information society that is ethically sound? This is the question I discuss in this paper. The task is to formulate an information ethics that can treat the world of data, information, knowledge and communication as a new environment, the infosphere. This information ethics must be able to address and solve the ethical challenges arising in the new environment on the basis of the fundamental principles of respect for information, its conservation and valorisation. It must be an ecological ethics for the information environment.;Information Ethics: An Environmental Approach to the Digital Divide;Floridi, L.;2002;Floridi, L.;Ethics and Philosophy of Information
This paper analyses the relations between philosophy of information (PI), library and information science (LIS) and social epistemology (SE). In the first section, it is argued that there is a natural relation between philosophy and LIS but that SE cannot provide a satisfactory foundation for LIS. SE should rather be seen as sharing with LIS a common ground, represented by the study of information, to be investigated by a new discipline, PI. In the second section, the nature of PI is outlined as the philosophical area that studies the conceptual nature of information, its dynamics and problems. In the third section, LIS is defined as a form of applied PI. The hypothesis supported is that PI should replace SE as the philosophical discipline that can best provide the conceptual foundation for LIS. In the conclusion, it is suggested that the 'identity' crisis undergone by LIS has been the natural outcome of a justified but precocious search for a philosophical counterpart that has emerged only recently: namely, PI. The development of LIS should not rely on some borrowed, pre-packaged theory. As applied PI, LIS can fruitfully contribute to the growth of basic theoretical research in PI itself and thus provide its own foundation.;On defining library and information science as applied philosophy of information;Floridi, L.;2002;Floridi, L.;Ethics and Philosophy of Information
"The paper provides a critical review of thedebate on the foundations of Computer Ethics(CE). Starting from a discussion of Moor'sclassic interpretation of the need for CEcaused by a policy and conceptual vacuum, fivepositions in the literature are identified anddiscussed: the ``no resolution approach'',according to which CE can have no foundation;the professional approach, according to whichCE is solely a professional ethics; the radicalapproach, according to which CE deals withabsolutely unique issues, in need of a uniqueapproach; the conservative approach, accordingto which CE is only a particular appliedethics, discussing new species of traditionalmoral issues; and the innovative approach,according to which theoretical CE can expandthe metaethical discourse with a substantiallynew perspective. In the course of the analysis,it is argued that, although CE issues are notuncontroversially unique, they are sufficientlynovel to render inadequate the adoption ofstandard macroethics, such as Utilitarianismand Deontologism, as the foundation of CE andhence to prompt the search for a robust ethicaltheory. Information Ethics (IE) is proposed forthat theory, as the satisfactory foundation forCE. IE is characterised as a biologicallyunbiased extension of environmental ethics,based on the concepts of information object/infosphere/entropy rather thanlife/ecosystem/pain. In light of the discussionprovided in this paper, it is suggested that CEis worthy of independent study because itrequires its own application-specific knowledgeand is capable of supporting a methodologicalfoundation, IE.";Mapping the foundationalist debate in computer ethics;Floridi, L. & Sanders, J.;2002;Floridi, L.;Ethics and Philosophy of Information
What is the most general common set ofattributes that characterises something asintrinsically valuableand hence as subject to some moral respect, andwithout which something would rightly beconsidered intrinsically worthless or even positivelyunworthy and therefore rightly to bedisrespected in itself? Thispaper develops and supports the thesis that theminimal condition of possibility of an entity'sleast intrinsic value is to be identified with itsontological status as an information object.All entities, even when interpreted as only clusters ofinformation, still have a minimal moral worthqua information objects and so may deserve to be respected. Thepaper is organised into four main sections.Section 1 models moral action as an information systemusing the object-oriented programmingmethodology (OOP). Section 2 addresses the question of whatrole the several components constituting themoral system can have in an ethical analysis. If theycan play only an instrumental role, thenComputer Ethics (CE) is probably bound to remain at most apractical, field-dependent, applied orprofessional ethics. However, Computer Ethics can give rise to amacroethical approach, namely InformationEthics (IE), if one can show that ethical concern should beextended to include not only human, animal orbiological entities, but also information objects. Thefollowing two sections show how this minimalistlevel of analysis can be achieved. Section 3 provides anaxiological analysis of information objects. Itcriticises the Kantian approach to the concept ofintrinsic value and shows that it can beimproved by using the methodology introduced in the first section.The solution of the Kantian problem prompts thereformulation of the key question concerningthe moral worth of an entity: what is theintrinsic value of x qua an object constituted by itsinherited attributes? In answering thisquestion, it is argued that entitiescan share different observable propertiesdepending on the level of abstraction adopted,and that it is still possible to speak of moral value even at thehighest level of ontological abstractionrepresented by the informational analysis. Section 4 develops aminimalist axiology based on the concept ofinformation object. It further supports IE's position byaddressing five objections that may undermineits acceptability.;On the intrinsic value of information objects and the infosphere;Floridi, L.;2002;Floridi, L.;Ethics and Philosophy of Information
"Computational and information‐theoretic research in philosophy has become increasingly fertile and pervasive, giving rise to a wealth of interesting results. In consequence, a new and vitally important field has emerged, the philosophy of information (PI). This essay is the first attempt to analyse the nature of PI systematically. PI is defined as the philosophical field concerned with the critical investigation of the conceptual nature and basic principles of information, including its dynamics, utilisation, and sciences, and the elaboration and application of information‐theoretic and computational methodologies to philosophical problems. I argue that PI is a mature discipline for three reasons: it represents an autonomous field of research; it provides an innovative approach to both traditional and new philosophical topics; and it can stand beside other branches of philosophy, offering a systematic treatment of the conceptual foundations of the world of information and the information society.";What is the Philosophy of Information?;Floridi, L.;2003;Floridi, L.;Ethics and Philosophy of Information
"Moral reasoning traditionally distinguishes two types of evil:moral (ME) and natural (NE). The standard view is that ME is theproduct of human agency and so includes phenomena such as war,torture and psychological cruelty; that NE is the product ofnonhuman agency, and so includes natural disasters such asearthquakes, floods, disease and famine; and finally, that morecomplex cases are appropriately analysed as a combination of MEand NE. Recently, as a result of developments in autonomousagents in cyberspace, a new class of interesting and importantexamples of hybrid evil has come to light. In this paper, it iscalled artificial evil (AE) and a case is made for considering itto complement ME and NE to produce a more adequate taxonomy. Byisolating the features that have led to the appearance of AE,cyberspace is characterised as a self-contained environment thatforms the essential component in any foundation of the emergingfield of Computer Ethics (CE). It is argued that this goes someway towards providing a methodological explanation of whycyberspace is central to so many of CE's concerns; and it isshown how notions of good and evil can be formulated incyberspace. Of considerable interest is how the propensity for anagent's action to be morally good or evil can be determined evenin the absence of biologically sentient participants and thusallows artificial agents not only to perpetrate evil (and forthat matter good) but conversely to `receive' or `suffer from'it. The thesis defended is that the notion of entropy structure,which encapsulates human value judgement concerning cyberspace ina formal mathematical definition, is sufficient to achieve thispurpose and, moreover, that the concept of AE can be determinedformally, by mathematical methods. A consequence of this approachis that the debate on whether CE should be considered unique, andhence developed as a Macroethics, may be viewed, constructively,in an alternative manner. The case is made that whilst CE issuesare not uncontroversially unique, they are sufficiently novel torender inadequate the approach of standard Macroethics such asUtilitarianism and Deontologism and hence to prompt the searchfor a robust ethical theory that can deal with them successfully.The name Information Ethics (IE) is proposed for that theory. Itis argued that the uniqueness of IE is justified by its beingnon-biologically biased and patient-oriented: IE is anEnvironmental Macroethics based on the concept of data entityrather than life. It follows that the novelty of CE issues suchas AE can be appreciated properly because IE provides a newperspective (though not vice versa). In light of the discussionprovided in this paper, it is concluded that Computer Ethics isworthy of independent study because it requires its ownapplication-specific knowledge and is capable of supporting amethodological foundation, Information Ethics.";Artificial evil and the foundation of computer ethics;Floridi, L. & Sanders, J.;2001;Floridi, L.;Ethics and Philosophy of Information
Mature information societies are characterised by mass production of data that provide insight into human behaviour. Analytics (as in big data analytics) has arisen as a practice to make sense of the data trails generated through interactions with networked devices, platforms and organisations. Persistent knowledge describing the behaviours and characteristics of people can be constructed over time, linking individuals into groups or classes of interest to the platform. Analytics allows for a new type of algorithmically assembled group to be formed that does not necessarily align with classes or attributes already protected by privacy and anti-discrimination law or addressed in fairness- and discrimination-aware analytics. Individuals are linked according to offline identifiers (e.g. age, ethnicity, geographical location) and shared behavioural identity tokens, allowing for predictions and decisions to be taken at a group rather than individual level. This article examines the ethical significance of such ad hocgroups in analytics and argues that the privacy interests of algorithmically assembled groups in inviolate personality must be recognised alongside individual privacy rights. Algorithmically grouped individuals have a collective interest in the creation of information about the group, and actions taken on its behalf. Group privacy is proposed as a third interest to balance alongside individual privacy and social, commercial and epistemic benefits when assessing the ethical acceptability of analytics platforms.;From Individual to Group Privacy in Big Data Analytics;Mittelstadt, B.;2017;Mittelstadt, B.;Ethics and Philosophy of Information
Personal Health Monitoring (PHM) uses electronic devices which monitor and record health-related data outside a hospital, usually within the home. This paper examines the ethical issues raised by PHM. Eight themes describing the ethical implications of PHM are identified through a review of 68 academic articles concerning PHM. The identified themes include privacy, autonomy, obtrusiveness and visibility, stigma and identity, medicalisation, social isolation, delivery of care, and safety and technological need. The issues around each of these are discussed. The system / lifeworld perspective of Habermas is applied to develop an understanding of the role of PHMs as mediators of communication between the institutional and the domestic environment. Furthermore, links are established between the ethical issues to demonstrate that the ethics of PHM involves a complex network of ethical interactions. The paper extends the discussion of the critical effect PHMs have on the patient's identity and concludes that a holistic understanding of the ethical issues surrounding PHMs will help both researchers and practitioners in developing effective PHM implementations.;The Ethical Implications of Personal Health Monitoring;Mittelstadt, B., Fairweather, B. & Shaw, M. et al.;2015;Mittelstadt, B.;Ethics and Philosophy of Information
The chapter undertakes a comparison of different approaches to the ethical assessment of novel technologies by looking at two recent research projects. ETICA was a FP7 sister project to PHM-Ethics, responsible for identification and ethical evaluation of information and communication technologies emerging in the next 10–15 years. The aims, methods, outcomes and recommendations of ETICA are compared to those of PHM-Ethics, with identification of linkages and similar findings. A relationship is identified between the two projects, in which the assessment methodologies developed in the projects are shown to operate at separate, but complementary levels. ETICA sought to reform EU ethics governance for emerging ICTs. The outcomes of PHM-Ethics are analyzed within the policy recommendations of ETICA, which demonstrate how the PHM-Ethics toolbox can contribute to ethics governance reform and context-sensitive ethical assessment of the sort called for by ETICA.;PHM-Ethics and ETICA: Complementary Approaches to Ethical Assessment;Mittelstadt, B., Stahl, B. & Fairweather, B.;2013;Mittelstadt, B.;Ethics and Philosophy of Information
"Recent work on interpretability in machine learning and AI has focused on the building of simplified models that approximate the true criteria used to make decisions. These models are a useful pedagogical device for teaching trained professionals how to predict what decisions will be made by the complex system, and most importantly how the system might break. However, when considering any such model it's important to remember Box's maxim that ""All models are wrong but some are useful."" We focus on the distinction between these models and explanations in philosophy and sociology. These models can be understood as a ""do it yourself kit"" for explanations, allowing a practitioner to directly answer ""what if questions"" or generate contrastive explanations without external assistance. Although a valuable ability, giving these models as explanations appears more difficult than necessary, and other forms of explanation may not have the same trade-offs. We contrast the different schools of thought on what makes an explanation, and suggest that machine learning might benefit from viewing the problem more broadly.";Explaining Explanations in AI;Mittelstadt, B., Russell, C. & Wachter, S.;2019;Mittelstadt, B.;"Ethics and Philosophy of Information; Digital Politics and Government; Information Governance and Security"
"In recent years a substantial literature has emerged concerning bias, discrimination, and fairness in AI and machine learning. Connecting this work to existing legal non-discrimination frameworks is essential to create tools and methods that are practically useful across divergent legal regimes. While much work has been undertaken from an American legal perspective, comparatively little has mapped the effects and requirements of EU law. This Article addresses this critical gap between legal, technical, and organisational notions of algorithmic fairness. Through analysis of EU non-discrimination law and jurisprudence of the European Court of Justice (ECJ) and national courts, we identify a critical incompatibility between European notions of discrimination and existing work on algorithmic and automat-ed fairness. A clear gap exists between statistical measures of fairness as embedded in myriad fairness toolkits and governance mechanisms and the context-sensitive, often intuitive and ambiguous discrimination metrics and evidential requirements used by the ECJ; we refer to this approach as “contextual equality.” This Article makes three contributions. First, we review the evidential requirements to bring a claim under EU non-discrimination law. Due to the disparate nature of algorithmic and human discrimination, the EU’s current requirements are too contextual, reliant on intuition, and open to judicial interpretation to be automated. Many of the concepts fundamental to bringing a claim, such as the composition of the disadvantaged and advantaged group, the severity and type of harm suffered, and requirements for the relevance and admissibility of evidence, require normative or political choices to be made by the judiciary on a case-by-case basis. We show that automating fairness or non-discrimination in Europe may be impossible because the law, by design, does not provide a static or homogenous framework suited to testing for discrimination in AI systems. Second, we show how the legal protection offered by non-discrimination law is challenged when AI, not humans, discriminate. Humans discriminate due to negative attitudes (e.g. stereotypes, prejudice) and unintentional biases (e.g. organisational practices or internalised stereotypes) which can act as a signal to victims that discrimination has occurred. Equivalent signalling mechanisms and agency do not exist in algorithmic systems. Compared to traditional forms of discrimination, automated discrimination is more abstract and unintuitive, subtle, intangible, and difficult to detect. The increasing use of algorithms disrupts traditional legal remedies and procedures for detection, investigation, prevention, and correction of discrimination which have predominantly relied upon intuition. Consistent assessment procedures that define a common standard for statistical evidence to detect and assess prima facie automated discrimination are urgently needed to support judges, regulators, system controllers and developers, and claimants. Finally, we examine how existing work on fairness in machine learning lines up with procedures for assessing cases under EU non-discrimination law. A ‘gold standard’ for assessment of prima facie discrimination has been advanced by the European Court of Justice but not yet translated into standard assessment procedures for automated discrimination. We propose ‘conditional demographic disparity’ (CDD) as a standard baseline statistical measurement that aligns with the Court’s ‘gold standard’. Establishing a standard set of statistical evidence for automated discrimination cases can help ensure consistent procedures for assessment, but not judicial interpretation, of cases involving AI and automated systems. Through this proposal for procedural regularity in the identification and assessment of auto-mated discrimination, we clarify how to build considerations of fairness into automated systems as far as possible while still respecting and enabling the contextual approach to judicial interpretation practiced under EU non-discrimination law.";Why Fairness Cannot Be Automated: Bridging the Gap Between EU Non-Discrimination Law and AI;Wachter, S., Mittelstadt, B. & Russell, C.;2020;Mittelstadt, B.;"Ethics and Philosophy of Information; Digital Politics and Government; Information Governance and Security"
The therapeutic paradigm in Alzheimer’s disease (AD) is shifting from symptoms management toward prevention goals. Secondary prevention requires the identification of individuals without clinical symptoms, yet “at-risk” of developing AD dementia in the future, and thus, the use of predictive modeling. Objective:The objective of this study was to review the ethical concerns and social implications generated by this new approach. We conducted a systematic literature review in Medline, Embase, PsycInfo, and Scopus, and complemented it with a gray literature search between March and July 2018. Then we analyzed data qualitatively using a thematic analysis technique. Results:We identified thirty-one ethical issues and social concerns corresponding to eight ethical principles: (i) respect for autonomy, (ii) beneficence, (iii) non-maleficence, (iv) equality, justice, and diversity, (v) identity and stigma, (vi) privacy, (vii) accountability, transparency, and professionalism, and (viii) uncertainty avoidance. Much of the literature sees the discovery of disease-modifying treatment as a necessary and sufficient condition to justify AD risk assessment, overlooking future challenges in providing equitable access to it, establishing long-term treatment outcomes and social consequences of this approach, e.g., medicalization. The ethical/social issues associated specifically with predictive models, such as the adequate predictive power and reliability, infrastructural requirements, data privacy, potential for personalized medicine in AD, and limiting access to future AD treatment based on risk stratification, were covered scarcely. Conclusion:The ethical discussion needs to advance to reflect recent scientific developments and guide clinical practice now and in the future, so that necessary safeguards are implemented for large-scale AD secondary prevention.;Ethical and Social Implications of Using Predictive Modeling for Alzheimer’s Disease Prevention: A Systematic Literature Review;Angehrn, Z., Sostar, J. & Nordon, C. et al. ;2020;Mittelstadt, B.;Ethics and Philosophy of Information
"Dementia has been described as the greatest global health challenge in the 21st Century on account of longevity gains increasing its incidence, escalating health and social care pressures. These pressures highlight ethical, social, and political challenges about healthcare resource allocation, what health improvements matter to patients, and how they are measured. This study highlights the complexity of the ethical landscape, relating particularly to the balances that need to be struck when allocating resources; when measuring and prioritizing outcomes; and when individual preferences are sought. Health outcome prioritization is the ranking in order of desirability or importance of a set of disease-related objectives and their associated cost or risk. We analyze the complex ethical landscape in which this takes place in the most common dementia, Alzheimer’s disease. Narrative review of literature published since 2007, incorporating snowball sampling where necessary. We identified, thematized, and discussed key issues of ethical salience. Eight areas of ethical salience for outcome prioritization emerged: 1) Public health and distributive justice, 2) Scarcity of resources, 3) Heterogeneity and changing circumstances, 4) Knowledge of treatment, 5) Values and circumstances, 6) Conflicting priorities, 7) Communication, autonomy and caregiver issues, and 8) Disclosure of risk. These areas highlight the difficult balance to be struck when allocating resources, when measuring and prioritizing outcomes, and when individual preferences are sought. We conclude by reflecting on how tools in social sciences and ethics can help address challenges posed by resource allocation, measuring and prioritizing outcomes, and eliciting stakeholder preferences.";Health Outcome Prioritization in Alzheimer’s Disease: Understanding the Ethical Landscape;McKeown, A., Turner, A. & Angehrn, Z. et al.;2020;Mittelstadt, B.;Ethics and Philosophy of Information
Artificial intelligence (AI) ethics is now a global topic of discussion in academic and policy circles. At least 84 public–private initiatives have produced statements describing high-level principles, values and other tenets to guide the ethical development, deployment and governance of AI. According to recent meta-analyses, AI ethics has seemingly converged on a set of principles that closely resemble the four classic principles of medical ethics. Despite the initial credibility granted to a principled approach to AI ethics by the connection to principles in medical ethics, there are reasons to be concerned about its future impact on AI development and governance. Significant differences exist between medicine and AI development that suggest a principled approach for the latter may not enjoy success comparable to the former. Compared to medicine, AI development lacks (1) common aims and fiduciary duties, (2) professional history and norms, (3) proven methods to translate principles into practice, and (4) robust legal and professional accountability mechanisms. These differences suggest we should not yet celebrate consensus around high-level principles that hide deep political and normative disagreement.;Principles alone cannot guarantee ethical AI;Mittelstadt, B.;2019;Mittelstadt, B.;Ethics and Philosophy of Information
In modern information societies, individuals generate streams of diverse and potentially valuable data. Digital technologies now easily and routinely record data about the behaviours and preferences of individuals at an unprecedented scale. Analytic techniques to make sense of this glut of data have grown in parallel, ushering in what some have called the age of ‘Big Data’.;The Ethics of Biomedical ‘Big Data’ Analytics;Mittelstadt, B.;2019;Mittelstadt, B.;Ethics and Philosophy of Information
"Big Data analytics and artificial intelligence (AI) draw non-intuitive and unverifiable inferences and predictions about the behaviors, preferences, and private lives of individuals. These inferences draw on highly diverse and feature-rich data of unpredictable value, and create new opportunities for discriminatory, biased, and invasive decision-making. Data protection law is meant to protect people’s privacy, identity, reputation, and autonomy, but is currently failing to protect data subjects from the novel risks of inferential analytics. The legal status of inferences is heavily disputed in legal scholarship, and marked by inconsistencies and contradictions within and between the views of the Article 29 Working Party and the European Court of Justice (ECJ). This Article shows that individuals are granted little control or oversight over how their personal data is used to draw inferences about them. Compared to other types of personal data, inferences are effectively “economy class” personal data in the General Data Protection Regulation (GDPR). Data subjects’ rights to know about (Articles 13–15), rectify (Article 16), delete (Article 17), object to (Article 21), or port (Article 20) personal data are significantly curtailed for inferences. The GDPR also provides insufficient protection against sensitive inferences (Article 9) or remedies to challenge inferences or important decisions based on them (Article 22(3)). This situation is not accidental. In standing jurisprudence the ECJ has consistently restricted the remit of data protection law to assessing the legitimacy of input personal data undergoing processing, and to rectify, block, or erase it. Critically, the ECJ has likewise made clear that data protection law is not intended to ensure the accuracy of decisions and decision-making processes involving personal data, or to make these processes fully transparent. Current policy proposals addressing privacy protection (the ePrivacy Regulation and the EU Digital Content Directive) and Europe’s new Copyright Directive and Trade Secrets Directive also fail to close the GDPR’s accountability gaps concerning inferences. This Article argues that a new data protection right, the “right to reasonable inferences,” is needed to help close the accountability gap currently posed by “high risk inferences,” meaning inferences drawn from Big Data analytics that damage privacy or reputation, or have low verifiability in the sense of being predictive or opinion-based while being used in important decisions. This right would require ex-ante justification to be given by the data controller to establish whether an inference is reasonable. This disclosure would address (1) why certain data form a normatively acceptable basis from which to draw inferences; (2) why these inferences are relevant and normatively acceptable for the chosen processing purpose or type of automated decision; and (3) whether the data and methods used to draw the inferences are accurate and statistically reliable. The ex-ante justification is bolstered by an additional ex-post mechanism enabling unreasonable inferences to be challenged.";A Right to Reasonable Inferences: Re-Thinking Data Protection Law in the Age of Big Data and AI;Wachter, S. & Mittelstadt, B.;2019;Mittelstadt, B.;"Ethics and Philosophy of Information; Digital Politics and Government; Information Governance and Security"
There has been much discussion of the “right to explanation” in the EU General Data Protection Regulation, and its existence, merits, and disadvantages. Implementing a right to explanation that opens the ‘black box’ of algorithmic decision-making faces major legal and technical barriers. Explaining the functionality of complex algorithmic decision-making systems and their rationale in specific cases is a technically challenging problem. Some explanations may offer little meaningful information to data subjects, raising questions around their value. Data controllers have an interest to not disclose information about their algorithms that contains trade secrets, violates the rights and freedoms of others (e.g. privacy), or allows data subjects to game or manipulate decision-making. Explanations of automated decisions need not hinge on the general public understanding how algorithmic systems function. Even though such interpretability is of great importance and should be pursued, explanations can, in principle, be offered without opening the black box. Looking at explanations as a means to help a data subject act rather than merely understand, one could gauge the scope and content of explanations according to the specific goal or action they are intended to support. From the perspective of individuals affected by automated decision-making, we propose three aims for explanations: (1) to inform and help the individual understand why a particular decision was reached, (2) to provide grounds to contest the decision if the outcome is undesired, and (3) to understand what would need to change in order to receive a desired result in the future, based on the current decision-making model. We assess how each of these goals finds support in the GDPR, and the extent to which they hinge on opening the ‘black box’. We suggest data controllers should offer a particular type of explanation, ‘unconditional counterfactual explanations’, to support these three aims. These counterfactual explanations describe the smallest change to the world that can be made to obtain a desirable outcome, or to arrive at the “closest possible world.” As multiple variables or sets of variables can lead to one or more desirable outcomes, multiple counterfactual explanations can be provided, corresponding to different choices of nearby possible worlds for which the counterfactual holds. Counterfactuals describe a dependency on the external facts that lead to that decision without the need to convey the internal state or logic of an algorithm. As a result, counterfactuals serve as a minimal solution that bypasses the current technical limitations of interpretability, while striking a balance between transparency and the rights and freedoms of others (e.g. privacy, trade secrets).;Counterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR;Wachter, S., Mittelstadt, B. & Russell, C.;2017;Mittelstadt, B.;"Ethics and Philosophy of Information; Digital Politics and Government; Information Governance and Security"
This paper poses the question of whether people have a duty to participate in digital epidemiology. While an implied duty to participate has been argued for in relation to biomedical research in general, digital epidemiology involves processing of non-medical, granular and proprietary data types that pose different risks to participants. We first describe traditional justifications for epidemiology that imply a duty to participate for the general public, which take account of the immediacy and plausibility of threats, and the identifiability of data. We then consider how these justifications translate to digital epidemiology, understood as an evolution of traditional epidemiology that includes personal and proprietary digital data alongside formal medical datasets. We consider the risks imposed by re-purposing such data for digital epidemiology and propose eight justificatory conditions that should be met in justifying a duty to participate for specific digital epidemiological studies. The conditions are then applied to three hypothetical cases involving usage of social media data for epidemiological purposes. We conclude with a list of questions to be considered in public negotiations of digital epidemiology, including the application of a duty to participate to third-party data controllers, and the important distinction between moral and legal obligations to participate in research.;Is there a duty to participate in digital epidemiology?;Mittelstadt, B., Benzel, J. & Engelmann, L. et al.;2018;Mittelstadt, B.;Ethics and Philosophy of Information
The internet of things is increasingly spreading into the domain of medical and social care. Internet-enabled devices for monitoring and managing the health and well-being of users outside of traditional medical institutions have rapidly become common tools to support healthcare. Health-related internet of things (H-IoT) technologies increasingly play a key role in health management, for purposes including disease prevention, real-time tele-monitoring of patient’s functions, testing of treatments, fitness and well-being monitoring, medication dispensation, and health research data collection. H-IoT promises many benefits for health and healthcare. However, it also raises a host of ethical problems stemming from the inherent risks of Internet enabled devices, the sensitivity of health-related data, and their impact on the delivery of healthcare. This paper maps the main ethical problems that have been identified by the relevant literature and identifies key themes in the on-going debate on ethical problems concerning H-IoT.;Ethics of the health-related internet of things: a narrative review;Mittelstadt, B.;2017;Mittelstadt, B.;Ethics and Philosophy of Information
The conjunction of wireless computing, ubiquitous Internet access, and the miniaturisation of sensors have opened the door for technological applications that can monitor health and well-being outside of formal healthcare systems. The health-related Internet of Things (H-IoT) increasingly plays a key role in health management by providing real-time tele-monitoring of patients, testing of treatments, actuation of medical devices, and fitness and well-being monitoring. Given its numerous applications and proposed benefits, adoption by medical and social care institutions and consumers may be rapid. However, a host of ethical concerns are also raised that must be addressed. The inherent sensitivity of health-related data being generated and latent risks of Internet-enabled devices pose serious challenges. Users, already in a vulnerable position as patients, face a seemingly impossible task to retain control over their data due to the scale, scope and complexity of systems that create, aggregate, and analyse personal health data. In response, the H-IoT must be designed to be technologically robust and scientifically reliable, while also remaining ethically responsible, trustworthy, and respectful of user rights and interests. To assist developers of the H-IoT, this paper describes nine principles and nine guidelines for ethical design of H-IoT devices and data protocols.;Designing the Health-related Internet of Things: Ethical Principles and Guidelines;Mittelstadt, B.;2017;Mittelstadt, B.;Ethics and Philosophy of Information
Do we have a right to transparency when we use content personalization systems? Building on prior work in discrimination detection in data mining, I propose algorithm auditing as a compatible ethical duty for providers of content personalization systems to maintain the transparency of political discourse. I explore barriers to auditing that reveal the practical limitations on the ethical duties of service providers. Content personalization systems can function opaquely and resist auditing. However, the belief that highly complex algorithms, such as bots using machine learning, are incomprehensible to human users should not be an excuse to surrender high quality political discourse. Auditing is recommended as a way to map and redress algorithmic political exclusion in practice. However, the opacity of algorithmic decision making poses a significant challenge to the implementation of auditing.;Auditing for Transparency in Content Personalization Systems;Mittelstadt, B.;2016;Mittelstadt, B.;Ethics and Philosophy of Information
Computing technologies and artifacts are increasingly integrated into most aspects of our professional, social, and private lives. One consequence of this growing ubiquity of computing is that it can have significant ethical implications that computing professionals need to be aware of. The relationship between ethics and computing has long been discussed. However, this is the first comprehensive survey of the mainstream academic literature of the topic. Based on a detailed qualitative analysis of the literature, the article discusses ethical issues, technologies that they are related to, and ethical theories, as well as the methodologies that the literature employs, its academic contribution, and resulting recommendations. The article discusses general trends and argues that the time has come for a transition to responsible research and innovation to ensure that ethical reflection of computing has practical and manifest consequences.;The Ethics of Computing: A Survey of the Computing-Oriented Literature;Stahl, B., Timmermans, J. & Mittelstadt, B.;2016;Mittelstadt, B.;Ethics and Philosophy of Information
Recent years have seen an influx of ambient assisted living (AAL) technologies capable of remotely monitoring the health and behaviours of individuals at home. AAL can enable elderly and infirm patients to age more independently at home, potentially reducing the need for home visits or in-patient care. This paper reports on the results of an empirical study in the UK with potential users of AAL concerning the ethical implications of the devices for informal carers. Responsibilities and moral obligations for care can be subtly shifted to the community of informal carers under the guise of patient empowerment and self-care. The ethical acceptability of such shifts are unclear, raising questions over what sacrifices can reasonably be expected of informal carers when AAL is introduced for the sake of the patient, and to the benefit of public healthcare services.;MORAL OBLIGATIONS TO MONITOR IN INFORMAL CARE: THE CASE OF AMBIENT ASSISTED LIVING;Mittelstadt, B.;2015;Mittelstadt, B.;Ethics and Philosophy of Information
Empirical research into the ethics of emerging technologies, often involving foresight studies, technology assessment or application of the precautionary principle, raises significant epistemological challenges by failing to explain the relative epistemic status of contentious normative claims about future states. This weakness means that it is unclear why the conclusions reached by these approaches should be considered valid, for example in anticipatory ethical assessment or governance of emerging technologies. This paper explains and responds to this problem by proposing an account of how the epistemic status of uncertain normative claims can be established in ethical and political discourses based on Jürgen Habermas’ discourse ethics. To better understand the nature of the problem, the relationship between norms, facts and the future is explored in light of potential meta-ethical fallacies faced in the field of empirical ethics. Weaknesses of current approaches to anticipatory ethical assessment and governance are then explored, including the Precautionary Principle and Technology Assessment. We argue that the epistemic status of uncertain normative claims can be understood within Habermas’ approach to political discourse, which requires ‘translation’ of uncertain claims to be comprehensible to other stakeholders in discourse. Translation thus provides a way to allow for uncertain normative claims to be considered alongside other types of validity claims in discourse. The paper contributes a conceptual account of the epistemic status of uncertain normative claims in discourse and begins to develop a ‘methodology of translation’ which can be further developed for approaches to research and ethical assessment supporting anticipatory evidence-based policy, governance and system design.;How to Shape a Better Future? Epistemic Difficulties for Ethical Assessment and Anticipatory Governance of Emerging Technologies;Mittelstadt, B., Stahl, B. & Fairweather, B.;2015;Mittelstadt, B.;Ethics and Philosophy of Information
Value-Sensitive Design (VSD) offers a methodology to make social and moral values central to the design and development of new technologies. Although VSD incorporates views from philosophy and stakeholders involved in the process, it notably lacks reflexivity on the position of those involved in conducting the methodology itself e.g. ethicists, researchers and/or designers. Where technology is considered value-laden, the neutrality of those involved in the methodology seems to be taken for granted. The problem of a lack of reflexivity among VSD practitioners means stakeholder values may have less influence on technological design than is methodologically intended. Understanding VSD as an extension of applied ethical assessment offers a fruitful outlook on how to tackle this problem. As in any sort of ethical assessment, reflexivity and transparency are key to allow all stakeholders to participate, and to ensure outcomes are not decided through power relations alone. This will promote reflexivity and forward a culturally-sensitive approach to VSD that may simplify the process across different cultures. If the designs produced by VSD are seen as products of ethical debates, the lack of reflexivity in VSD undermines the credibility of the designs, understood in terms of how far the designs reflect the values of stakeholders encountered and not merely those of the designers. Without reflexivity the reasons supporting prioritisation and analysis of stakeholder values lose credibility, undermining the designs and technologies produced through VSD. As it stands, it is difficult to defend VSD and its designs as products of a coherent design methodology which identifies clear reasons for preferring certain values over others. To begin to correct this deficiency, the cultural-specificity of dominant 'value frameworks' in VSD should be reflexively assessed against the outputs of the SATORI FP7 project by practitioners reporting on VSD.;Reflexivity and Value-Sensitive Design;Timmermans, J. & Mittelstadt, B.;2014;Mittelstadt, B.;Ethics and Philosophy of Information
Personal health monitoring (PHM) systems capable of gathering pervasive physiological and behavioural data are currently in development to supplementexisting medical resources. As a technology designed to operate in the private sphere,PHM can digitise, record and analyse the lives of patients, creating opportunities for data sharing, mining and social categorisation. Medical care and health outcomesmay be improved through increasingly granular monitoring and personalisedinterventions, but these outcomes may come at the cost of user privacy and relatedethical implications. As an emerging technology, the opportunity remains to proactively respond to the potential normative risks of a PHM-enabled future. In this paper, a critical overview of the treatment of privacy, risk and PHM in academicliterature is offered. The current discourse is defined by a conceptually narrowdefinition of privacy among developers of PHM systems and security architecture,which suggests that emerging PHM systems may fail to meet the context-specific privacy expectations of users.;PRIVACY, RISK AND PERSONAL HEALTHMONITORING;Mittelstadt, B., Fairweather, B. & McBride, N. ;2013;Mittelstadt, B.;Ethics and Philosophy of Information
Personal Health Monitoring (PHM) technologies are currently in development to supplementmedical care environments with health monitoring outside “brick and mortar” settings to better meet the needs of people with long-term illnesses. This review identifies commonthemes in the current literature discussing ethics of PHM and gaps in need of further research.Identified themes include privacy, autonomy, medicalization, social isolation, visibility andimpact on healthcare providers. An in-depth discussion of the ethical issues of PHM wasrarely found in the searched literature. Areas in need of further research include inadvertentmonitoring and the impact of PHM on families and patient relationships.;ETHICAL ISSUES OF PERSONAL HEALTH MONITORING:A LITERATURE REVIEW;Mittelstadt, B., Fairweather, B. & McBride, N.;2011;Mittelstadt, B.;Ethics and Philosophy of Information
Recent years have seen an influx of medical technologies capable of remotely monitoring the health and behaviours of individuals to detect, manage and prevent health problems. Known collectively as personal health monitoring (PHM), these systems are intended to supplement medical care with health monitoring outside traditional care environments such as hospitals, ranging in complexity from mobile devices to complex networks of sensors measuring physiological parameters and behaviours. This research project assesses the potential ethical implications of PHM as an emerging medical technology, amenable to anticipatory action intended to prevent or mitigate problematic ethical issues in the future. PHM fundamentally changes how medical care can be delivered: patients can be monitored and consulted at a distance, eliminating opportunities for face-to-face actions and potentially undermining the importance of social, emotional and psychological aspects of medical care. The norms evident in this movement may clash with existing standards of ‘good’ medical practice from the perspective of patients, clinicians and institutions. By relating utilitarianism, virtue ethics and theories of surveillance to Habermas’ concept of colonisation of the lifeworld, a conceptual framework is created which can explain how PHM may be allowed to change medicine as a practice in an ethically problematic way. The framework relates the inhibition of virtuous behaviour among practitioners of medicine, understood as a moral practice, to the movement in medicine towards remote monitoring. To assess the explanatory power of the conceptual framework and expand its borders, a qualitative interview empirical study with potential users of PHM in England is carried out. Recognising that the inherent uncertainty of the future undermines the validity of empirical research, a novel epistemological framework based in Habermas’ discourse ethics is created to justify the empirical study. By developing Habermas’ concept of translation into a procedure for assessing the credibility of uncertain normative claims about the future, a novel methodology for empirical ethical assessment of emerging technologies is created and tested. Various methods of analysis are employed, including review of academic discourses, empirical and theoretical analyses of the moral potential of PHM. Recommendations are made concerning ethical issues in the deployment and design of PHM systems, analysis and application of PHM data, and the shortcomings of existing research and protection mechanisms in responding to potential ethical implications of the technology.;On the Ethical Implications of Personal Health Monitoring;Mittelstadt, B.;2013;Mittelstadt, B.;Ethics and Philosophy of Information
In this chapter I propose an ethical analysis of information warfare, the warfare waged in the cyber domain. The goal is twofold, filling the theoretical vacuum surrounding this phenomenon and providing the conceptual grounding for the definition of new ethical regulations for information warfare. I argue that Just War Theory is a necessary but not sufficient instrument for considering the ethical implications of information warfare and that a suitable ethical analysis of this kind of warfare is developed when Just War Theory is merged with Information Ethics. In the initial part of the chapter, I describe information warfare and its main features and highlight the problems that arise when Just War Theory is endorsed as a means of addressing ethical problems engendered by this kind of warfare. In the final part, I introduce the main aspects of Information Ethics and define three principles for a just information warfare resulting from the integration of Just War Theory and Information Ethics.;Just Information Warfare;Taddeo, M.;2017;Taddeo, M.;Ethics and Philosophy of Information
"The World Economic Forum’s Global Risks Report 2019 ranked cyber attacks among the top-ten most impactful global risks. A report published in 2019 by the Ponemon Institute shows that 90% of companies supporting national critical infrastructures—energy, health, industrial and manufacturing, and transport—experienced at least one cyber attacks between 2017 and 2019 that led to data breaches or significant disruption of operations (Ponemon Institute LLC 2019). These reports are two of a long series of studies conducted over the past decade on the status of cybersecurity. From year to year, data about cyber attacks and their impact continue to increase indicating that cyber attacks pose an ever-growing threat for information societies. There are two lessons to be learned from these data. The first lesson is not controversial, digital infrastructures are porous. We should think of them as agile, flexible, but brittle systems. This brittleness, as I argued elsewhere (Taddeo 2016, 2017a), favours offence over defence, explaining in part the continue growth of cyber threats and the escalation of their impact. The more digital technologies become pervasive, the wider becomes the surface of attacks, and with it also number of successful attacks grows. Think for example about the distribution of Internet of Things (IoT). In 2018, a Symantec study reported an average of 5200 attacks per month on IoT devices, the figure almost double the 3650 attacks counted in 2016. The second lesson may be harder to learn, for it is about the inadequacy of the ways in which we have framed and governed cybersecurity. This is clear when considering that data on the escalation of number and impact of cyber attacks, despite the growing value of the cybersecurity market and the increasing efforts of companies and state actors to improve the security of information systems and infrastructures (Technavio 2018). The lack of effective cybersecurity measures has a potential knock-on effect on the information revolution, and on the development of information societies around the globe (Floridi 2016). Two aspects are relevant here: international stability and trust. Without proper security measures in place, cyber threats may undermine the stability of information societies (Taddeo and Floridi 2018a), making digital technologies a source of risks as well as a source of development. The series of cyber attacks that allegedly Russia and the US launched against each other’s national critical infrastructures between 2018 and 2019Footnote 1 is indicative of how cyber attacks may pose a threat to national stability. At the same time, lack of security of digital technologies will erode the trust of users (Taddeo 2010, 2012, 2017b); this in turn will cripple adoption, and hinder innovation. Learning the second lesson entails reconsidering the frameworks underpinning the governance of cybersecurity. In this respect, there is a mounting consensus on treating cybersecurity as a public good to be managed in the public interest (Mulligan and Schneider 2011; Schneider et al. 2016; Weber 2017). I agree with this view. ‘Cyber’ is a constitutive elements of information societies, it is interwoven with the physical, economic, social and political elements, and its security it is essential to foster societal development, technological progress (Floridi 2014), and also to harness the potential of digital technologies to deliver socially good outcomes (Taddeo and Floridi 2018b). Cybersecurity encompasses a wide set of practices, from risk assessment and penetration tests; disaster recovery; cryptography; access control and surveillance; architecture, software, and network security; to hack-back and security operations, and physical security. Framing cybersecurity as a public good without a careful distinction of between practices, scopes, and actors is conceptually unwarranted and problematic when considering the governance of cybersecurity. At a high level of abstraction cybersecurity has three main domains—engineering systems that are robust and can withstand attacks; design methods and system for threat and anomaly detection to guarantee a system’s resilience; define system responses to attacks. While I agree that system robustness qualifies as a public good, I argue that this is not the case for system’s resilience and response.";Is Cybersecurity a Public Good?;Taddeo, M.;2019;Taddeo, M.;Ethics and Philosophy of Information
"In 2017, the WannCry and NotPeya showed that attacks targeting the cyber component of infrastructures (e.g. attacks on power plants), services (e.g. attacks to banks or hospitals servers), and endpoint devices (e.g. attacks on mobiles and personal computers) have a great disruptive potential and could cause serious damage to our information societies. WannaCry crippled hundreds of IT systems. And NotPetya costed pharmaceutical giant Merck, shipping ﬁrm Maersk and logistics company FedEx around US$300 million each. At a global level, cyber crime causes multi-billion dollar losses to businesses, with average losses per organization running from US$3.8 to US$16.8 million in the smallest and largest quartiles respectively (Accenture 2017). The picture did not improve in 2018. Data show that over the year 2.6 million people encountered newly discovered malware on a daily basis.1 Attacks ranged over 1.7 million diﬀerent forms of malware, and 60% of the attacks lasted less than 1h. Cyber attacks are escalating in frequency, impact, and sophistication. The escalation is due to several factors, for example, attacking in cyberspace is easier than defending; most attacks remain unattributed and, therefore, unpunished. Moreover, as defences are porous, cyber attacks are more likely to succeed than not (Taddeo 2017b). Artiﬁcial intelligence (AI)2 could help to improve defences and reduce the impact of cyber attacks. This is why initiatives to develop applications of AI for cybers security applicationsare attracting increasing attention both within the private and public sector (The 2019 Oﬃcial Annual Cybercrime Report 2019).";Three Ethical Challenges of Applications of Artificial Intelligence in Cybersecurity;Taddeo, M.;2019;Taddeo, M.;Ethics and Philosophy of Information
In a recent report,1 the UK Digital, Culture, Media and Sport (DCMS) Commit- tee focused on the role and responsibilities of online service providers (OSPs) with respect to the circulation of fake news and their impact on democratic processes, like public debate and political elections. The first recommendation offered in the report calls for “compulsory Code of Ethics for tech companies overseen by inde- pendent regulator”. The recommendation is sensible and should be adopted. Schol- ars working in the area of digital ethics have often stressed the need for a code of ethics shaping the conduct of OSPs (Taddeo and Floridi 2017). And an authority with teeth enforcing the code should be a measure welcome by the public sector, civil society, and OSPs themselves. On the one hand, the authority would ensure that OSPs respect essential values and principles safeguarding users’ rights and fun- damental processes of our societies. On the other hand, an authority endorsing a code of ethics and recognising (when appropriate) compliance with it would help OSPs to improve their reputation, build their trustworthiness, and hence breed users’ trust (Taddeo 2017).;The Civic Role of Online Service Providers;Taddeo, M.;2019;Taddeo, M.;Ethics and Philosophy of Information
In this article, I analyse deterrence theory and argue that its applicability to cyberspace is limited and that these limits are not trivial. They are the consequence of fundamental differences between deterrence theory and the nature of cyber conflicts and cyberspace. The goals of this analysis are to identify the limits of deterrence theory in cyberspace, clear the ground of inadequate approaches to cyber deterrence, and define the conceptual space for a domain-specific theory of cyber deterrence, still to be developed.;The Limits of Deterrence Theory in Cyberspace;Taddeo, M.;2018;Taddeo, M.;Ethics and Philosophy of Information
"Deterrence in cyberspace is possible. But it requires an effort to develop a new domain-specific, conceptual, normative, and strategic framework. To be successful, cyber deterrence needs to shift from threatening to prevailing. I argue that by itself, deterrence is insufficient to ensure stability of cyberspace. An international regime of norms regulating state behaviour in cyberspace is necessary to complement cyber deterrence strategies and foster stability. Enforcing this regime requires an authority able to ensure States compliance with the norms at an international level, run investigations into suspected State-run (or Statesponsored) cyber operations to define attribution, expose breaches of the norms, and impose adequate sanctions and punishments. These requirements define a political mandate for an authority that will have a deep impact on international relations and geo-political equilibriums. The UN Security Council has the necessary resources and the political and coercive power to meet these requirements. The time has come to embrace this power to consolidate and enforce an international regime of norms to regulate state behaviour in cyberspace. Problems, mistakes, and even failures are to be expected, but they must not hinder the process. In March 2018, the US Computer Emergency Readiness Team (CERT) issued an alert on a series of cyber attacks attributed to Russian government and targeting US governmental offices and infrastructures in the energy, nuclear, water, aviation, and critical manufacturing sectors. Later in the year, the director of the US National Intelligence echoed the CERT’s alert, stating that the US digital infrastructure “is literally under attack”. The US case offers a good example of the level of threat that cyber attacks pose to national security and defense of mature information societies (Floridi 2016). Cyber attacks are escalating in frequency, impact, and sophistication. And state actors often play a central role in the escalation process. Starting in 2003, States have relayed frequently on cyber operations for espionage and sabotage purposes. Well-known examples range from Titan Rain (2003), the Russian attack against Estonia (2006) and Georgia (2008), to Stuxnet and Operation Olympic Game (2006–2012), WannaCray and NotPetya (2017). This trend will continue. The relatively low entry-cost and the high chances of success mean that states will keep developing, relying on, and deploying cyber attacks, thus increasing the risks of their escalation. Scholars, militaries, and policy makers have stressed that deterrence may play a significant role in mitigating these risks and fostering stability of cyberspace (Freedberg 2014; UN Institute for Disarmament Research 2014; Taddeo 2017b). Most of the existing analyses refer to deterrence theory. They address deterrence as a coercive strategy based on conditional threats with the goal of persuading the opponent to behave in a desirable way. According to this theory, in a scenario in which as State is planning to attack another State, deterrence will be effective if the defendant is able to identify with certainty the opponent (attribution) and communicate to it (signalling) a credible threat (punishments or denial) proportionate to the damage that the opponent is planning to cause, but severe enough to outweigh any advantage that the opponent may gain from attacking. However, applying deterrence theory to cyberspace poses serious problems. The distributed and the interconnected nature of the domain (Chadwick and Howard 2009) makes it difficult to define territoriality and sovereignty of States and hence to identify the boundaries for States’ actions. The non-physical nature of cyber attacks (Taddeo 2012) hampers the assessment of the damage that they may cause and, hence, of the proportionality of responses. The difficulties to attribute with certainty cyber attacks to their authors undermine the very core of deterrence theory: if the opponent cannot be identified, it is impossible to issue a meaningful threat. The success of deterrence theory, and some suggest of deterrence itself, in cyberspace hinges upon the possibility to address these problems (Kugler 2009). Some analyses maintain that, given the differences with kinetic (violent) conflicts, solving these problems is impossible and that deterrence theory cannot be applied to the case of cyber conflicts (Lan et al. 2010). They conclude that deterrence in cyberspace is unattainable. Others hold the opposite view, and stress that it is possible to deter in cyberspace, precisely because deterrence theory can be successfully applied in this domain (Crosston 2011). Both positions are misled. They both draw on an analogy between deterrence of cyber attacks and deterrence of kinetic attacks (Taddeo 2016) and conclude that cyber deterrence is possible only insofar as deterrence theory can be applied to cyberspace (Owens et al. 2009; Nye 2011). This approach overlooks the specific nature of cyber attacks and cyberspace, and disregards the dynamics of cyber conflicts (Taddeo (2014, 2016, 2017b). Kinetic and cyber conflicts differ radically in several, crucial aspects, ranging from clarity of attribution, the destructive power of the attacks, to the nature of the involved actors and targets (Libicki 2009; Floridi and Taddeo 2014). For these reasons, analogies between cyber and kinetic conflicts are not warranted and should be abandoned. Efforts should focus on developing an in-depth understanding of cyberspace and cyber conflicts and define a domain-specific framework for deterrence. The alternative is risky. It is equivalent to forcing the proverbial square peg (cyber deterrence) into a round hole (deterrence theory): we are more likely to smash the toy than to win the game. As USN Commander Bebber stated: [Military] history suggests that applying the wrong operational framework to an emerging strategic environment is a recipe for failure. During the World War I, both sides failed to realize that large scale artillery barrages followed by massed infantry assaults were hopeless on a battlefield that strongly favored well-entrenched defense supported by machine gun technology. […] The failure to adapt had disastrous consequences. This is also the case when considering cyber deterrence. Analyses of cyber deterrence need to consider the strategic nature of cyberspace and the new capabilities availed by technology, in order to provide the right conceptual framework and define successful deterrence strategies. At the same time, cyber attacks and defense evolve along with digital technology. As the latter becomes increasingly autonomous and smart, leveraging the potential of artificial intelligence (AI) (Yang et al. 2018), so do cyber attacks and defense strategies. Both the public and private sectors are already testing AI systems in autonomous war games (Taddeo and Floridi 2018). The 2016 DARPA Cyber Grand Challenge was a landmark in this respect. The Challenge was the first competition in which AI capabilities for defense were successfully tested and showed to be able to identify and patch their own vulnerabilities, while also countering threats and targeting the vulnerabilities of antagonist systems. Strategically, cyberspace is an environment of persistent offense, where attacking is tactically and strategically more advantageous than defending. As Harknett and Goldman (2016) argue, in an offense-persistent environment, defense can achieve tactical and operational success in the short term if it can constantly adjust to the means of attack, but it cannot win strategically. Offense will persist and interactions with the enemy will remain constant. Given the strategic nature of cyberspace and the role of AI in cyber defense, I argued elsewhere (Taddeo 2018) that, to be effective, a theory of cyber deterrence rests on three elements: target identification, retaliation, and demonstration (Fig. 1). According to the model showed in Fig. 1, target identification is essential for deterrence. It allows the defendant to isolate (and counter-attack) enemy systems independently from the identification of the actors behind them, thus side-stepping the attribution problem, while identifying a justifiable target for retaliation. Identifying the attacking system and retaliate is feasible task, one which AI systems for defense can already achieve. Deterrence in cyberspace works by demonstrating the defendant’s capability to retaliate an occurring attack and harming the opponent’s system. While it may not deter an incoming cyber attack, retaliation will deter the next rounds of attacks coming from the same opponent. This is because, given the offense-persistent nature of cyberspace, the mere threat of retaliation will not be sufficient, at first, to change the opponent’s intentions to attacks. The chances of success and the likelihood that the attack will remain unattributed remain too high for threats, albeit credible, to be effective. Thus, to be successful, cyber deterrence need to shift from threatening to prevailing. By deploying this strategy, States will be able to build a reputation on the basis of their capability and commitment to retaliate, which will lead, over time, to stronger cyber deterrence postures. While this model would enable deterrence of cyber attacks, by itself it is insufficient to ensure stability of cyberspace. This is true especially when considering how the rising distribution and automation, multiple interactions, and fast-pace performance of cyber attacks make control progressively less effective, while increasing the risks for unforeseen consequences, proportionality breaches, and escalation of responses. An international regime of norms regulating state behaviour in cyberspace is necessary to complement cyber deterrence strategies and foster stability (Taddeo and Floridi 2018). This is why the 2017 failure of the UN Governmental Group of Experts on “Developments in the Field of Information and Telecommunications in the Context of International Security” (GGE) to provide recommendations on State conduct in cyberspace is problematic (Taddeo 2017a). Over the past 20 years, the UN GGE, the Organization for Cyber Security and Co-operation in Europe (OSCE), and the ASEAN Regional Forum (ARF), and several national governments (G7 and G20) have been building consensus to define such a regime of norms. The time has come now to build on these initiatives and define binding norms for state actors in cyberspace. These norms will have to be enforced by an independent authority able to exert coercive power and impose sanctions. This authority cannot (and should not) be the result of a multi-stakeholder or a neutral, private-led initiative, as suggested for example by the proposal for a Digital Geneva Convention.Footnote 6 This would impose too heavy civil responsibilities on the private sector and create an authority too weak to face the political pressure resulting from ensuring State compliance to the regime of norms. Enforcing this regime requires an authority able to (i) ensure States compliance with the norms at an international level, (ii) run investigations into suspected State-run (or State-sponsored) cyber operations to define attribution, (iii) expose breaches of the norms, and (iv) impose adequate sanctions and punishments. These requirements define a political mandate for an authority that will have a deep impact on international relations and geo-political equilibriums. Points (i)–(iv) resonate with Article 26 of the UN Charter, which defines the mission of the Security Council: […] to promote the establishment and maintenance of international peace and security with the least diversion for armaments of the worlds human and economic resources, the Security Council shall be responsible for formulating, with the assistance of the Military Staff Committee […] plans for the establishment of a system for the regulation of armaments. Undeniably, the UN Security Council has the necessary resources, the political, and coercive power to achieve (i)–(iv). The time has come to embrace this power to consolidate and enforce an international regime of norms to regulate state behaviour in cyberspace. Problems, mistakes, and even failures are to be expected, but they must not hinder the process. This special issue has the goal of landscaping the debate on cyber deterrence and its role in fostering cyber stability. For this reason, it includes contributions focusing on strategies for cyber deterrence as well as articles addressing the ethical and regulatory aspects of state behaviour in cyberspace. More in detail, the first two articles focus on the strategic aspects of cyber deterrence. “Five Kinds of Cyber Deterrence” (Ryan 2017) sets the tone of the issue by mapping the main approaches to cyber deterrence provided in the extant literature. “The Limits of Deterrence Theory in Cyberspace” (Taddeo 2017b) identifies the limits of deterrence theory in cyberspace and define the conceptual space for a domain-specific theory of cyber deterrence. “Just War, Cyber War, and the Concept of Violence” (Finlay 2018) shifts the focus on normative aspects of cyber conflicts to consider whether cyber threats may justifiably be characterized as a form of “violence.” “Warfighting for Cyber Deterrence: a Strategic and Moral Imperative” (Lonsdale 2017) also offers a normative analysis of cyber conflicts and deterrence. “Why the World Needs an International Cyberwar Convention” (Eilstrup-Sangiovanni 2017) draws on existing normative regimes for the regulation on the use of weapons to argue for the feasibility of an international convention on the use of cyber weapons. “Deterrence in Cyberspace: a Silver Bullet or a Sacred Cow?” (Lawson 2017) concludes the special issue by considering the way in which different deterrence strategies could be implemented in cyberspace. Before leaving the reader to this special issue, I would like to express my sincere gratitude to the authors who contributed to it, as well as to the colleagues with whom I have had the opportunity of discussing several of the topics addressed in this issue, in particular Paul Cornish, Grahm Fairclough, and the members of the Digital Ethics Lab of the University of Oxford. I would also like to thank Luciano Floridi, the editor-in-chief of Philosphy & Technology, for his support during the preparation of this issue and throughout the process the led to defining many of the ideas presented in this introduction.";Deterrence and Norms to Foster Stability in Cyberspace;Taddeo, M.;2018;Taddeo, M.;Ethics and Philosophy of Information
Trust is a facilitator of interactions among the members of a system, whether these be human agents, artiﬁcial agents or a combination of both (a hybrid system). Elsewhere, I have argued that the occurrences of trust are related to, and affect, preexisting relations, like purchasing, negotiation, communication, and delegation (Taddeo 2010a,b). Trust is not to be considered a relation itself but a property of relations, something that changes the way relations occur. Consider, for example, a case of communication. Alice talks to Bob and she informs him that the grocery store down the road is closed for the day As Bob trusts Alice, he believes her and decides not to walk to the shop to double check, instead he starts searching for an alternative place to shop for his groceries. Between Alice and Bob there is a ﬁrst-order relation, the communication, which ranges over the two agents, and there is the second-order property of trust that ranges over the ﬁrst-order-relation and affects the way it occurs.;Trusting Digital Technologies Correctly;Taddeo, M.;2017;Taddeo, M.;Ethics and Philosophy of Information
"In April 2017, the foreign ministers of the G7 countries approved a ‘Declaration on Responsible States Behaviour in Cyberspace’ (G7 Declaration 2017). The Declaration addresses a mounting concern about international stability and the security of our societies after the fast-pace escalation of cyber attacks occurred during the past decade. In the opening statement, the G7 ministers stress their concern […] about the risk of escalation and retaliation in cyberspace […]. Such activities could have a destabilizing effect on international peace and security. We stress that the risk of interstate conflict as a result of ICT incidents has emerged as a pressing issue for consideration. […], (G7 Declaration 2017, 1). Paradoxically, state actors often play a central role in the escalation of cyber attacks. State-run cyber attacks have been launched for espionage and sabotage purposes since 2003. Well-known examples include Titan Rain (2003), the Russian attack against Estonia (2006) and Georgia (2008), Red October targeting mostly Russia and Eastern European Countries (2007), Stuxnet and Operation Olympic Game against Iran (2006–2012). In 2016, a new wave of state-run (or state-sponsored) cyber attacks ranged from the Russian cyber attack against Ukraine power plant, to the Chinese and Russian infiltrations US Federal Offices, to the Shamoon/Greenbag cyber-attacks on government infrastructures in Saudi Arabia. This trend will continue. The relatively low entry-cost and the high chances of success mean that states will keep developing, relying on, and deploying cyber attacks. At the same time, the ever more likely AI leap of cyber capabilities (Cath et al. 2017)—the use of AI and Machine Learning techniques for cyber offence and defence—indicates that cyber attacks will escalate in frequency, impact, and sophistication. Historically, escalation of interstate conflicts has been arrested using offensive or political strategies, sometimes in combination. Both have been deployed in cyberspace. The first failed; the second needs to be consolidated and enforced (Taddeo and Glorioso 2016a, b).";Deterrence by Norms to Stop Interstate Cyber Attacks;Taddeo, M.;2017;Taddeo, M.;Ethics and Philosophy of Information
When asked what his goal was for the 1815 Waterloo Campaign, the Duke of Wellington answered “Why, to beat the French” (Gray 1984, 9).  By French he meant Napoleon, and by beating him he meant defeating him for good, so that Napoleon could not pose a threat to European states any longer. A conflict was the most effective means to achieve this goal. Fast-forward two hundred years, now is China vs USA, the domain is cyberspace where China has been launching attacks against the USA for at least four years to acquire relevant information from USA companies and governmental offices.  The USA would like to stop the cyber-attacks, the best response in this case is not a conflict but a diplomatic move: the American and Chinese presidents meet and define bilateral agreements to stop state-run cyber-attacks between their two countries. This conflict was not won by either of the two actors, but solved by both of them.  The Waterloo example highlights that there is a relation between political power and conflict waging (Freedman 1998). Historically, the capability of a state actor to win a conflict has often been equated with its ability to gain or maintain political power. This equation can be read minimally – the capability to win a conflict is a necessary condition to gain or maintain power – or maximally – the capability to win a conflict is a sufficient condition to gain or maintain power. When considering cyber conflicts and the dynamics of cyberspace, this equation, even the minimalist reading, no longer holds true. There is, indeed, a strong relation between cyber conflicts and political power, but it is different from the one linking kinetic conflicts and political power.;Cyber Conflicts and Political Power in Information Societies;Taddeo, M.;2017;Taddeo, M.;Ethics and Philosophy of Information
"Data Philanthropy-the donation of data from private companies-is becoming increasing more popular, as corporations, like Genentech and Pfizer donate their data, and international organisations, like the UN, start to create the infrastructure to facilitate the sharing of corporate-owned data (Kirkpatrick 2013). However, competing tensions on data control and ownership (Kaisler et al. 2013; Andrejevic 2014; Kostkova et al. 2016), limited technical understanding, and the lack of adequate frameworks for coordination and governance (Mayer-Schönberger and Kenneth 2013; Vayena et al. 2015) pose serious obstacles to the attempts to share data among different actors, especially when these include private corporations. This was the case, for example, in 2014 during the Ebola crisis in West Africa, when gaining access to mobile network operators’ data on population movement would have facilitated tracking the spreading of the disease, but proved to be impossible, because of issues concerning commercial interests, users’ privacy, national security, as well as regulatory uncertainty. Understanding how to access these data and how to harness their value for the common good is one of the main challenges of this decade. Many governments are […] beginning to consider adopting the technologies needed for real-time analytics, to be sure […] the data that could help give them the additional agility needed to meet the challenges of governance in the 21st century is accumulating behind corporate firewalls. One of the most serious obstacles in meeting this challenge comes from the risks and sensitivities of maximizing the accessibility and use of personal data (Taddeo 2016). For, despite being anonymised and stripped of any reference that may link back to their subjects, once shared and aggregated data can lead to users re-identification. The possibility of re-identification is not new, but it has grown significantly with the chances to access and aggregate big data sets and with the refinement of analytics techniques (Kaye et al. 2012; de Montjoye et al. 2015). Re-identification and the subsequent breaching of individual privacy unveil a tension between individual rights and data philanthropy, which if left unaddressed risks hindering the latter. This tension requires careful consideration, lest it invites a zero-sum approach. This approach could prompt an overprotective and detrimental attitude of individuals, companies, and institutions. For individuals would easily prioritise the protection of their rights over the possible benefits of data philanthropy and restrain access to their data, and so would do private companies to secure the trust of their costumers and avoid legal problems. While regulators and research institutions may avoid fostering this practice to elude privacy risks for individuals, de facto crippling research, especially the one depending on biobanks (Gymrek et al. 2013) and medical registries with aggregated clinical data (Kaye 2012; Mascalzoni et al. 2014). The zero-sum approach would also impair data sharing for humanitarian or policy purposes (more on this presently). Data philanthropy is morally ambiguous (Taddeo 2016), as it can either foster social development, knowledge, and the flourishing of information societies or can help steering the design of current and future societies in the opposite direction. This is not to argue against data philanthropy. It is rather to emphasise that, although there is something morally desirable about it, data philanthropy poses serious ethical problems. Clearly, its moral ambiguity is not tantamount to moral neutrality. In that data philanthropy is more likely to foster morally good outcomes, like societal and individual welfare, scientific progress, and better governance, than the opposite. Yet, in itself data philanthropy is not sufficient to ensure morally good results. The moral ambiguity of data philanthropy, on the one side, and its moral desirability, on the other, unveil the infraethical nature of this phenomenon. Infraethics is a neologism introduced in (Floridi 2012) to refer to not-yet-ethical framework of implicit expectations, attitudes, and practices that can facilitate and promote moral decisions and actions (Floridi 2012, 738). According to the analysis proposed in (Floridi 2014), the information revolution has unveiled that morally good behaviour is the result of both moral values and an ethical infrastructure able to foster them. Much in the same way in which societies require a socio-political infrastructure to function and prosper, human interactions require an ethical infrastructure able to support the flourishing of moral actions. The elements constitutive of a given infraethics are not good in themselves, nor are they sufficient to determine morally good outcomes, but they are likely to facilitate morally good actions. Trust, respect, and loyalty offer good examples of infraethical principles. They are often described as moral principles, but they are better understood as elements of the infraethics of a given society, because they facilitate the achievement of the goal that the members of that society may have, irrespective of its moral value. Trust, respect, and loyalty, for example, are crucial for a happy marriage to prosper; at the same time, they are essential for criminal organisations to grow and consolidate their power (Gambetta 1998; Taddeo and Floridi 2011). The moral ambiguity of infraethics is resolved once it is combined with the right moral values. As Floridi stresses: the best pipes may improve the flow but do not improve the quality of the water, and water of the highest quality is wasted if the pipes are rusty or leaky. […] because an infraethics is not morally good in itself, but it is what is most likely to yield moral goodness if properly designed and combined with the right moral values (Floridi 2014, 193). The infraethics of mature information societies encompasses, among others, trust (Taddeo 2010a, b), security (Taddeo 2013, 2014), transparency (Turilli and Floridi 2009) and, as I argue, data philanthropy (Taddeo 2016). Data philanthropy has the potential to foster a host of morally good behaviours by extending our knowledge and understanding of the world, improving governance, and ultimately by favouring the development of open, pluralistic, and tolerant information societies. The increasing use of data to support scientific research (Kurtz et al. 2005), policy making, and humanitarian processes, see for example the use of social data to analyse teenagers’ attitude towards contraception in developing countries, and the managing of emergencies, as in the case of IBM donating its weather data to map the spreading of Zika virus, offer good examples of the case in point. The infraethical nature of data philanthropy unveils that the tension between data philanthropy and individual rights is operational, rather than structural. Thus, it can be solved once the right infrastructures and protocols are in place. A first step in this direction has been proposed by the UN Global Pulse, which envisages the creation of a data commons, where non-sensitive data can be shared after adequate anonymization and aggregation, and the establishing of a sentinel network, where companies can share more sensitive data behind firewalls. However, more work needs to be done in this direction, as the design of the right infrastructures and protocols depends on a better understanding of individual consent to access and use of their data; the design of auditing processes to minimise the chances for unethical consequences; the definition of individual, corporate, and institutional responsibilities to share/donate their data (Floridi and Taddeo 2016); and, ultimately, a refined understanding of the way in which individual rights are understood, harmonised, and fulfilled in mature information societies. As stressed by Vayen and Tasioulas “big data developments stimulate interactions […] that impact both the content of these rights and the ways in which they may be productively exercised”, (Vayena and Tasioulas 2016). Ethical analyses are necessary more than ever to understand and shape this impact and ensure that the value and the possibilities to improve private and public life brought about by data philanthropy are fully harnessed.";Data Philanthropy and Individual Rights;Taddeo, M.;2017;Taddeo, M.;Ethics and Philosophy of Information
In mature information societies, sharing data is increasingly recognized as a crucial means to foster their development. However, competing tensions on data control and ownership, limited technical understanding, and the lack of an adequate governance framework pose serious challenges to attempts to share data among different actors. Data philanthropy, understood as the donation of data from both individuals and private companies, has been proposed as means to meet these challenges. While at first sight data philanthropy may seem an uncontroversial phenomenon, a closer analysis reveals a bewildering network of problems. In this article, I analyse the role of data philanthropy in contemporary societies and the moral problems that it yields. I argue that the solution to these problems rests on the understanding of the infraethical nature of data philanthropy and on the design of an ethical framework encompassing the right infraethics and the right ethics. This is a framework able to address the changes brought about by the information revolution and to harness the opportunities that these pose for the prosperity of current and future information societies. This article is part of the themed issue ‘The ethical impact of data science’.;Data philanthropy and the design of the infraethics for information societies;Taddeo, M.;2016;Taddeo, M.;Ethics and Philosophy of Information
"Efforts to regulate cyber conflicts—and cyber-defence postures more generally—rose to prominence almost a decade ago, when the risks for national and international security and stability arising from the cyber domain became clear. As I argued elsewhere (Taddeo 2014), these efforts often rely on an analogy-based approach, according to which the regulatory problems concerning cyber conflicts are only apparent, insofar as these are not radically different from other forms of conflicts. Those endorsing this approach claim that the existing legal framework governing armed conflicts is sufficient to regulate the cyber battlefield. All that is needed is an in-depth analysis of such laws and an adequate interpretation of the phenomena. As Schmitt stresses. “a thick web of international law norms suffuses cyber-space. These norms both outlaw many malevolent cyber-operations and allow states to mount robust responses” (Schmitt 2013, 177). While the use of analogies to regulate cyber conflicts proved to be effective in the short term, and was necessary a decade ago to avoid the so-called digital Wild West; in the medium- and long-term, this approach poses serious risks to the stability of current and future information societies. For efforts based on analogies between kinetic and cyber conflicts often end with ad hoc solutions, fall short of political and ethical foresight, and overlook, and are limited by, the theoretical vacuum underlying them. A relation of mutual influence exists between the way in which conflicts are waged and the societies waging them (Taddeo and Glorioso 2016a, b). For this reason the analogy-based approach risks entrapping future information societies in the past, thereby missing the opportunity to address questions concerning the impact of this new form of conflict on our societies, on their values, the rights and security of their citizens, and on national and international politics. And not just that. If taken too far, analogies between kinetic and cyber conflicts become misleading and pose more problems than they can solve. This is, for example, the case of cyber deterrence. Estimates indicate that the cyber security market will grow to US$170 billion by 2020, posing the risk of a progressive militarisation of cyber domain ensuing a cyber arms race and competition for digital supremacy, and hence increasing the possibility of escalation (Markets and Markets 2015). In this scenario, cyber deterrence is crucial to maintain international security and foster stability. However, deploying deterrence strategies within the cyber domain is proving to be a serious challenge, due to the global reach, the anonymity, the distributed, and the interconnected nature of the domain (Chadwick and Howard 2009). In this case, relying on analogies with nuclear deterrence aggravates, rather than helping to meet, the challenge. Consider, for example, game-theory models for nuclear deterrence, such as the famous “brinkmanship” model (Powell 2008). This model relies on a clear identification of the attacker, and on demonstrating state’s capability of retaliating and commitment to retaliate (credibility) should the attacker not desist from his/her intent. While credibility and capability may not be problematic in the case of cyber deterrence, the identification of the source of the threat (attribution) in the cyber domain is one of the crucial hurdles to address. The difficulties in identifying the attacker make problematic the strategic assessment of pains and gains, and the understanding of the attacker’s strategies, payoffs, and preferences, making this model inadequate to define successful deterrence strategies in the cyber domain. This is not tantamount to saying that game theory models do not apply to the case of cyber deterrence; but it is indicative of the need to develop new domain-specific models able to account for the specificity of the cyber domain and of the conflicts to deter. This concerns (Taddeo 2012, 2014; Dipert 2013; Floridi and Taddeo 2014): The domain: ranging from the virtual to the physical; The agents and targets: involving artificial alongside human agents, as well as physical and virtual objects; and The level of violence: spanning from non-violent to potentially highly violent phenomena. These aspects of the cyber domain and of cyber conflicts are of great relevance for they are reshaping our understanding of key concepts such as harm, violence, target, combatants, weapons, attack, state sovereignty, and territoriality, and political power (Chadwick and Howard 2009; Cornish 2016). Clearly, understanding such conceptual changes—and identifying their impact on international relations and on military strategies—are preliminary and necessary steps to any attempt to identify legitimate strategies and to define the right policies, norms, and laws regulating cyber conflicts. For this reason, it is necessary to realise the limits of the analogy-based approach, and to move past it. As Betz and Stevens (2013) put it: “It is little wonder that we attempt to classify […] the unfamiliar present and unknowable future in terms of a more familiar past, but we should remain mindful of the limitations of analogical reasoning in cyber security”. Analogies can be powerful, for they inform the way in which we think and constraint ideas and reasoning within a conceptual space (Wittgenstein 2009). However, if  the conceptual space is not the right one, analogies become misleading and detrimental for any attempt to develop innovative and in-depth understanding of new phenomena, as in the case of cyber deterrence, and they should be abandoned altogether. When the conceptual space is the right one, analogies are at best a step on the Wittgenstein’s ladder and need to be disregarded once they have taken us to the next level of the analysis. This is the case of the analogies between kinetic and cyber conflicts. I believe that the time has come to develop a new framework to understand and regulate this cyber conflicts. This framework relies on three methodological pillars: political ontology, political constructionism, and data ethics. Let me explain. There is an ontological hiatus between the entities involved in cyber conflicts and those taken into consideration in kinetic ones (Taddeo 2014). An analysis of the nature of the environment, of the agents involved in conflicts, and of the political power exerted in the cyber domain is thus a necessary preliminary step to any attempt to regulate this new type of conflict. As Hay put, it “ontological assumptions (relating to the nature of the political reality that is the focus of our analytical attentions) are logically antecedent to the epistemological and methodological choices” (Hay 2011, 1–2). ‘Political constructionism’ is a neologism that refers to the design process of policies, norms, and laws. It builds on Floridi’s analysis of norms as artefacts. “designed according to a set of requirements, which are specified on the basis of available resources, in order to implement a set of desired functions, and all this in view of achieving some ultimate purpose […]”, (Floridi 2015, 1100). Data ethics builds on political ontology and provides the purpose required by political constructionism for the design of norms. Data ethics is the. “branch of ethics that studies and evaluates moral problems related to data (including generation, recording, curation, processing, dissemination, sharing and use), algorithms (including artificial intelligence, artificial agents, machine learning and robots) and corresponding practices (including responsible innovation, programming, hacking and professional codes), in order to formulate and support morally good solutions (e.g. right conducts or right values)”, (Floridi and Taddeo 2016, 3). As such, data ethics embraces the correct level of abstraction to identify the key ethical principles that should underpin the regulation of cyber conflicts (Taddeo and Glorioso 2016a), and which will contribute to shape open, pluralistic, and stable information societies. A conceptual framework built on these three pillars will deliver the adequate responses to address the threats arising from the cyber domain. The alternative is developing unsatisfactory, short-sighted approaches and facing the risk of a cyber backlash: a deceleration of the digitization process imposed by governments and international institutions to prevent this kind of conflicts to erode both in the trust in economy and in political institutions.";On the Risks of Relying on Analogies to Understand Cyber Conflicts;Taddeo, M.;2016;Taddeo, M.;Ethics and Philosophy of Information
"Philosophy and computing have often been related in the history of human culture. In the age of the information revolution, this relation has grown to define an entire area of philosophical enquiry. Over the past decades we have thought of this area as delineated by two Cartesian axes: conceptual and methodological. On the conceptual axis we find the deep philosophical questions concerning the concepts of information, computation, algorithms, cognition, intelligence, and language. On the methodological axis we find philosophical research developed through computer science methods. Formal methods and levels of abstraction are notable examples of these methodologies. However, something is missing from this picture. As the information revolution unveils its full potential and our societies grow into mature information societies, we come to realise that computing has changed our daily practises and, more than that, it has provided us with new lenses to understand our environment and with new means to shape it—consider for example the impact of machine learning, Big Data, data science, and virtual reality on scientific research. These changes prompt new philosophical enquiries, which do not belong to the conceptual or the methodological axes, rather they are located on a third Cartesian axis, the contextual one. Philosophical enquiries located along the contextual axis face new, pressing problems, concerning the way we produce scientific knowledge (philosophy of science), the nature of this knowledge (epistemology), and of the tools we use to produce it (philosophy of computing, philosophy of mathematics, and philosophy of statistics), as well as their ethical implications (ethics). It is with the contextual axis in the picture that the area of philosophy and computing is fully delineated and its importance becomes evident. Philosophical analyses change over time to reflect their context: the scientific knowledge on which societies rely and the principles, both ethical and cultural, that shape them. The value of philosophy rests in this dynamism. Philosophy remains meaningful insofar as it changes to address the needs of the social, cultural, and historical context to which it belongs. A meaningful philosophy is a philosophy of its time. Together the three Cartesian axes ground philosophy in the information age. They embed it in its time and, in doing so, they contribute to enhance the value of philosophical analyses. Since its foundation, Minds & Machines has been a leading—and to a large extent a pioneering—journal offering a venue for publication of multidisciplinary research on philosophy and computing positioned on the conceptual or methodological axes. These remain central areas of enquiry, especially now as new milestones have been achieved in cognitive science, artificial intelligence, and quantum computing. Nonetheless, I believe that the time has come to extend the scope of Minds & Machines to philosophical enquiries located along the contextual axis. This is the vision that will guide my work as editor-in-chief. It rests on the conviction that in this way the journal will be able to serve a growing community of scholars interested in furthering our understanding of the reality in which we live and, by doing in so, in contributing to shape a better one for current and future generations. To achieve this goal, Minds & Machines will continue to publish the best research focusing on philosophy and computing, broadly understood. We will welcome high-quality articles, irrespective of their philosophical and disciplinary background and will assess them only with respect to the rigor of their approach and methodology. Many factors contribute to the success of a scientific journal, but publishing high-quality research remains the crucial one. For this reason, I hope that the research community served by Minds & Machines will continue to find it an interesting and a valuable venue for publication. Before leaving the reader to the third issue of 2016, I wish to express my gratitude to the authors and the reviewers who contributed to it. I am also very grateful to Gregory Wheeler, the former editor-in-chief of Minds and Machines and to the members of the scientific and executive boards. Their quality and enthusiastic commitment to the journal will be essential for it to continue to grow. I wish to express my deepest appreciation to Ties Nijssen, publishing editor at Springer; his help over the past months has been extraordinary and is only matched by the insightful conversations we have had on the relevance of philosophy and on the ways in which scientific journals can enhance it.";Philosophy and Computing in Information Societies;Taddeo, M.;2016;Taddeo, M.;Ethics and Philosophy of Information
"The special issue collects a selection of papers presented during the Computer Ethics: Philosophical Enquiries (CEPE) 2013 conference. This is a series of conferences organized by the International Association for Ethics and Information Technology (INSEIT) (http://inseit.net/), a professional organization formed in 2001 and which gathers experts in information and computer ethics prompting interdisciplinary research and discussions on ethical problems related to design and deployment of information and communication technologies (ICTs). During the past two decades, CEPE conferences have been a focal point for the research concerning crucial topics (Buchanan 1999, 2011), such as privacy (Hildebrandt, Mireille 2008), online trust (Taddeo 2010; Taddeo and Floridi 2011), online identity (Ess 2012), value-sensitive design (Friedman and Peter H. Kahn, Alan Borning 2006), cyber-warfare (Floridi and Taddeo 2014; Taddeo, Mariarosaria 2014), along with education and professional ethics (Buchanan and D. Ocholla 2011). In this special issue, we present the reader with six articles dwelling upon ethical problems characterizing contemporary information societies: The Democratic Governance of Information Societies: A Critique to the Theory of Stakeholders, Semantic Web Regulatory Models: Why Ethics Matter, The Realignment of the Sources of the Law and their Meaning in an Information Society, Levels of Trust in the Context of Machine Ethics, Developing Automated Deceptions and the Impact on Trust, and Moral Deskilling and Upskilling in a New Machine Age: Reflections on the Ambiguous Future of Character. In addition, this issue also includes a commentary describing the Online Manifesto Initiative; more on this presently.";Information Societies, Ethical Enquiries;Taddeo, M. & Buchanan, E.;2015;Taddeo, M.;Ethics and Philosophy of Information
This paper investigates the ethical issues surrounding the concept of Internet neutrality focusing specifically on the correlation between neutrality and fairness. Moving from an analysis of the many available definitions of Internet neutrality and the heterogeneity of the Internet infrastructure, the common assumption that a neutral Internet is also a fair Internet is challenged. It is argued that a properly neutral Internet supports undesirable situations in which few users can exhaust the majority of the available resources or in which specific types of applications and services cannot be developed or properly deployed. The solution offered to these shortcomings is based on (1) an environmental approach to the Internet, (2) the four guiding principles of Floridi’s Information Ethics and (3) a principle called ‘Information Diversity’. The paper is divided into six sections. Section 1 briefly presents the debate concerning the concepts of network and Internet neutrality. Section 2 poses a general and unifying definition of Internet neutrality based on the critical assessment of several domain-specific approaches to the problem of neutrality. Section 3 is dedicated to the analysis of the relationship between Internet neutrality and the ethical principle of fairness. Section 4 introduces Floridi’s Information Ethics, the definition of Information Diversity and an analysis of how they can be used to address the limitations of Internet neutrality. Section 5 summarises the ethics of Internet neutrality and Information Diversity defining their relationship. Section 6 reviews the arguments presented in the paper clarifying the foundational role played by Information Diversity and Information Ethics in Internet policy-making activity.;Internet Neutrality: Ethical Issues in the Internet Environment;Turilli, M., Vaccaro, A. & Taddeo, M.;2012;Taddeo, M.;Ethics and Philosophy of Information
This paper introduces a multi-modal polymorphic type theory to model epistemic processes characterized by trust, defined as a second-order relation affecting the communication process between sources and a receiver. In this language, a set of senders is expressed by a modal prioritized context, whereas the receiver is formulated in terms of a contextually derived modal judgement. Introduction and elimination rules for modalities are based on the polymorphism of terms in the language. This leads to a multi-modal non-homogeneous version of a type theory, in which we show the embedding of the modal operators into standard group knowledge operators.;A modal type theory for formalizing trusted communications;Primiero, G. & Taddeo, M.;2012;Taddeo, M.;Ethics and Philosophy of Information
Much of the ethical debate on peer-to-peer (P2P) focuses on the use of this technology for unauthorized and illegal sharing of copyrighted materials. This fixation has led some to believe that P2P is an intrinsically unethical technology. However, P2P has a much wider significance than the unauthorized circulation of copyrighted material. Scholars therefore need to evaluate the global ethical effects of P2P architecture on society, as opposed to focusing on the specific implementations of this architecture. Several criteria have been proposed for the assessment of these implications, ranging from the effects of P2P on the technological progress of a society to its influence on the development of virtuous interactions. This article contributes to this debate by presenting a new approach, which distinguishes between local effects (content related) and systemic effects (communication modality related) of the use of P2P and focuses on the latter. Employing Floridi's information ethics, it considers whether or not P2P is an ethical technology, in a global sense.;Analyzing Peer-to-Peer Technology Using Information Ethics;Taddeo, M. & Vaccaro, A.;2011;Taddeo, M.;Ethics and Philosophy of Information
This paper contributes to the debate on online trust addressing the problem of whether an online environment satisfies the necessary conditions for the emergence of trust. The paper defends the thesis that online environments can foster trust, and it does so in three steps. Firstly, the arguments proposed by the detractors of online trust are presented and analysed. Secondly, it is argued that trust can emerge in uncertain and risky environments and that it is possible to trust online identities when they are diachronic and sufficient data are available to assess their reputation. Finally, a definition of trust as a second-order property of first-order relation is endorsed in order to present a new definition of online trust. According to such a definition, online trust is an occurrence of trust that specifically qualifies the relation of communication ongoing among individuals in digital environments. On the basis of this analysis, the paper concludes by arguing that online trust promotes the emergence of social behaviours rewarding honest and transparent communications.;The Case of Online Trust;Turilli, M., Vaccaro, A. & Taddeo, M.;2010;Taddeo, M.;Ethics and Philosophy of Information
This paper provides a new analysis of e-trust, trust occurring in digital contexts, among the artificial agents of a distributed artificial system. The analysis endorses a non-psychological approach and rests on a Kantian regulative ideal of a rational agent, able to choose the best option for itself, given a specific scenario and a goal to achieve. The paper first introduces e-trust describing its relevance for the contemporary society and then presents a new theoretical analysis of this phenomenon. The analysis first focuses on an agent’s trustworthiness, this one is presented as the necessary requirement for e-trust to occur. Then, a new definition of e-trust as a second-order-property of first-order relations is presented. It is shown that the second-order-property of e-trust has the effect of minimising an agent’s effort and commitment in the achievement of a given goal. On this basis, a method is provided for the objective assessment of the levels of e-trust occurring among the artificial agents of a distributed artificial system.;Modelling Trust in Artificial Agents, A First Step Toward the Analysis of e-Trust;Taddeo, M.;2010;Taddeo, M.;Ethics and Philosophy of Information
The paper provides a selective analysis of the main theories of trust and e-trust (that is, trust in digital environments) provided in the last twenty years, with the goal of preparing the ground for a new philosophical approach to solve the problems facing them. It is divided into two parts. The first part is functional toward the analysis of e-trust: it focuses on trust and its definition and foundation and describes the general background on which the analysis of e-trust rests. The second part focuses on e-trust, its foundation and ethical implications. The paper ends by synthesising the analysis of the two parts.;Defining Trust and E-Trust: From Old Theories to New Problems;Taddeo, M.;2009;Taddeo, M.;Ethics and Philosophy of Information
The NATO Cooperative Cyber Defence Centre of Excellence (NATO CCD COE) workshop on ‘Ethics and Policies for Cyber Warfare’ took place on 11–12 November 2014 at Magdalen College, Oxford. It brought together 10 distinguished experts from the United Kingdom, the United States, Germany, Spain, Italy, the Netherlands, Norway, and Estonia, gathering representatives from academia and from international organisations such as the European Union, the United Nations Institute of Disarmament Research, the Cyber Security Centre in Oxford, and Oxford University. The workshop was chaired by Dr Mariarosaria Taddeo and Captain Ludovica Glorioso and was the second of its kind organised by the NATO CCD COE.;NATO CCD COE Workshop on ‘Ethics and Policies for Cyber Warfare’ – A Report;Cath, C., Glorioso, L. & Taddeo, M.;2016;Taddeo, M.;Ethics and Philosophy of Information
Over the past couple of months, the practice of ad blocking has received heightened ethical scrutiny. If you’re unfamiliar with the term, “ad blocking” refers to software—usually web browser plug-ins, but increasingly mobile apps—that stop most ads from appearing when you use websites or apps that would otherwise show them. Arguments against ad blocking tend to focus on the potential economic harms. Because advertising is the dominant business model on the internet, if everyone used ad-blocking software then wouldn’t it all collapse? If you don’t see (or, in some cases, click on) ads, aren’t you getting the services you currently think of as “free”—actually for free? By using ad-blocking, aren’t you violating an agreement you have with online service providers to let them show you ads in exchange for their services? Isn’t ad blocking, as the industry magazine AdAge has called it, “robbery, plain and simple”? In response, defenders of ad blocking tend to counter with arguments that ads are often “annoying,” and that blocking them is a way to force advertising to get better. Besides, they say, users who block ads wouldn’t have bought the advertisers’ products anyway. Many users also object to having data about their browsing and other behavioral habits tracked by advertising companies. Some also choose to block ads in hopes of speeding up page load times or reducing their overall data usage. What I find remarkable is the way both sides of this debate seem to simply assume the large-scale capture and exploitation of human attention to be ethical and/or inevitable in the first place. This demonstrates how utterly we have all failed to understand the role of attention in the digital age—as well as the implications of spending most of our lives in an environment designed to compete for it.;Why It’s OK to Block Ads;Williams, J.;2015;Williams, J.;Ethics and Philosophy of Information
Accurate understanding and forecasting of traffic is a key contemporary problem for policymakers. Road networks are increasingly congested, yet traffic data is often expensive to obtain, making informed policy-making harder. This paper explores the extent to which traffic disruption can be estimated using features from the volunteered geographic information site OpenStreetMap (OSM). We use OSM features as predictors for linear regressions of counts of traffic disruptions and traffic volume at 6,500 points in the road network within 112 regions of Oxfordshire, UK. We show that more than half the variation in traffic volume and disruptions can be explained with OSM features alone, and use cross-validation and recursive feature elimination to evaluate the predictive power and importance of different land use categories. Finally, we show that using OSM’s granular point of interest data allows for better predictions than the broader categories typically used in studies of transportation and land use.;Estimating Traffic Disruption Patterns with Volunteered Geographic Information;Camargo, C., Bright, J. & McNeill, G. et al.;2020;Bright, J.;Digital Politics and Government
In recent years, local government has been undergoing changes which are strongly influenced by the growing digitization of governmental operations. In this paper, we expand on the concepts of Digital Era Governance and its successor, Essentially Digital Government, by introducing the concept of Algorithmic Bureaucracy, which looks at the impacts of artificial intelligence on the socio-technical nature of public administration. We report on a mixed-method study, which focused on how the growth of data science is changing the ways that local government works in the United Kingdom. Under Algorithmic Bureaucracy, the direct and indirect effects of public administrative changes on the level of social problem solving may become positive in two cases: 1) where through artificial intelligence and isocratic administration the explainability of algorithmic processes increases individual and staff competence, and 2) where algorithms take on some of the role of processing institutional and policy complexity much more effectively than humans.;Algorithmic Bureaucracy;Vogl, T., Seidelin, C. & Ganesh, B. et al.;2019;Bright, J.;Digital Politics and Government
When visualizing geospatial network data, it is possible to position nodes according to their geographic locations or to position nodes using standard network layout algorithms that ignore geographic location. Such data is increasingly common in interactive displays of Internet-connected sensor data, but network layouts that ignore geographic location data are rarely employed. We conduct a user experiment to compare the effects of geographic and force-directed network layouts on three common network tasks: locating a node, determining the path length between two nodes, and comparing the degree of two nodes. We found a geographic layout was superior for locating a node but inferior for determining the path length between two nodes. The two layouts performed similarly when participants compared the degree of two nodes. We also tested a relaxed- or pseudogeographic layout created with multidimensional scaling and found it performed as well or better than the pure geographic layout on all tasks but remained inferior to the force-directed layout for the path-length task. We suggest interactive displays of geospatial network data allow viewers to switch between geographic and force-directed layouts, although further research is needed to understand the extent to which viewers are able to choose the most appropriate layout for a given task.;Where'd it go? How Geographic and Force-directed Layouts Affect Network Task Performance;Hale, S., McNeill, G. & Bright, J.;2017;Bright, J.;Digital Politics and Government
"This paper addresses the implications of 'big data' on the smart city paradigm. In addition to grids of sensors to track traffic flows or monitor service delivery, urban governments around the world are starting to experiment with re-purposing stores of data collected by third parties: using mobile phone data to track movement or social media to identify failing services. The use of this type of data has considerable potential to both augment the existing smart city vision and to spread it out to small and medium sized cities that are unable to afford investment in sensor grids, creating what we call a "" lightweight "" version of the smart city. However, it also implies a number of problems which previously smart cities were less prone to. After defining the lightweight smart city this paper reviews these challenges, mainly in the area of interpretation biases, before offering pointers to potential remedies and solutions.";The Lightweight Smart City and Biases in Repurposed Big Data;Voigt, C. & Bright, J.;2016;Bright, J.;Digital Politics and Government
In recent years, local authorities in the UK have begun to adopt a variety of “smart” technological changes to enhance service delivery. These changes are having profound impacts on the structure of public administration. Focusing on the particular case of artificial intelligence, specifically autonomous agents and predictive analytics, a combination of desk research, a survey questionnaire, and interviews were used to better understand the extent and nature of these changes in local government. Findings suggest that local authorities are beginning to adopt smart technologies and that these technologies are having an unanticipated impact on how public administrators and computational algorithms become imbricated in the delivery of public services. This imbrication is described as algorithmic bureaucracy, and it provides a framework within which to explore how these technologies transform both the socio‐technical relationship between workers and their tools, as well as the ways that work is organized in the public sector.;Smart Technology and the Emergence of Algorithmic Bureaucracy: Artificial Intelligence in UK Local Authorities;Vogl, T., Seidelin, C. & Ganesh, B. et al.;2020;Bright, J.;Digital Politics and Government
This paper explores the extent to which different party systems in Europe effectively represent their citizens. We argue that many European countries suffer from a “representative deficit”, which occurs when a significant portion of citizens have to vote for a political party whose stated views are actually quite different from their own. We measure the extent of this deficit in different European countries using data from EU Profiler and euandi, two Voting Advice Applications which served millions of users during the EP elections in 2009 and 2014 respectively. We find wide variation in the extent to which political parties are accurately tuned in to the preferences of their voters, a variation which is not clearly linked to the number of political parties or the proportionality of the electoral system. We attempt to explain some of this variation, and explore the reasons why some party systems offer better representation than others.;The representative deficit in different European Party Systems: an analysis of the elections to the European Parliament 2009-2014;Bright, J. Garzia, J. & Lacey, J. et al.;2020;Bright, J.;Digital Politics and Government
Extremist exploitation of social media platforms is an important regulatory question for civil society, government, and the private sector. Extremists exploit social media for a range of reasons—from spreading hateful narratives and propaganda to financing, recruitment, and sharing operational information. Policy responses to this question fit under two headings, strategic communication and content moderation. At the center of both of these policy responses is a calculation about how best to limit audience exposure to extremist narratives and maintain the marginality of extremist views, while being conscious of rights to free expression and the appropriateness of restrictions on speech. This special issue on “Countering Extremists on Social Media: Challenges for Strategic Communication and Content Moderation” focuses on one form of strategic communication, countering violent extremism. In this editorial we discuss the background and effectiveness of this approach, and introduce five articles which develop multiple strands of research into responses and solutions to extremist exploitation of social media. We conclude by suggesting an agenda for future research on how multistakeholder initiatives to challenge extremist exploitation of social media are conceived, designed, and implemented, and the challenges these initiatives need to surmount.;Countering Extremists on Social Media: Challenges for Strategic Communication and Content Moderation;Ganesh, B. & Bright, J.;2020;Bright, J.;Digital Politics and Government
Accurate modelling of local population movement patterns is a core, contemporary concern for urban policymakers, affecting both the short-term deployment of public transport resources and the longer-term planning of transport infrastructure. Yet, while macro-level population movement models (such as the gravity and radiation models) are well developed, micro-level alternatives are in much shorter supply, with most macro-models known to perform poorly at smaller geographical scales. In this paper, we take a first step to remedy this deficit, by leveraging two novel datasets to analyse where and why macro-level models of human mobility break down. We show how freely available data from OpenStreetMap concerning land use composition of different areas around the county of Oxfordshire in the UK can be used to diagnose mobility models and understand the types of trips they over- and underestimate when compared with empirical volumes derived from aggregated, anonymous smartphone location data. We argue for new modelling strategies that move beyond rough heuristics such as distance and population towards a detailed, granular understanding of the opportunities presented in different regions.;Diagnosing the performance of human mobility models at small spatial scales using volunteered geographical information;Camargo, C., Bright, J. & Hale, S.;2019;Bright, J.;Digital Politics and Government
"“Peace” and (violent) “conflict” are often seen as conceptual mirror images of one another; peace is the absence of conflict, and conflict is the absence of peace. Given this conceptual interdependence, some scholars see that the study of war-making and the study of peacemaking are complementary—or even functionally identical—academic projects. Others, however, see that studies of violence and war-making are antithetic to studies of peace and peacemaking. The six contributions to this Journal of Global Security Studies forum explore these contrasting perspectives, with a view to assessing the “state of the discipline” of peace and conflict studies (and cognate disciplines, such as security studies). The introduction offers provocations for debate. The two contributions that follow consider connections and disconnections between the study of conflict and studies of postconflict peacebuilding and transitional justice, respectively. The next two contributions focus on areas of investigation that do not fit neatly into either the “peace” or “conflict” categories—gender and nonviolence—and the authors explore how studies of these topics might create bridges between scholarship on peace and studies of violent conflict. The concluding contribution argues that “mainstream” peace and conflict research has come to be dominated by positivist treatments of war and violence, and it draws attention to alternate approaches that have the potential to transform and ameliorate social relations.";Studying Peace and Studying Conflict: Complementary or Competing Projects?;Gledhill, J. & Bright, J.;2019;Bright, J.;Digital Politics and Government
One well-known characteristic of participatory websites is that the distribution of contribution levels is highly skewed: most people who make use of the service contribute only a little, but a small minority, often known as ‘power users’, contribute a lot. These power users are understudied in the literature on online democratic participation: this article aims to fill this gap. Based on a unique observational dataset of hundreds of thousands of users of an electronic petitioning platform, we show that having more free time is an important determinant of becoming a power user, as is having a positive initial experience with the site. We also show that power users are both more effective than regular users and that their interests differ substantially from ‘normal’ users: meaning that these small groups have a powerful (and distorting) influence on overall outcomes for the site.;Power users in online democracy: their origins and impact;Bright, J., Bermudez, S. & Pilet, J-B. et al.;2019;Bright, J.;Digital Politics and Government
"Content analysis of news stories is a cornerstone of the communication studies field. However, much research is conducted at the level of individual news articles, despite the fact that news events are frequently presented as “stories” by news outlets: chains of connected articles offering follow up reporting as new facts emerge or covering the same event from different angles. These stories are theoretically highly important; they also create measurement issues for general quantitative studies of news output. Yet, thus far, the field has lacked an efficient method for detecting groups of articles which form stories in a way that enables their analysis. In this work, we present a novel, automated method for identifying news stories from within a corpus of articles, which makes use of techniques drawn from the fields of information retrieval and network analysis. We demonstrate the application of the method to a corpus of almost 40,000 news articles, and show that it can effectively identify valid story chains within the corpus. We use the results to make observations about the prevalence and dynamics of stories within the UK news media, showing that more than 50% of news production takes place within the form of story chains.";Understanding News Story Chains using Information Retrieval and Network Clustering Techniques;Nicholls, T. & Bright, J.;2018;Bright, J.;Digital Politics and Government
Crowdsourced knowledge websites such as Wikipedia and OpenStreetMap are increasingly attracting a critical literature which has highlighted the fact that the contributor bases of these sites are often geodemographically biased: drawn from more affluent and better educated segments of the population. However, while bias in contributors is well known, we know less about whether this also results in a bias in outcomes on these websites: or whether the partial portion of the population which does make contributions also works to “fill in the blanks”, by adding knowledge about other less well-off neighbouring areas which have not attracted a contributor base. This article addresses the question of whether such “neighbourhood effects” exist in practice. It makes use of a novel dataset of alcohol license data in the UK to assess variation in the completeness of the volunteer geographic information site OpenStreetMap. The results support existing literature in showing that completeness is related to demographics: areas with higher levels of wealth and education typically exhibit higher levels of completeness. The article then makes a novel contribution by showing evidence of the existence of neighbourhood effects: poorer regions with more affluent neighbours typically having higher levels of completeness than poorer regions which are also surrounded by poorer neighbours. The results suggest that crowdsourced knowledge websites can aspire to a kind of completeness even whilst user bases remain partial and biased.;Geodemographic biases in crowdsourced knowledge websites: Do neighbours fill in the blanks?;Bright, J., de Sabbata, S. & Lee, S.;2018;Bright, J.;Digital Politics and Government
There is a growing interest in using OpenStreetMap [OSM] data in health research. We evaluate the usefulness of OSM data for researching the spatial availability of alcohol, a field which has been hampered by data access difficulties. We find OSM data is about 50% complete, which appears adequate for replicating findings from other studies using alcohol licensing data. Further, we show how OSM quality metrics can be used to select areas with more complete alcohol data. The ease of access and use may create opportunities for analysts and researchers seeking to understand broad patterns of alcohol availability.;OpenStreetMap data for alcohol research: Reliability assessment and quality indicators;Bright, J., de Sabbata, S. & Lee, S. et al.;2018;Bright, J.;Digital Politics and Government
Scholars in the field of peace and conflict studies have long worried that their discipline is divided – between studies of war and war making, and studies of peace and peacemaking. However, empirical research into the existence, extent, and nature of such a division is scarce. We remedy this by addressing two questions: 1) how is work in the field of peace and conflict studies distributed between its two nominal pillars: “peace” and (violent) “conflict”? and 2) to what extent is there communication and exchange between the two sets of studies? Making use of a unique combination of methods, we find that studies of violence hold a dominant position in the field, although there is also a sizable body of work that explores topics of peace, understood as conflict prevention and/or response. That said, we find limited evidence of intellectual exchange between studies of war/making and peace/making. We also find evidence of gendered, regional, and methodological divides. We argue that such schisms may be preventing scholars of peace and conflict from collectively realizing the founding ontological goal of their discipline, which was to understand the causes of war in order to contribute to an understanding of how conflict can be managed peacefully.;A Divided Discipline? Mapping Peace and Conflict Studies;Bright, J. & Gledhill, J.;2018;Bright, J.;Digital Politics and Government
This article is a systematic large-scale study of the reasons driving political fragmentation on social media. Making use of a comparative dataset of the Twitter discussion activities of 115 political groups in 26 countries, it shows that groups that are further apart in ideological terms interact less, and that groups that sit at the extremes of the ideological scale are particularly likely to have lower patterns of interaction. Indeed, exchanges between centrists who sit on different sides of the left–right divide are more likely than connections between centrists and extremists who are from the same ideological wing. In light of the results, theory about exposure to different ideological viewpoints online is enhanced.;Explaining the Emergence of Political Fragmentation on Social Media: The Role of Ideology and Extremism;Bright, J.;2018;Bright, J.;Digital Politics and Government
"The emergence of large stores of transactional data generated by increasing use of digital devices presents a huge opportunity for policymakers to improve their knowledge of the local environment and thus make more informed and better decisions. A research frontier is hence emerging which involves exploring the type of measures that can be drawn from data stores such as mobile phone logs, Internet searches and contributions to social media platforms and the extent to which these measures are accurate reflections of the wider population. This paper contributes to this research frontier, by exploring the extent to which local commuting patterns can be estimated from data drawn from Twitter. It makes three contributions in particular. First, it shows that heuristics applied to geolocated Twitter data offer a good proxy for local commuting patterns; one which outperforms the current best method for estimating these patterns (the radiation model). This finding is of particular significance because we make use of relatively coarse geolocation data (at the city level) and use simple heuristics based on frequency counts. Second, it investigates sources of error in the proxy measure, showing that the model performs better on short trips with higher volumes of commuters; it also looks at demographic biases but finds that, surprisingly, measurements are not significantly affected by the fact that the demographic makeup of Twitter users differs significantly from the population as a whole. Finally, it looks at potential ways of going beyond simple frequency heuristics by incorporating temporal information into models.";Estimating local commuting patterns from geolocated Twitter data;McNeill, G., Bright, J. & Hale, S.;2017;Bright, J.;Digital Politics and Government
This aim of this article is to explore the potential use of Wikipedia page view data for predicting electoral results. Responding to previous critiques of work using socially generated data to predict elections, which have argued that these predictions take place without any understanding of the mechanism which enables them, we first develop a theoretical model which highlights why people might seek information online at election time, and how this activity might relate to overall electoral outcomes, focussing especially on information seeking incentives related to swing voters and new parties. We test this model on a novel dataset drawn from a variety of countries in the 2009 and 2014 European Parliament elections. We show that while Wikipedia offers little insight into absolute vote outcomes, it does offer good information about changes in overall turnout at elections and about changes in vote share for particular parties. These results are used to enhance existing theories about the drivers of aggregate patterns in online information seeking, by suggesting that voters are cognitive misers who seek information only when considering changing their vote.;Wikipedia traffic data and electoral prediction: towards theoretically informed models;Yasseri, T. & Bright, J.;2016;Bright, J.;Digital Politics and Government
"This article seeks to explain variation in news sharing patterns on social media. It finds that news editors have considerable power to shape the social media agenda through the use of “story importance cues” but also shows that there are some areas of news reporting (such as those related to crime and disasters) where this power does not apply. This highlights the existence of a social “news gap” where social media filters out certain types of news, producing a social media news agenda which has important differences from its traditional counterpart. The discussion suggests that this may be consequential for perceptions of crime and engagement with politics; it might even stimulate a partial reversal of the tabloidization of news outlets.";The Social News Gap: How News Reading and News Sharing Diverge;Bright, J.;2016;Bright, J.;Digital Politics and Government
This article offers an empirically driven critical consideration of the idea of transnationalising Europe’s voting space, which would mean allowing European citizens to vote for a party from any member state at the European Parliament elections. We argue that such a move would reduce the second-order problem in European elections, as it would force political parties to move away from campaigning solely on national issues. We also claim that it would improve the extent to which Europeans are represented in their parliament and would be particularly welcomed by citizens currently dissatisfied with the state of their national democracy. We offer evidence to back up these claims, based on data on the political preferences of almost half a million Europeans and 274 European parties.;Europe’s voting space and the problem of second-order elections: A transnational proposal;Bright, J. Garzia, J. & Lacey, J. et al.;2015;Bright, J.;Digital Politics and Government
"This article provides an exploration of why security politics might change the behaviour of parliaments or legislatures, bringing together theory from diverse fields; Illustrates a series of potential ways of measuring legislative behaviour; Is a systematic quantitative test for the existence of security politics in a field which relies almost exclusively on qualitative methodology; Contributes to the current debate in the field of critical security studies over the definition of the politics of security. This article takes up the recent challenge to critical security studies posed by Browning and McDonald to define the effects of ‘the politics of security’. It focuses in particular on the behaviour of legislatures during the passage of legislation relating to crime and security. Effective scrutiny of this type of policy is crucial, but legislatures are often accused of failing to provide it. However, empirical work in the area remains limited: we know little about exactly how legislatures change their behaviour at such critical junctures. This article seeks to fill this gap. It offers firstly an exploration of diverse strands of work on the notion of ‘security politics’. Secondly, it offers an empirical test based on a dataset covering UK legislation from the period 2007–2012. The results suggest the appearance of security legislation causes parliament to heighten scrutiny, raising questions about the real nature of ‘security politics’.";In Search of the Politics of Security;Bright, J.;2014;Bright, J.;Digital Politics and Government
Massive open online courses (MOOCs) offer the possibility of entirely virtual learning environments, with lectures, discussions, and assignments all distributed via the internet. The virtual nature of MOOCs presents considerable advantages to students in terms of flexibility to learn what they want, when they want. Yet despite their virtual focus, some MOOC users also seek to create face-to-face communities with students taking similar courses or using similar platforms. This paper aims to assess the learner motivations behind creation of these offline communities. Do these face-to-face meetings represent an added extra to the learning experience, with students taking advantage of the context of the MOOCs to create new personal and professional connections? Or, are offline meetups filling a gap for students who feel that not all learning can take place online? We also assess the extent to which these patterns vary between developing and industrialised regions, thus testing the claim that MOOCs are helping to democratise access to education around the world. Our research is based on a unique source of socially generated big data, drawn from the website ‘meetup.com’, which gives us a data set of over 4000 MOOC related events taking place in over 140 countries around the world over a two year period. We apply a mixed methods approach to this data, combining large-scale analysis with more in-depth thematic hand coding, to more fully explore the reasons why some learners add a ‘real’ component to their virtual learning experience.;The real component of virtual learning: motivations for face-to-face MOOC meetings in developing and industrialised countries;Bulger, M., Bright, J. & Cobo, C.;2015;Bright, J.;Digital Politics and Government
Anecdotal evidence suggests social media are used by individuals and groups wanting to incite hatred and violence, yet the empirical evidence we present in this report suggests the these extreme forms of speech are actually marginal. Building on a collaboration between the University of Oxford and Addis Ababa University we examined thousands of comments made by Ethiopians on Facebook during four months around the time of the country’s general election. Hate speech – defined as statements to incite others to discriminate or act against individuals or groups on grounds of their ethnicity, nationality, religion or gender – was found in just 0.7% of overall statements in the representative sample. These findings may have wide implications for the many countries trying to address growing concerns about the role played by social media in promoting radicalisation or violence. Ethiopia represented an exceptional case study because of its distinct languages, which allowed gaining a realistic sample of the overall online debates focused on one country. We analysed Facebook statements made by Ethiopians, both in their homeland and abroad, in the run-up to and just after the general election on 24 May 2015. We found that fans or followers rather than people with any real influence online are mainly responsible for the violent or aggressive speech that appeared on Facebook pages in the sample. These individuals appear to use Facebook to vent their anger against more powerful sections of society. Around 18% of total comments in the sample were written by fans or followers compared with 11% of comments made by highly influential speakers (the owners of web pages). One fifth (21.8%) of hostile comments were grounded in political differences, only slightly higher than the overall average of 21.4% of all conversations containing hostile comments. Religion and ethnicity provoked fewer hostile comments (10% and 14% of overall comments in sample respectively). The findings are based on the analysis of more than 13,000 statements posted on 1,055 Facebook pages between February and June 2015. They mapped Facebook profiles, pages, and groups that had 100 or more followers or likes or members, respectively. All content in the sample studied had to include an Ethiopian language and raise discussion topics about Ethiopia. We focused on popular spaces on Facebook, analysing such pages daily to map ongoing trends, but also included comments on some online random pages or pages capturing particular events, such as a protest or publicised speeches. Posts, status updates and comments were tracked over time.;Mechachal: Online Debates and Elections in Ethiopia - From Hate Speech to Engagement in Social Media;Gagliardone, I., Pohjonen, M. & Beyene, Z. et al.;2016;Bright, J.;Digital Politics and Government
If elections were invented today, they would probably be referred to as “crowdsourcing the government.” First coined in a 2006 issue of Wired magazine (Howe, 2006), the term crowdsourcing has come to be applied loosely to a wide variety of situations where ideas, opinions, labor, or something else is “sourced” in from a potentially large group of people. While most commonly applied in business contexts, there is an increasing amount of buzz around applying crowdsourcing techniques in government and policy contexts as well (Brabham, 2013). Though there is nothing qualitatively new about involving more people in government and policy processes, digital technologies in principle make it possible to increase the quantity of such involvement dramatically, by lowering the costs of participation (Margetts, John, Hale, & Yasseri, 2015) and making it possible to tap into people's free time (Shirky, 2010). This difference in quantity is arguably great enough to obtain a quality of its own. We can thus be justified in using the term “crowdsourcing for public policy and government” to refer to new digitally enabled ways of involving people in any aspect of democratic politics and government, not replacing but rather augmenting more traditional participation routes such as elections and referendums. In this editorial, we will briefly highlight some of the key emerging issues in research on crowdsourcing for public policy and government. Our entry point into the discussion is a collection of research papers first presented at the Internet, Politics, and Policy 2014 (IPP2014) conference organized by the Oxford Internet Institute (University of Oxford) and the Policy & Internet journal.1 The theme of this very successful conference—our third since the founding of the journal—was “crowdsourcing for politics and policy.” Out of almost 80 papers presented at the conference in September last year, 14 of the best have now been published as peer‐reviewed articles in this journal, including five in this issue. A further handful of papers from the conference focusing on labor issues will be published in the next issue, but we can already now take stock of all the articles focusing on government, politics, and policy.;Crowdsourcing for Public Policy and Government;Lehdonvirta, V. & Bright, J.;2015;Bright, J.;"Digital Politics and Government; Digital Economies; Information Geography and Inequality"
Are holders of important ministerial positions more likely to survive in cabinet than their colleagues who hold less important positions? This study examines the relationship between the importance of a ministerial position and the length of time ministers are able to survive in government. It is based on an original dataset of cabinet ministers in seven West European countries from 1945 to 2011. Employing a little-used measure of ministerial survival based on overall time in government, it is found that holders of important ministerial positions are more durable than their colleagues who hold less important ministerial positions. Age, prior government experience and the size of the party to which the minister belongs are also associated with consistently significant effects. Further, the study explores the determinants of survival for two types of risk – exiting government with one’s party and exiting without it – showing that the effects of ministerial importance and other covariates are markedly different for these two types of exit. The findings have important implications for the understanding of ministerial and governmental stability.;Ministerial Importance and Survival in Government: Tough at the Top?;Bright, J., Döring, H. & Little, C.;2015;Bright, J.;Digital Politics and Government
"The rapid development of online media as a major location for news consumption has stimulated a variety of debates about how journalism is changing in the Internet era. Of particular importance have been worries about a potential turn toward populism, whereby journalists and editors shift away from reporting what is newsworthy to what their audience wants to hear supported by the widespread availability of audience metrics. A wealth of ethnographic research has pointed to the potential importance of such statistics; but little quantitative work has been conducted to test for the existence of a relationship between audience behavior and editorial decisions. This study seeks to fill that gap. Based on a novel data set of over 40,000 articles published in five major UK news outlets over a period of 6 weeks, we explore the relationship between a news story’s readership and its likelihood of being removed from the front page, based on the “most read” lists common to many news websites. We find that being a most read article decreased the short-term likelihood of being removed from the front page by around 25% and that this effect was broadly similar for both political and entertainment news. Surprisingly, we find a considerably greater influence in “quality” publications than their tabloid counterparts. Our results are discussed as evidence of a still limited, but potentially developing, turn toward online populism.";The Life and Death of Political News: Measuring the Impact of the Audience Agenda Using Online Data;Bright, J. & Nicholls, T.;2013;Bright, J.;Digital Politics and Government
Today, our more-than-ever digital lives leave significant footprints in cyberspace. Large scale collections of these socially generated footprints, often known as big data, could help us to re-investigate different aspects of our social collective behaviour in a quantitative framework. In this contribution we discuss one such possibility: the monitoring and predicting of popularity dynamics of candidates and parties through the analysis of socially generated data on the web during electoral campaigns. Such data offer considerable possibility for improving our awareness of popularity dynamics. However they also suffer from significant drawbacks in terms of representativeness and generalisability. In this paper we discuss potential ways around such problems, suggesting the nature of different political systems and contexts might lend differing levels of predictive power to certain types of data source. We offer an initial exploratory test of these ideas, focussing on two data streams, Wikipedia page views and Google search queries. On the basis of this data, we present popularity dynamics from real case examples of recent elections in three different countries.;Can electoral popularity be predicted using socially generated big data?;Yasseri, T. & Bright, J.;2014;Bright, J.;Digital Politics and Government
After a period of relative laissez faire, governments around the world are beginning to attempt to regulate online life, for a variety of reasons, through various mechanisms of surveillance and control. The drive to enforce the respect of copyright is at the forefront of these attempts, a highly controversial topic which pits proponents of the rights of the creative industry against advocates of freedom of speech. Apart from their inflammatory nature, one distinguishing characteristic of most of these schemes is that they are mediated: that is, they are conducted with the help of third parties, most often internet service providers. The mediation of surveillance is something as yet relatively underexplored by the field of surveillance studies, whose theoretical tools are by and large focussed on a two way relationship between watcher and watched. This article seeks to remedy this deficit, by examining the dynamics of mediation in the context of online copyright enforcement. We argue that, far from being a neutral process, the displacement of surveillance to third parties has a crucial impact on the way in which it is conducted. In particular, the expanding capacity of mediators becomes a reason for justifying surveillance in and of itself.;Mediating Surveillance: The Developing Landscape of European Online Copyright Enforcement;Bright, J. & Agustina, J.;2013;Bright, J.;Digital Politics and Government
Securitisations permit the breaking of rules: but which rules? This article argues that any given security situation could be handled by a variety of different ‘rule breaking’ procedures, and that securitisations themselves, whilst permitting rule breaking in general, do not necessarily specify in advance which rules in particular have to be broken. This begs the question: how do specific threats result in specific rule breaking measures? This article explores this question through reference to ‘control orders’, an unusual legal procedure developed in the UK during the course of the war on terrorism. Once applied to an individual, a control order gives the government a meticulous control over every aspect of their life, up to and including deciding on which educational qualifications they can take. Despite this control, individuals under the regime remain technically ‘free’: and have frequently used this freedom to abscond from the police who are supposed to be watching them. How did a security policy which controls a suspect's educational future, but not their physical movements, develop? This article aims to answer this question, and in so doing present a reevaluation of the mechanisms through which the effects of securitisation manifest themselves.;Securitisation, terror, and control: towards a theory of the breaking point;Bright, J.;2012;Bright, J.;Digital Politics and Government
If surveillance technologies are to be democratically controlled, then knowledge of these technologies is required. What do they do? How do they work? What are the costs? Yet gaining this knowledge in the context of a new surveillance technology such as biometrics can be problematic, because no settled definition exists. Competing versions of biometrics appear in both public and governmental discourse on the technology: different ideas about how often it fails, where it can be used and even what it does. This paper is an exploration of how these different versions compete with each other, and how knowledge about a new surveillance technology such as biometrics is thus constructed. Through reference to original research in the context of the use of biometrics in the UK, points of stability and instability in the definition of biometrics are identified, and some of the processes through which instable definitions become stable are tracked. From this empirical story, conclusions are drawn both for the process of construction of the meaning of technologies, and the general practice of surveillance in modern society. In particular, this paper aims to show how notions such as democratic control (central to the legitimation of state surveillance) become problematic when the very meaning of a technology is negotiable.;From this empirical story, conclusions are drawn both for the process of construction of the meaning of technologies, and the general practice of surveillance in modern society. In particular, this paper aims to show how notions such as democratic control (central to the legitimation of state surveillance) become problematic when the very meaning of a technology is negotiable.;Bright, J.;2011;Bright, J.;Digital Politics and Government
"The first major 2016 election contender US presidential with for a female one of election candidate the two was main as the a political parties. Gender attitudes played a sub-stantial role in the campaign as Hillary Clinton was both celebrated and reviled for her gender. Donald Trump was critiqued for his regressive remarks toward and about women. At the same time both candidates used social media to attract and connect with constituents. Donald Trump regularly received substantial press coverage for controversial statements made on Twitter and controversial statements made toward women. This essay examines the role of Twitter, misog-yny, and the rural voter in the 2016 presidential election. In Andi Zeisler’s (2016) New York Times essay, she highlights how “bitch” was used as “both an epithet and an honorific for Mrs. Clinton.” As much as this derogatory term has been used to silence women, it was also partially reclaimed by the Clinton campaign as her supporters implied that “Bitches get stuff done”(Zeisler 2016; Horowitz 2016). Donald Trump support-ers also applied the term to Hillary Clinton as they purchased merchandise and tweeted phrases like “Trump That Bitch,”“Lock the bitch up,”“take the bitch down,”“devilbitch,” and simply “bitch”(Bellstrom 2016; Rozsa 2016; Horowitz 2016). The official Trump campaign never overtly tweeted these phrases or produced any of the altright memes targeting the Clinton campaign.";MISOGYNY, TWITTER, AND THE RURAL VOTER;Stephens, M., Tong, L. & Hale, S. et al.;2018;Hale, S.;Digital Politics and Government
As people spend increasing proportions of their daily lives using social media, such as Twitter and Facebook, they are being invited to support myriad political causes by sharing, liking, endorsing, or downloading. Chain reactions caused by these tiny acts of participation form a growing part of collective action today, from neighborhood campaigns to global political movements. Political Turbulence reveals that, in fact, most attempts at collective action online don't succeed, but some give rise to huge mobilizations--even revolutions. Drawing on large-scale data generated from the Internet and real-world events, this book shows how mobilizations that succeed are unpredictable, unstable, and often unsustainable. To better understand this unruly new force in the political world, the authors use experiments that test how social media influence citizens deciding whether or not to participate. They show how different personality types react to these social influences and identify which types of people are willing to participate at an early stage in a mobilization when there are few supporters or signals of viability. The authors argue that pluralism is the model of democracy that is emerging in the social media age--not the ordered, organized vision of early pluralists, but a chaotic, turbulent form of politics. This book demonstrates how data science and experimentation with social data can provide a methodological toolkit for understanding, shaping, and perhaps even predicting this democratic turbulence.;Political Turbulence: How Social Media Shape Collective Action;Margetts, H., John, P. & Hale, S.;2015;Hale, S.;Digital Politics and Government
We examine whether biases exist in the British websites stored in the Internet Archive data. We find that the Internet Archive contains a surprisingly small subset, about 24%, of the web pages of the website used for our case study (the travel site, TripAdvisor). Furthermore, the subset of data we found in the Internet Archive appears to be biased toward large prominent web pages and is not a random sample of the web pages on the site. This bias could create serious problems for research using archived websites, and we discuss this issue at the end of the chapter.;Live versus archive: comparing a web archive to a population of web pages;Hale, S. & Blank, G.;2017;Hale, S.;"Digital Politics and Government; Information Geography and Inequality"
The World Wide Web is enormous and in constant flux, with more web content lost to time than is currently accessible via the live web. The growing body of archived web material available to researchers is poten- tially immensely valuable as a record of important aspects of modern society, but there have previously been few tools available to facilitate research using archived web materials (Dougherty and Meyer, 2014). Furthermore, based on the many talks we have given over the years to a variety of audiences, some researchers are not even aware of the exis- tence of web archives or their possible uses. However, with the develop- ment of new tools and techniques such as those used in this chapter and others in this volume, the use of web archives to understand the history of the web itself and shed light on broader changes in society is emerging as a promising research area (Dougherty et al., 2010). The web is likely to provide insight into social changes just as other historical artefacts, such as newspapers and books, have done for scholars interested in the pre-digital world. As the web becomes increasingly embedded in all spheres of everyday life and the number of web pages continues to grow, there is a compelling case to be made for examining changes in both the structure and content of the web. However, while interfaces such as the Wayback Machine1 allow access to individual web pages one at a time, there have been relatively few attempts to work with large collections of web archive data using computational approaches across the corpus. The research presented in this chapter used hyperlink data extracted from the Jisc UK Web Domain Dataset (Jisc, n.d.-a) covering the period from 1996 to 2010 to undertake a longitudinal analysis of the United Kingdom (UK) national web domain, .uk, focusing on the four largest second level domains: .co.uk, .org.uk, .gov.uk, and .ac.uk. We explore the growth of these domains, and examine the link density within and between them. Next we look in more detail at the academic second-level domain, .ac.uk, to understand the relationship between link density among UK academic institutions and measures of affiliation, status, performance and geographic distance. Overall, these results are used both to understand the growth and structure of the .uk domain, but also to demonstrate the benefits and challenges of this type of anal- ysis more generally.;Analysing the UK web domain and exploring 15 years of UK universities on the web: Using Web Archives to Understand the Past and the Present;Meyer, E., Yasseri, T. & Hale, S. et al.;2017;Hale, S.;"Digital Politics and Government; Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
Viz-Blocks is a simple browser-based UI for data exploration and document creation. It incorporates the Vega-Lite grammar of graphics for standard visualizations (including multiple views and interaction) whereas 'code blocks' provide the full power of the JavaScript ecosystem for creating custom visualizations and other bespoke content. Visualizations are treated as reusable 'blocks' that are easily created, modified and compared during exploration. When preparing results for dissemination, visualizations can be customized and combined with Markdown and image blocks to produce a single or multi-page HTML document that is easily styled and exported. Viz-blocks was designed in consultation with academics, students and policy makers to bridge the gap between visualization tools and more traditional document-authoring tools. The application is aimed at a wide audience: the lightweight, hybrid UI allows all users to access the core functionality, while experienced users can take advantage of code-blocks and the option to use advanced features of Vega-Lite via JSON/YAML snippets.;Viz-Blocks: Building Visualizations and Documents in the Browser;Graham, M. & Hale, S.;2019;Hale, S.;"Digital Politics and Government; Digital Economies; Information Geography and Inequality"
Online abusive content detection is an inherently difficult task. It has received considerable attention from academia, particularly within the computational linguistics community, and performance appears to have improved as the field has matured. However, considerable challenges and unaddressed frontiers remain, spanning technical, social and ethical dimensions. These issues constrain the performance, efficiency and generalizability of abusive content detection systems. In this article we delineate and clarify the main challenges and frontiers in the field, critically evaluate their implications and discuss potential solutions. We also highlight ways in which social scientific insights can advance research. We discuss the lack of support given to researchers working with abusive content and provide guidelines for ethical research.;Challenges and frontiers in abusive content detection;Vidgen, B., Harris, A. & Nguyen, D. et al.;2019;Hale, S.;Digital Politics and Government
The Internet has become a fundamental resource for activism as it facilitates political mobilization at a global scale. Petition platforms are a clear example of how thousands of people around the world can contribute to social change. Avaaz.org, with a presence in over 200 countries, is one of the most popular of this type. However, little research has focused on this platform, probably due to a lack of available data. In this work we retrieved more than 350K petitions, standardized their field values, and added new information using language detection and named-entity recognition. To motivate future research with this unique repository of global protest, we present a first exploration of the dataset. In particular, we examine how social media campaigning is related to the success of petitions, as well as some geographic and linguistic findings about the worldwide community of Avaaz.org. We conclude with example research questions that could be addressed with our dataset.;Online Petitioning Through Data Exploration and What We Found There: A Dataset of Petitions from Avaaz.org;Aragón, P., Saez-Trumper, D. & Redi, M. et al.;2018;Hale, S.;Digital Politics and Government
Tile maps are an important tool in thematic cartography with distinct qualities (and limitations) that distinguish them from better‐known techniques such as choropleths, cartograms and symbol maps. Specifically, tile maps display geographic regions as a grid of identical tiles so large regions do not dominate the viewer's attention and small regions are easily seen. Furthermore, complex data such as time series can be shown on each tile in a consistent format, and the grid layout facilitates comparisons across tiles. Whilst a small number of handcrafted tile maps have become popular, the time‐consuming process of creating new tile maps limits their wider use. To address this issue, we present an algorithm that generates a tile map of the specified type (e.g. square, hexagon, triangle) from raw shape data. Since the ‘best’ tile map depends on the specific geography visualized and the task to be performed, the algorithm generates and ranks multiple tile maps and allows the user to choose the most appropriate. The approach is demonstrated on a range of examples using a prototype browser‐based application.;Generating Tile Maps;McNeill, G. & Hale, S.;2017;Hale, S.;Digital Politics and Government
The number and quality of user reviews greatly affects consumer purchasing decisions. While reviews in all languages are increasing, it is still often the case (especially for non-English speakers) that there are only a few reviews in a person's first language. Using an online experiment, we examine the value that potential purchasers receive from interfaces showing additional reviews in a second language. The results paint a complicated picture with both positive and negative reactions to the inclusion of foreign-language reviews. Roughly 26-28% of subjects clicked to see translations of the foreign-language content when given the opportunity, and those who did so were more likely to select the product with foreign-language reviews than those who did not.;Foreign-language Reviews: Help or Hindrance?;Hale, S. & Eleta, I.;2017;Hale, S.;Digital Politics and Government
"The number of user reviews of tourist attractions, restaurants, mobile apps, etc. is increasing for all languages; yet, research is lacking on how reviews in multiple languages should be aggregated and displayed. Speakers of different languages may have consistently different experiences, e.g., different information available in different languages at tourist attractions or different user experiences with software due to internationalization/localization choices. This paper assesses the similarity in the ratings given by speakers of different languages to London tourist attractions on TripAdvisor. The correlations between different languages are generally high, but some language pairs are more correlated than others. The results question the common practice of computing average ratings from reviews in many languages.";User Reviews and Language: How Language Influences Ratings;Hale, S.;2016;Hale, S.;Digital Politics and Government
This article analyzes users who edit Wikipedia articles about Okinawa, Japan, in English and Japanese. It finds these users are among the most active and dedicated users in their primary languages, where they make many large, high-quality edits. However, when these users edit in their non-primary languages, they tend to make edits of a different type that are overall smaller in size and more often restricted to the narrow set of articles that exist in both languages. Design changes to motivate wider contributions from users in their non-primary languages and to encourage multilingual users to transfer more information across language divides are presented.;Cross-language Wikipedia Editing of Okinawa, Japan;Hale, S.;2015;Hale, S.;Digital Politics and Government
Wikipedia is one of the largest platforms based on the concept of asynchronous, distributed, collaborative work. A systematic collaborative exploration and assessment of Wikipedia content and coverage is however still largely missing. On the one hand editors routinely perform quality and coverage control of individual articles, while on the other hand academic research on Wikipedia is mostly focused on global issues, and only sporadically on local assessment. In this paper, we argue that collaborative visualizations have the potential to fill this gap, affording editors to collaboratively explore and analyse patterns in Wikipedia content, at different scales. We illustrate how a collaborative visualization service can be an effective tool for editors to create, edit, and discuss public visualizations of Wikipedia data. Combined with the large Wikipedia user-base, and its diverse local knowledge, this could result in a large-scale collection of evidence for critique and activism, and the potential to enhance the quantity and quality of Wikipedia content.;Collaborative Visualizations for Wikipedia Critique and Activism;de Sabbata, S., Çöltekin, A. & Eccles, K. et al.;2015;Hale, S.;"Digital Politics and Government; Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
"This paper presents a multilingual study on, per single post of microblog text, (a) how much can be said, (b) how much is written in terms of characters and bytes, and (c) how much is said in terms of information content in posts by different organizations in different languages. Focusing on three different languages (English, Chinese, and Japanese), this research analyses Weibo and Twitter accounts of major embassies and news agencies. We first establish our criterion for quantifying ""how much can be said"" in a digital text based on the openly available Universal Declaration of Human Rights and the translated subtitles from TED talks. These parallel corpora allow us to determine the number of characters and bits needed to represent the same content in different languages and character encodings. We then derive the amount of information that is actually contained in microblog posts authored by selected accounts on Weibo and Twitter. Our results confirm that languages with larger character sets such as Chinese and Japanese contain more information per character than English, but the actual information content contained within a microblog text varies depending on both the type of organization and the language of the post. We conclude with a discussion on the design implications of microblog text limits for different languages.";How much is said in a microblog?: A multilingual inquiry based on Weibo and Twitter;Liao, H-T., Fu, K-W. & Hale, S.;2015;Hale, S.;Digital Politics and Government
This article analyzes one month of edits to Wikipedia in order to examine the role of users editing multiple language editions (referred to as multilingual users). Such multilingual users may serve an important function in diffusing information across different language editions of the encyclopedia, and prior work has suggested this could reduce the level of self-focus bias in each edition. This study finds multilingual users are much more active than their single-edition (monolingual) counterparts. They are found in all language editions, but smaller-sized editions with fewer users have a higher percentage of multilingual users than larger-sized editions. About a quarter of multilingual users always edit the same articles in multiple languages, while just over 40% of multilingual users edit different articles in different languages. When non-English users do edit a second language edition, that edition is most frequently English. Nonetheless, several regional and linguistic cross-editing patterns are also present.;Multilinguals and Wikipedia editing;Hale, S.;2014;Hale, S.;Digital Politics and Government
This article analyzes the global connectivity of the Twitter retweet and mentions network and the role of multilingual users engaging with content in multiple languages. The network is heavily structured by language with most mentions and retweets directed to users writing in the same language. Users writing in multiple languages are more active, authoring more tweets than monolingual users. These multilingual users play an important bridging role in the global connectivity of the network. The mean level of insularity from speakers in each language does not correlate straightforwardly with the size of the user base as predicted by previous research. Finally, the English language does play more of a bridging role than other languages, but the role played collectively by multilingual users across different languages is the largest bridging force in the network.;Global connectivity and multilinguals in the Twitter network;Hale, S.;2014;Hale, S.;Digital Politics and Government
This research analyzes edits by foreign-language users in Wikipedia articles about Okinawa, Japan, in the Japanese and English editions of the encyclopedia. Okinawa, home to both English and Japanese speaking users, provides a good case to look at content differences and cross-language editing in a small geographic area on Wikipedia. Consistent with prior work, this research finds large differences in the representations of Okinawa in the content of the two editions. The number of users crossing the language boundary to edit both editions is also extremely small. When users do edit in a non-primary language, they most frequently edit articles that have cross-language (interwiki) links, articles that are edited more by other users, and articles that have more images. Finally, the possible value of edits from foreign-language users and design possibilities to motivate wider contributions from foreign-language users are discussed.;Okinawa in Japanese and English wikipedia;Hale, S.;2014;Hale, S.;Digital Politics and Government
Now that so much of collective action takes place online, web-generated data can further understanding of the mechanics of Internet-based mobilisation. This trace data offers social science researchers the potential for new forms of analysis, using real-time transactional data based on entire populations, rather than sample-based surveys of what people think they did or might do. This paper uses a 'big data' approach to track the growth of over 8,000 petitions to the UK Government on the No. 10 Downing Street website for two years, analysing the rate of growth per day and testing the hypothesis that the distribution of daily change will be leptokurtic (rather than normal) as previous research on agenda setting would suggest. This hypothesis is confirmed, suggesting that Internet-based mobilisation is characterized by tipping points (or punctuated equilibria) and explaining some of the volatility in online collective action. We find also that most successful petitions grow quickly and that the number of signatures a petition receives on its first day is a significant factor in explaining the overall number of signatures a petition receives during its lifetime. These findings have implications for the strategies of those initiating petitions and the design of web sites with the aim of maximising citizen engagement with policy issues.;Petition growth and success rates on the UK No. 10 Downing Street website;Hale, S., Margetts, H. & Yasseri, T.;2013;Hale, S.;Digital Politics and Government
This paper describes two case studies examining the impact of platform design on cross-language communications. The sharing of off-site hyperlinks between language editions of Wikipedia and between users on Twitter with different languages in their user descriptions are analyzed and compared in the context of the 2011 Tohoku earthquake and tsunami in Japan. The paper finds that a greater number of links are shared across languages on Twitter, while a higher percentage of links are shared between Wikipedia articles. The higher percentage of links being shared on Wikipedia is attributed to the persistence of links and the ability for users to link articles on the same topic together across languages.;Impact of platform design on cross-language information exchange;Hale, S.;2012;Hale, S.;Digital Politics and Government
Contour or boundary descriptors may be used in content-based image retrieval to effectively identify appropriate images when image content consists primarily of a single object of interest. The registration of object contours for the purposes of comparison is complicated when the objects of interest are characterized by open contours and when reliable feature points for contour alignment are absent. We present an application that employs an iterative approach to the alignment of open contours for the purposes of image retrieval and demonstrate its success in identifying individual bottlenose dolphins from the profiles of their dorsal fins.;Iterative 3-D Pose Correction and Content-Based Image Retrieval for Dorsal Fin Recognition;Hale, S.;2006;Hale, S.;Digital Politics and Government
"Due to the significant improvement of disaster-related information, further reduction of disaster risks re-quires not only governments to provide more scientifically accurate information but also the public to take appropriate action at the correct times. Meanwhile, there is ongoing work to integrate social media into Early Warning Systems (EWSs), but the ecology of social media information during crises remains poorly understood. This study seeks to understand public responses on social media to EWSs using the case of a 2015 typhoon (the Kanto-Tohoku Heavy Rain) in Japan. Using a corpus of 35 million tweets, computational methods such as topic modelling, and geospatial analysis we find that: 1) emergency warnings are likely to have people be more attentive to the warnings but this does not translate to an increased discussion of actions such as evacuation; 2) the expected shift of public attention (from awareness to preparation and then action) seems to happen on social media. Overall, we show that analysis of social media data can compliment tra-ditional survey-based approaches to understand how the public respond to information from Early Warning Systems.";Social media and early warning systems for natural disasters: A case study of Typhoon Etau in Japan;Kitazawa, K. & Hale, S.;2020;Hale, S.;Digital Politics and Government
Social media has become an emerging alternative to opinion polls for public opinion collection, while it is still posing many challenges as a passive data source, such as structurelessness, quantifiability, and representativeness. Social media data with geotags provide new opportunities to unveil the geographic locations of users expressing their opinions. This paper aims to answer two questions: 1) whether quantifiable measurement of public opinion can be obtained from social media and 2) whether it can produce better or complementary measures compared to opinion polls. This research proposes a novel approach to measure the relative opinion of Twitter users towards public issues in order to accommodate more complex opinion structures and take advantage of the geography pertaining to the public issues. To ensure that this new measure is technically feasible, a modeling framework is developed including building a training dataset by adopting a state-of-the-art approach and devising a new deep learning method called Opinion-Oriented Word Embedding. With a case study of tweets on the 2016 U.S. presidential election, we demonstrate the predictive superiority of our relative opinion approach and we show how it can aid visual analytics and support opinion predictions. Although the relative opinion measure is proved to be more robust than polling, our study also suggests that the former can advantageously complement the latter in opinion prediction.;Measuring relative opinion from location-based social media: A case study of the 2016 U.S. presidential election;Gong, Z., Cai, T. & Thill, J-C. et al.;2020;Hale, S.;Digital Politics and Government
"Word embeddings are increasingly used for the automatic detection of semantic change; yet, a robust evaluation and systematic comparison of the choices involved has been lacking. We propose a new evaluation framework for semantic change detection and find that (i) using the whole time series is preferable over only comparing between the first and last time points; (ii) independently trained and aligned embeddings perform better than continuously trained embeddings for long time periods; and (iii) that the reference point for comparison matters. We also present an analysis of the changes detected on a large Twitter dataset spanning 5.5 years.";Room to Glo: A Systematic Comparison of Semantic Change Detection Approaches with Word Embeddings;Shoemark, P., Liza, F. & Nguyen, D. et al.;2019;Hale, S.;Digital Politics and Government
Social media provide access to behavioural data at an unprecedented scale and granularity. However, using these data to understand phenomena in a broader population is difficult due to their non-representativeness and the bias of statistical inference tools towards dominant languages and groups. While demographic attribute inference could be used to mitigate such bias, current techniques are almost entirely monolingual and fail to work in a global environment. We address these challenges by combining multilingual demographic inference with post-stratification to create a more representative population sample. To learn demographic attributes, we create a new multimodal deep neural architecture for joint classification of age, gender, and organization-status of social media users that operates in 32 languages. This method substantially outperforms current state of the art while also reducing algorithmic bias. To correct for sampling biases, we propose fully interpretable multilevel regression methods that estimate inclusion probabilities from inferred joint population counts and ground-truth population counts. In a large experiment over multilingual heterogeneous European regions, we show that our demographic inference and bias correction together allow for more accurate estimates of populations and make a significant step towards representative social sensing in downstream applications with multilingual social media.;Demographic Inference and Representative Population Estimates from Multilingual Social Media Data;Wang, Z., Hale, S. & Adelani, D. et al.;2019;Hale, S.;Digital Politics and Government
"The digital revolution has affected museums in many ways, both directly and indirectly. A major external change is the rise of user‐written reviews; that is, reviews written by museum visitors and posted on the Internet. User‐generated reviews pose challenges to museums, as they are publicly available and largely outside the control of museums. This article discusses research on reviews of accredited museums in London. Our data set consists of all reviews written about 88 London museums that were posted on TripAdvisor during 2014, a total of 22,940 reviews. Using a technique called topic modelling, we describe 19 themes in reviewers’ stories of their visits. We find that museum visitors pay attention to the ancillary aspects of their visit: queues, cost, food service, toilets, and activities for children. They make fewer comments on the cultural side of the museum experience. However, these cultural aspects do matter and are associated with positive reviews. We argue that reviewers consider museums as part of a wider leisure sector. The article concludes with a discussion of the implications of our study for museum management and assess the usefulness of user‐generated content as a source of data on museum visitors.";TripAdvisor Reviews of London Museums: A New Approach to Understanding Visitors;Alexander, V., Blank, G. & Hale, S.;2019;Hale, S.;"Digital Politics and Government; Information Geography and Inequality"
Recent election surprises and regime changes have left the impression that politics has become more fast-moving and unstable. While modern politics does seem more volatile, there is little systematic evidence to support this claim. This paper seeks to address this gap in knowledge by reporting data over the last seventy years using public opinion polls and traditional media data from the UK and Germany. These countries are good cases to study because both have experienced considerable changes in electoral behaviour and have new political parties during the time period studied. We measure volatility in public opinion and in media coverage using approaches from information theory, tracking the change in word-use patterns across over 700,000 articles. Our preliminary analysis suggests an increase in the number of opinion issues over time and a growth in lack of predictability of the media series from the 1970s. ;Volatility in the Issue Attention Economy;Camargo, C., Hale, S. & John, P. et al.;2018;Hale, S.;Digital Politics and Government
Cultural organizations are categorized by cultural products (high or popular culture) and by organizational form (nonprofit or commercial). In sociology, these classifications are understood predominantly through a Bourdieusian lens, which links cultural consumption to habitus and a class-based struggle for distinction. However, people’s engagement with institutionalized cultural classifications may be expressed differently on the Internet, where a culture of hierarchy-free equality is (sometimes) idealized. Using digital trace data from a representative sample of 280 user-generated reviews of four London cultural organizations, we find that reviewers are concerned with practical issues over cultural content, displaying a popular orientation to cultural consumption (an “audience-focus” or an “embodied” approach). A very small minority of reviewers claim status honor on a variety of bases, including symbolic mastery of traditional cultural capital. Overall, we find an online space in the cultural sphere in which cultural hierarchies are not relevant.;Digital traces of distinction? Popular orientation and user-engagement with status hierarchies in TripAdvisor reviews of cultural organizations;Alexander, V., Blank, G. & Hale, S.;2018;Hale, S.;"Digital Politics and Government; Information Geography and Inequality"
Multilingualism is common offline, but we have a more limited understanding of the ways multilingualism is displayed online and the roles that multilinguals play in the spread of content between speakers of different languages. We take a computational approach to studying multilingualism using one of the largest user-generated content platforms, Wikipedia. We study multilingualism by collecting and analyzing a large dataset of the content written by multilingual editors of the English, German, and Spanish editions of Wikipedia. This dataset contains over two million paragraphs edited by over 15,000 multilingual users from July 8 to August 9, 2013. We analyze these multilingual editors in terms of their engagement, interests, and language proficiency in their primary and non-primary (secondary) languages and find that the English edition of Wikipedia displays different dynamics from the Spanish and German editions. Users primarily editing the Spanish and German editions make more complex edits than users who edit these editions as a second language. In contrast, users editing the English edition as a second language make edits that are just as complex as the edits by users who primarily edit the English edition. In this way, English serves a special role bringing together content written by multilinguals from many language editions. Nonetheless, language remains a formidable hurdle to the spread of content: we find evidence for a complexity barrier whereby editors are less likely to edit complex content in a second language. In addition, we find that multilinguals are less engaged and show lower levels of language proficiency in their second languages. We also examine the topical interests of multilingual editors and find that there is no significant difference between primary and non-primary editors in each language.;Understanding Editing Behaviors in Multilingual Wikipedia;Kim, S., Park, S. & Hale, S. et al.;2016;Hale, S.;Digital Politics and Government
The movements of ideas and content between locations and languages are unquestionably crucial concerns to researchers of the information age, and Twitter has emerged as a central, global platform on which hundreds of millions of people share knowledge and information. A variety of research has attempted to harvest locational and linguistic metadata from tweets to understand important questions related to the 300 million tweets that flow through the platform each day. Much of this work is carried out with only limited understandings of how best to work with the spatial and linguistic contexts in which the information was produced, however. Furthermore, standard, well-accepted practices have yet to emerge. As such, this article studies the reliability of key methods used to determine language and location of content in Twitter. It compares three automated language identification packages to Twitter's user interface language setting and to a human coding of languages to identify common sources of disagreement. The article also demonstrates that in many cases user-entered profile locations differ from the physical locations from which users are actually tweeting. As such, these open-ended, user-generated profile locations cannot be used as useful proxies for the physical locations from which information is published to Twitter.;Where in the World Are You? Geolocation and Language Identification in Twitter;Graham, M., Hale, S. & Gaffney, D.;2014;Hale, S.;"Digital Politics and Government; Digital Economies; Information Geography and Inequality"
Online social media has become an integral part of daily life for many Internet users and there are now hundreds of millions utilising these services around the world. Concomitant with this growth of usage is a desire by companies, government agencies, and academics to study and map the data trails left by people using services like Twitter, Flickr, and Facebook. The data shadows and information trails left by users online reveal social, economic, and political processes and practices. Twitter, in particular, is repeatedly used as a repository of social data because of its relatively open network that allows researchers access to almost any information published through the platform. Yet, despite the many studies (both inside and outside of academia) that draw on data from Twitter, there is little scholarship devoted to the geography of Twitter. As a first step, we decided to collect all georeferenced tweets sent between 5 March and 13 March 2012. It is important to point out that georeferenced tweets comprise fewer than 1% of all tweets and it is possible that significant geographic biases exist in where and how people georeference their content (Graham et al, 2012a). We then took a random 20% sample of that dataset: giving us approximately 4.5 million tweets that we spatially joined at the country level.;Featured Graphic. Mapping the Geoweb: A Geography of Twitter;Graham, M., Stephens, M. & Hale, S.;2013;Hale, S.;"Digital Politics and Government; Digital Economies; Information Geography and Inequality"
"Contemporary collective action, much of which involves social media and other Internet-based platforms, leaves a digital imprint which may be harvested to better understand the dynamics of mobilization. Petition signing is an example of collective action which has gained in popularity with rising use of social media and provides such data for the whole population of petition signatories for a given platform. This paper tracks the growth curves of all 20,000 petitions to the UK government over 18 months, analyzing the rate of growth and outreach mechanism. Previous research has suggested the importance of the first day to the ultimate success of a petition, but has not examined early growth within that day, made possible here through hourly resolution in the data. The analysis shows that the vast majority of petitions do not achieve any measure of success; over 99 percent fail to get the 10,000 signatures required for an official response and only 0.1 percent attain the 100,000 required for a parliamentary debate. We analyze the data through a multiplicative process model framework to explain the heterogeneous growth of signatures at the population level. We define and measure an average outreach factor for petitions and show that it decays very fast (reducing to 0.1% after 10 hours). After 24 hours, a petition's fate is virtually set. The findings seem to challenge conventional analyses of collective action from economics and political science, where the production function has been assumed to follow an S-shaped curve.";Modeling the Rise in Internet-based Petitions;Yasseri, T., Hale, S. & Margetts, H.;2014;Hale, S.;Digital Politics and Government
"For many people it is hard to imagine life without the Internet. It permeates daily life; it is the medium through which important communication happens (Castells, 2008); it provides the platform for content that augments our material world (Graham, 2010); and it is a vital source of information about almost every imaginable topic. However, it remains that only a minority of the population on our planet have access. This map was therefore created to better chart the global contours of access. This map [in colour online] illustrates the raw number of Internet users in each country as well as the percentage of the population with Internet access. It is created using 2008 statistics from the World Bank, which has tracked the number of Internet users per country and the number of Internet connections per 100 people since the 1990s as part of its Worldwide Governance Indicators project. The data are visualised with a cartogram in which the size of each country is drawn based on its proportion of global Internet users. The shading of each country reflects its Internet penetration rate darker shades indicate higher levels of Internet usage amongst the population. Countries with online populations of less than approximately 2 million have been removed from the map.";Featured Graphic: Digital Divide: The Geography of Internet Access;Graham, M., Hale, S. & Stephens, M.;2012;Hale, S.;"Digital Politics and Government; Digital Economies; Information Geography and Inequality"
"At least two software packages---DARWIN, Eckerd College, and FinScan, Texas A&M---exist to facilitate the identification of cetaceans---whales, dolphins, porpoises---based upon the naturally occurring features along the edges of their dorsal fins. Such identification is useful for biological studies of population, social interaction, migration, etc. The process whereby fin outlines are extracted in current fin-recognition software packages is manually intensive and represents a major user input bottleneck: it is both time consuming and visually fatiguing. This research aims to develop automated methods (employing unsupervised thresholding and morphological processing techniques) to extract cetacean dorsal fin outlines from digital photographs thereby reducing manual user input. Ideally, automatic outline generation will improve the overall user experience and improve the ability of the software to correctly identify cetaceans. Various transformations from color to gray space were examined to determine which produced a grayscale image in which a suitable threshold could be easily identified. To assist with unsupervised thresholding, a new metric was developed to evaluate the jaggedness of figures (""pixelarity"") in an image after thresholding. The metric indicates how cleanly a threshold segments background and foreground elements and hence provides a good measure of the quality of a given threshold. This research results in successful extractions in roughly 93% of images, and significantly reduces user-input time.";Unsupervised Threshold for Automatic Extraction of Dolphin Dorsal Fin Outlines from Digital Photographs in DARWIN (Digital Analysis and Recognition of Whale Images on a Network);Hale, S.;2012;Hale, S.;Digital Politics and Government
This research analyzes linguistic barriers and cross-lingual interaction through link analysis of more than 100,000 blogs discussing the 2010 Haitian earthquake in English, Spanish, and Japanese. In addition, cross-lingual hyperlinks are qualitatively coded. This study finds English-language blogs are significantly less likely to link cross-lingually than Spanish or Japanese blogs. However, bloggers' awareness of foreign language content increases over time. Personal blogs contain most cross-lingual links, and these links point to (primarily English-language) media. Finally, most cross-lingual links in the dataset signal a citation or reference relationship while a smaller number of cross-lingual links signal a translation. Although most bloggers link to other blogs in the same language, the dataset reveals a surprising level of human translation in the blogosphere.;Net Increase? Cross-Lingual Linking in the Blogosphere;Hale, S.;2012;Hale, S.;Digital Politics and Government
"We are aware that social media can be used as a potential source of data for research. It is used as a public platform for voicing opinions, sharing experiences and discussing public policy. Growing academic and commercial interest in social media analysis has stimulated theoretical and ethical debate. It has led to the development of new and innovative analytical methods designed to examine data. We commissioned this study to explore: how DWP might make use of these emerging analytical methods; the extent to which we could use social media data for social research purposes. We will consider the findings and recommendations in this report as we develop our analytical capabilities to increase our understanding about the effect our policies have on the public.";Use of social media for research and analysis;Bright, J., Margetts, H. & Hale, S. et al.;2014;Hale, S.;Digital Politics and Government
The sum of the world’s knowledge is always expanding, doubling every few years. While remarkable, this prompts important questions: Where is this knowledge being created? Who has access to it? How is it being distributed? And most importantly: How does our ability to access and produce codified knowledge change due to advances in information and communication technologies? American historian Elizabeth Eisenstein wrote about the advent of the printing press that a fifty-year- old person in the early 16th century would have seen more books being printed in her lifetime than in all of human history. The print revolution is not only a success story of the dramatic rise in the production of books. It happened in tandem with a revolution of access to knowledge in Europe. Prior to the invention of the printing press, the Catholic Church acted as an intermediary to knowledge as it controlled schools, scribes, and libraries. The instigators of the reformation, cognisant of the power of print, advocated for a more direct access of knowledge to people. Taken out of the hands of religious intermediaries, knowledge became more accessible – at least in Europe. Despite these fundamental shifts, the world’s codified knowledge remained concentrated among educated and affluent people in industrialized nations. This concentration spanned all phases in the cycle of information — from the production of knowledge to its distribution and access. Changing these patterns is inevitably hard, as access to knowledge is often a prerequisite to create new knowledge. Unsurprisingly, the geographies of knowledge remain largely characterized by strong core-periphery patterns. For example approximately one-third of commercial innovation in the US is concentrated in two small regions: Silicon Valley in California, and the Greater Boston area on the East Coast. These two regions house the world’s leading universities – established institutions to access, convey, and create knowledge. The Internet raised hopes that knowledge might become more accessible, reaching less affluent groups, as well as those farther away from the producers and sources of information. Many commentators speculated that people outside of industrialised nations would be able to gain access to all networked and codified knowledge, thus mitigating the world’s concentration of knowledge. These early expectations remain largely unrealised. Knowledge institutions, including producers and distributors, around the world remain concentrated. The reason may partially lie in the economic power of large knowledge institutions, and the existing structure of intellectual property rights. Together they prevent a more equitable access to (and equitable creation of) knowledge. As a result, many have once again pinned their hopes for change on technological innovation. A new generation of Internet tools have enabled not only global access to knowledge, but also the opportunity to establish new institutions for the sharing, peer-production and remixing of knowledge. The promise, in other words, is for a much more equitable production and distribution of the world’s knowledge. We approach this exciting and complex topic through ten visualisations – each one a snapshot highlighting a distinct dimension of the global distribution of knowledge. Visualisations empower us to quickly grasp complex dynamics and gain unexpected insights. We start with the preconditions to access knowledge in the digital age through literacy and Internet penetration. We continue by examining the established institutions of knowledge – from mass media to academic publishers, and discover concentrations of knowledge in different forms than might be commonly expected. Finally, we visualise peer-produced and user-generated knowledge – through new institutions of knowledge like Wikipedia that are founded on a need for broad participation. But even here we can observe significant concentrations of knowledge, leading to explosive questions. We offer these visualisations as a contribution to the important debate on the production of, and access to, the world’s knowledge. The information in this booklet has not yet been presented using the visualisations that we employ, and many of the datasets we use were collected specifically for this project. The result we hope is substantially more than the sum of its parts. We ultimately hope to show that although information can now be produced almost anywhere, we should never assume that information is produced everywhere. Much of the world remains, both literally and figuratively, absent from the global map of knowledge. ;Geographies of the World's Knowledge;Graham, M., Hale, S. & Stephens, M.;2011;Hale, S.;"Digital Politics and Government; Digital Economies; Information Geography and Inequality"
There are many reasons why passengers are unable or reluctant to use self-service e-gate systems. In order for designers to build better systems with higher uptake by end-users they need to have a more thorough understanding of the non-users. This paper investigates the reasons of non-use of Automated Border Control at European airports by applying Wyatt’s taxonomy and adding an “unawares„ category. It also presents possible solutions to turn current non-users into future users of e-gates.;Non-use of Automated Border Control Systems: Identifying Reasons and Solutions;Oostveen, A-M.;2014;Oostveen, A-M.;"Digital Politics and Government; Information Governance and Security"
This paper reports findings from participatory design research aimed at uncovering how technological interventions can engage users in the domain of privacy. Our work was undertaken in the context of a new design concept, “Privacy Trends”, whose aspiration is to foster technology users' digital literacy regarding ongoing privacy risks and elucidate how such risks fit within existing social, organizational, and political systems, leading to a longer‐term privacy concern. Our study reveals 2 challenges for privacy intervention design: the need to develop technology users' intrinsic motivations with the privacy domain and the importance of framing the concept of privacy within users' interests. Setting our study within a design context enables us to identify 4 design opportunities for fostering engagement with the privacy domain through technology design.;Understanding engagement with the privacy domain through design research;Vasalou, A., Oostveen, A-M. & Bowers, C. et al.;2014;Oostveen, A-M.;"Digital Politics and Government; Information Governance and Security"
Real-time location tracking of individuals has become relatively easy with the widespread availability of commercial wearable devices that use geographical positioning information to provide location-based services. One application of this technology is to allow parents to monitor the location of their children. This paper investigates child location tracking technology in the US and the UK and compares its privacy implications. Although overall the price levels and the technical capabilities are the same, we find that the features of the technology are different depending on the social context. This can be attributed to national regulations and law that shape how a technology can be used. These laws and regulations, influenced by cultural frameworks, values, and morality, differ considerably between the countries. Clarifying the expected impacts of technology on the lives of users and other stakeholders in terms of these contextual factors will help to inform public debate about technical possibilities and societal needs.;Child Location Tracking in the US and the UK: Same Technology, Different Social Implications;Oostveen, A-M., Vasalou, A. & van den Besselaar, P. et al.;2014;Oostveen, A-M.;"Digital Politics and Government; Information Governance and Security"
Outsourcing IT services is a common practice for many governments. This case study shows that outsourcing of elections is not without risk. Studying electronic voting in the Netherlands through documents obtained with Freedom of Information requests, we see that government agencies at both local and national level lacked the necessary knowledge and capability to identify appropriate voting technology, to develop and enforce proper security requirements, and to monitor performance. Furthermore, over the 20 years that e‐voting was used in the Netherlands, the public sector became so dependent on the private sector that a situation evolved where Dutch government lost ownership and control over both the e‐voting system and the election process.;Outsourcing Democracy: Losing Control of e‐Voting in the Netherlands;Oostveen, A-M.;2012;Oostveen, A-M.;"Digital Politics and Government; Information Governance and Security"
"In the Internet of Things (IoT), identification and access control technologies provide essential infrastructure to link data between a user's devices with unique identities, and provide seamless and linked up services. At the same time, profiling methods based on linked records can reveal unexpected details about users' identity and private life, which can conflict with privacy rights and lead to economic, social, and other forms of discriminatory treatment. A balance must be struck between identification and access control required for the IoT to function and user rights to privacy and identity. Striking this balance is not an easy task because of weaknesses in cybersecurity and anonymisation techniques. The EU General Data Protection Regulation (GDPR), set to come into force in May 2018, may provide essential guidance to achieve a fair balance between the interests of IoT providers and users. Through a review of academic and policy literature, this paper maps the inherent tension between privacy and identifiability in the IoT. It focuses on four challenges: (1) profiling, inference, and discrimination; (2) control and context-sensitive sharing of identity; (3) consent and uncertainty; and (4) honesty, trust, and transparency. The paper will then examine the extent to which several standards defined in the GDPR will provide meaningful protection for privacy and control over identity for users of IoT. The paper concludes that in order to minimise the privacy impact of the conflicts between data protection principles and identification in the IoT, GDPR standards urgently require further specification and implementation into the design and deployment of IoT technologies.";Normative challenges of identification in the Internet of Things: Privacy, profiling, discrimination, and the GDPR;Wachter, S.;2018;Wachter, S.;"Digital Politics and Government; Information Governance and Security"
Affinity profiling - grouping people according to their assumed interests rather than solely their personal traits - has become commonplace in the online advertising industry. Online platform providers use behavioural advertisement (OBA) and can infer very sensitive information (e.g. ethnicity, gender, sexual orientation, religious beliefs) about individuals to target or exclude certain groups from products and services, or to offer different prices. OBA and affinity profiling raise at least three distinct legal challenges: privacy, non-discrimination, and group level protection. Current regulatory frameworks may be ill-equipped to sufficiently protect against all three harms. I first examine several shortfalls of the General Data Protection Regulation (GDPR) concerning governance of sensitive inferences and profiling. I then show the gaps of EU non-discrimination law in relation to affinity profiling in terms of its areas of application (i.e. employment, welfare, goods and services) and the types of attributes and people it protects. I propose that applying the concept of ‘discrimination by association’ can help close some of these gaps in legal protection against OBA. This concept challenges the idea of strictly differentiating between assumed interests and personal traits when profiling people. Failing to acknowledge the potential relationship – be it directly or indirectly - between assumed interests and personal traits could render non-discrimination ineffective. Discrimination by association occurs when a person is treated significantly worse than others (e.g. not being shown an advertisement) based on their relationship or association (e.g. assumed gender or affinity) with a protected group. Crucially, the individual does not need to be a member of the protected group to receive protection. Protection does not hinge on whether the measure taken is based on a protected attribute that an individual actually possesses, or on their mere association with a protected group. Discrimination by association would help to overcome the argument that inferring one’s ‘affinity for’ and ‘membership in’ a protected group are strictly unrelated. Not needing to be a part of the protected group, as I will argue, also negates the need for people who are part of the protected group to ‘out’ themselves as members of the group (e.g. sexual orientation, religion) to receive protection, if they prefer. Finally, individuals who have been discriminated against but are not actually members of the protected group (e.g. people who have been misclassified as women) could also bring a claim. Even if these gaps are closed, challenges remain. The lack of transparent business models and practices could pose a considerable barrier to prove non-discrimination cases. Finally, inferential analytics and AI expand the circle of potential victims of undesirable treatment in this context by grouping people according to inferred or correlated similarities and characteristics. These new groups are not accounted for in data protection and non-discrimination law. I close with policy recommendations to address each of these legal challenges for OBA and affinity profiling.;Affinity Profiling and Discrimination by Association in Online Behavioural Advertising;Wachter, S.;2019;Wachter, S.;"Digital Politics and Government; Information Governance and Security"
Europe’s data protection laws must evolve to guard against pervasive inferential analytics in nascent digital technologies such as edge computing.;Data protection in the age of big data;Wachter, S.;2019;Wachter, S.;"Digital Politics and Government; Information Governance and Security"
The Internet of Things (IoT) requires pervasive collection and linkage of user data to provide personalised experiences based on potentially invasive inferences. Consistent identification of users and devices is necessary for this functionality, which poses risks to user privacy. The General Data Protection Regulation (GDPR) contains numerous provisions relevant to these risks, which may nonetheless be insufficient to ensure a fair balance between users’ and developers’ interests. A three-step transparency model is described based on known privacy risks of the IoT, the GDPR’s governing principles, and weaknesses in its relevant provisions. Eleven ethical guidelines are proposed for IoT developers and data controllers on how information about the functionality of the IoT should be shared with users above the GDPR’s legally binding requirements. Two use cases demonstrate how the guidelines apply in practice: IoT in public spaces and connected cities, and connected cars.;The GDPR and the Internet of Things: a three-step transparency model;Wachter, S.;2018;Wachter, S.;"Digital Politics and Government; Information Governance and Security"
"In this paper, we present selected results of a systematic study of different types of e-Research infrastructures. The paper is based on ongoing research to compare a range of e-Infrastructures of broad diversity focusing on: geographical diversity, representing efforts from around the globe; disciplinary diversity, including the natural sciences, social sciences and humanities; organizational diversity, for example, multi-institutional or federated; diverse levels of maturity, from those in the planning stage to those with a well-established user base; and diverse types of target user communities such as specialized niche, discipline-wide, or generic infrastructures. In presenting six initial cases, we discuss some general features that distinguish between different types of infrastructures across different fields of research. Previous analyses of e-Infrastructures have focused on the parallels between these infrastructures and the major infrastructures in society that support national populations. What our cases highlight instead is that e-Infrastructures consist of multiple types of overlapping and intersecting socio-technical configurations that serve quite diverse needs and groups of users. Indeed, the very term ‘infrastructures’ may be misleading insofar as it connotes support of whole communities of researchers on a large scale, which is currently still premature. The  paper derives implications of this heterogeneity for the future outlook on e-Infrastructures.";The Future of e-Research Infrastructures;Eccles, K., Schroeder, R. & Meyer, E. et al.;2009;Eccles, K.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
Although considerable effort and expense has gone into digitising content in the humanities, there is a lack of data about the actual uses and impacts these digitised resources are having on scholarship and on the public. In this paper, we report on a study completed in 2009 that examined five digitisation projects using a variety of measures to ascertain a nuanced picture of the usage and impact not only of those five projects, but also of digitised material in general. The paper focuses on the qualitative results obtained through interviews and focus groups to draw a picture of the ways in which digitised scholarly resources are having an impact. The paper concludes with the recommendations that the project made to JISC based on the research.;Digitisation as e-Research infrastructure: Access to materials and research capabilities in the Humanities;Meyer, E., Eccles, K. & McCarthy-Madsen, C.;2009;Eccles, K.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
The Everyday Sexism Project documents everyday examples of sexism reported by volunteer contributors from all around the world. It collected 100,000 entries in 13+ languages within the first 3 years of its existence. The content of reports in various languages submitted to Everyday Sexism is a valuable source of crowdsourced information with great potential for feminist and gender studies. In this paper, we take a computational approach to analyze the content of reports. We use topic-modeling techniques to extract emerging topics and concepts from the reports, and to map the semantic relations between those topics. The resulting picture closely resembles and adds to that arrived at through qualitative analysis, showing that this form of topic modeling could be useful for sifting through datasets that had not previously been subject to any analysis. More precisely, we come up with a map of topics for two different resolutions of our topic model and discuss the connection between the identified topics. In the low-resolution picture, for instance, we found Public space/Street, Online, Work related/Office, Transport, School, Media harassment, and Domestic abuse. Among these, the strongest connection is between Public space/Street harassment and Domestic abuse and sexism in personal relationships. The strength of the relationships between topics illustrates the fluid and ubiquitous nature of sexism, with no single experience being unrelated to another.;Topic Modeling of Everyday Sexism Project Entries;Meville, S., Eccles, K. & Yasseri, T.;2019;Eccles, K.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
The paper studies the transition to ICT-based support systems for scientific research. These systems currently attempt the transition from the project stage to the more permanent stage of an infrastructure. The transition leads to several challenges, including in the area of establishing adequate governance regimes, which not all projects master successfully. Studying a set of cases from Europe and America, we look at patterns in the size and scope of the undertakings, embeddedness in user communities, aims and responsibilities, mechanisms of coordination, forms of governance, and time horizon and funding. We find that, though configurations and landscapes are somewhat diverse, successful projects typically follow distinctive paths, either large-scale or small-scale, and become what we term ‘stable metaorganizations’ or ‘established communities.’;The Emerging Governance of E-Infrastructure;Barjak, F., Eccles, K. & Meyer, E. et al.;2013;Eccles, K.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
Webometric studies, using hyperlinks between websites as the basic data type, have been used to assess academic networks, the “impact factor” of academic communications and to analyse the impact of online digital libraries, and the impact of digital scholarly images. This study aims to be the first to use these methods to trace the impact, or success, of digitised scholarly resources in the Humanities. Running alongside a number of other methods of measuring impact online, the webometric study described here also aims to assess whether it is possible to measure a resource's impact using webometric analysis. Link data were collected for five target project sites and a range of comparator sites. The results show that digitised resources online can leave traces that can be identified and used to assess their impact. Where digitised resources are situated on shifting URLs, or amalgamated into larger online resources, their impact is difficult to measure with these methods, however. This study is the first to use webometric methods to probe the impact of digitised scholarly resources in the Humanities.;Measuring the web impact of digitised scholarly resources;Eccles, K., Thelwall, M. & Meyer, E.;2012;Eccles, K.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
"Researchers in the humanities adopt a wide variety of approaches to their research. Their work tends to focus on texts and images, but they use and also create a wide range of information resources, in print, manuscript and digital forms. Like other researchers, they face multiple demands on their time, and so they find the ease and speed of access to digital resources very attractive: some of them note that they are reluctant on occasion to consult texts that require a trip to a distant library or archive. Nevertheless, none of the participants in our study is yet ready to abandon print and manuscript resources in favour of digital ones. Rather, they engage with a range of resources and technologies, moving seamlessly between them. Such behaviours are likely to persist for some time. This is reflected also in how researchers disseminate their research. The overwhelmingly dominant channels are the long-established ones such as journal articles, conferences and workshops, monographs and book chapters. We found only limited use – except among philosophers - of blogs and other social media. We noted the doubts expressed in other fields about quality assurance for users of such media, but also concerns about how best to present material that will be read by non-academic audiences. A key change in humanities research over the past 10-15 years has been the growth of more formal and systematic collaboration between researchers. This is a response in part to new funding opportunities, but also to the possibilities opened up by new technology. Over recent years there has also been a shift from the model under which technology specialists tell researchers how to do their research to more constructive engagement. Like other researchers, scholars in the humanities use what works for them, finding technologies and resources that fit their research, and resisting any pressure to use something just because it is new. But there is little evidence as yet of their taking full advantage of the possibilities of more advanced tools for text-mining, grid or cloud computing, or the semantic web; and only limited uptake of even simple, freely-available tools for data management and sharing. Rather, they manage and store information on their desktops and laptops, and share it with others via email. Barriers to the adoption and take up of new technologies and services include lack of awareness and of institutional training and support, but also lack of standardization and inconsistencies in quality and functionality across different resources. These make for delays in research, repetitive searching, and limitations on researchers’ ability to draw connections and relationships between different resources.";Reinventing Research? Information Practices in the Humanities;Bulger, M., Meyer, E. & de la Flor, G. et al.;2011;Eccles, K.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
In 2015, in cooperation with ProQuest, Jisc commissioned this study of the Impacts of Digital Collections focused on two particular collections: Early English Books Online (EEBO) and House of Commons Parliamentary Papers (HCPP). These two collections are just a fraction of the number of collections that Jisc has purchased on behalf of its member institutions. While an understanding of these two collections is not necessarily generalizable to all digital collections (or even all Jisc-provided collections), they were selected because they are both relatively mature in the sense of having been available to users for over a decade, were thought to be well-known in the research community, and also appeal to users from multiple disciplines. The report includes 10 main highlights from the research: 1. The context of the use of digital resources is changing, but these changes are incremental and have a long development cycle prior to the realisation of impact. 2. The usage of both Early English Books Online and House of Commons Parliamentary Papers has been increasing steadily over the past decade. 3. While researchers at top universities are most likely to use EEBO and HCPP, less research-intensive HE institutions also benefit from both collections. 4. Researchers rely heavily on specific digital collections that they return to regularly, which is resulting in incremental changes in scholarly behaviour. 5. Resource use in the humanities is extremely diverse, and this makes providing access to needed resources and tools particularly challenging. 6. The citation evidence that is available shows a growing literature that mentions using EEBO or HCPP, and these publications in turn are reasonably well-cited. 7. The number and range of disciplines that refer to EEBO and HCPP is much more diverse than expected. 8. Researchers are more concerned with the content and functionality of the digital collections than in who provides the access. 9. The UK is unusual for providing national-level access across institutions through Jisc’s national purchasing. 10. Shifts to humanities data science and data-driven research are of growing interest to scholars, although there is still plenty of room for growth in this focus on digital humanities, particularly in teaching. The report concludes by arguing that digital collections have become fundamental to modern scholarship.;The Impacts of Digital Collections: Early English Books Online & House of Commons Parliamentary Papers;Meyer, E. & Eccles, K.;2016;Eccles, K.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
e-Infrastructures radically change the way research is conducted, overcoming distance to support a growing multitude of virtual research communities across the globe. The eResearch2020 consortium has conducted research on a diverse sample of e-Infrastructures from around the world, talking to both developers and users. The aim is to improve policy, enhance technology adoption and facilitate the creation of global virtual research communities. e-Infrastructures can be defined as networked tools, data and resources that support a community of researchers, broadly including all those who participate in and benefit from research. Following this definition, the term e-Infrastructure comprises very heterogeneous projects and institutions within the scientific community. e-Infrastructures include services as diverse as the physical supply of backbone connectivity, single- or multi-purpose grids, supercomputer infrastructure, data grids and repositories, tools for visualization, simulation, data management, storage, analysis and collection, tools for support in relation to methods or analysis, as well as remote access to research instruments and very large research facilities. ;The Role of e-Infrastructures in the Creation of Global Virtual Research Communities ;Meyer, E., Hüsing, T. & Robinson, S. et al.;2010;Eccles, K.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
Break-ups are a compelling topic for thinking about the networked society as they are both very common and run counter to the logic of connectivity that powers much of network society. The logic of connectivity is that connections between individuals or individuals and entities that are real should be accounted for. It is thought that the more such connections are accounted for, the more effectively any algorithm can facilitate future connections (Hogan), particularly for advertising purposes and enhanced user experience. A break-up signifies a dissolution of a link as two people change status from in a relationship to no longer in a relationship. A world of networks is a world of particulars and relationships between them. The particulars might be people, accounts, photos, or pieces of text. In social network analysis, everything contained by a particular boundary is a node. These nodes link to each other in some fashion. The set of nodes and relations define a network.;Break-Ups and the Limits of Encoding Love;Hogan, B.;2018;Hogan, B.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
Reddit’s men’s rights community (/r/MensRights) has been criticized for the promotion of misogynistic language, toxic masculinity and discourses that reinforce alt-right ideologies. Conversely, the men’s liberation (/r/MensLib) community integrates inclusive politics, intersectionality and masculinity within a broad umbrella of self-reflection that suggests toxic masculinity harms men as well as women. We use machine learning text classifiers, keyword frequencies, and qualitative approaches first to distinguish these two subreddits, and second to interpret the differences ideologically rather than topically. We further integrate platform metadata (referred to as ‘platform signals’) to distinguish the subreddits. These signals help us understand how similar terms can be used to arrive at different interpretations of gender and discrimination. Where /r/MensLib tends to see masculinity as an adjective and women as peers, /r/MensRights views being a man as an essential quality, men as the target of discrimination, and women as sources of personalized grievances.;Using Platform Signals for Distinguishing Discourses: The Case of Men’s Rights and Men’s Liberation on Reddit;LaViolette, J. & Hogan, B.;2019;Hogan, B.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
Social network site (SNS) platforms have the potential to be effective information-seeking channels due to their technical and social affordances, such as the ability to broadcast content to a large group and to aggregate one's contacts. This study tests the impact of a Facebook app that allows users to visualize their network of Facebook Friends to see how it influences who adolescents identify as good sources of information about college. Comparing Friends selected by 24 high school seniors before and after viewing Facebook network visualizations reveals that first-generation students were more likely to select higher quality information sources among their Facebook Friends after exposure to the visualization. Our results suggest that social media can help users identify good human information sources by making hidden resources in one's network more visible.;First-Generation Students and College: The Role of Facebook Networks as Information Sources;Jeon, G., Ellison, N. & Hogan, B. et al.;2016;Hogan, B.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
To what extent do friendship ties influence the conversation structure in open groups? Openly accessible Facebook groups offer the opportunity to examine how individuals leverage their existing friendship relations when speaking to a large and often heterogeneous audience. For example, those with many friends in the group may receive more positive signals from others and also may have their content validated more easily. Thus, while the group is ostensibly open to all, existing relationships may impede such openness on a practical level. We employ a stratified sample of 30 Facebook groups from UK Russell Group universities. Using multilevel regression, we examine the effects of several structural metrics at both the actor and group level on the magnitude of three conversational metrics: likelihood of initiating a conversation, responding to a conversation and receiving responses for content. We find that aspects of individual network positions, e.g. degree-centrality and eigenvector-centrality, as well as qualities of the group e.g. group-density and modularity, have a consistent and highly significant effect on conversational metrics, while the strength of these relationships clearly varies by group type. We contextualise our findings via Gibson’s notion of “conversational agency”, and point to future directions for designing and managing online communities.;Assessing the Structural Correlates between Friendship Networks and Conversational Agency in Facebook Groups;Polonski, V. & Hogan, B.;2015;Hogan, B.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
With video game use widely accepted and practiced in a wide variety of households worldwide, it is important for researchers to understand links between video game use and romantic relational experiences. Although unexplored within gaming literature, previous research has indicated the importance of attitudes of acceptance or approval within the couple relationship with acceptance of a partner's specific behavior being linked to relational outcomes. Using dyadic data from 6,756 couples (n = 13,512) from 16 different countries, an actor-partner interdependence moderating model was employed to evaluate how acceptance of video game use moderated the link between video game use and dyadic adjustment, while controlling for mental health, relational characteristics, and other demographic variables. Results indicate that higher reports of individual video game use were linked with improved rates of partner dyadic adjustment. Furthermore, results indicated that partner-interaction effects for acceptance of video game use moderated the relationships between video game use and dyadic adjustment. This supports the importance of considering contextual factors when examining gaming use and its links with other constructs.;Video Game Use, Acceptance, and Relationship Experiences: A Moderated Actor-Partner Interdependence Model;Norton, A., Brown, C. & Falbo, R. et al.;2020;Hogan, B.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
"This paper follows a research project meant to focus on understanding how universities and public research organisations (PRO) collaborate with firms in order to innovate and develop products or services using the same technology, an invention called Controlled Radical Polymerisation (CRP). We say ‘meant to’ quite deliberately as this project was plagued with a range of data collection issues. We aim to give the reader a sense of the complications our research team faced as we share and highlight the issues we did not properly consider in the process of collecting our data, and the strategies we implemented to save the project and collect useful data. These issues included the benefits of the choice of a face-to-face methodology, using our own networks to recruit participants, the underestimated influence of secrecy when collaborating with industry on an innovation project, the value of having a “local champion” and having specific procedures in place for operational matters; but most of all the importance of being flexible and agile. We share the lessons we learned during this journey. Importantly, we seek to give some practical advice and a sense of the reality of data collection – a reality which is often smoothed over and written up as unproblematic in academic publications.";Avoiding GIGO: Learnings from data collection in innovation research;Roden, B., Lusher, D. & Spurling, T. et al.;2020;Hogan, B.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
"How predictable are life trajectories? We investigated this question with a scientific mass collaboration using the common task method; 160 teams built predictive models for six life outcomes using data from the Fragile Families and Child Wellbeing Study, a high-quality birth cohort study. Despite using a rich dataset and applying machine-learning methods optimized for prediction, the best predictions were not very accurate and were only slightly better than those from a simple benchmark model. Within each outcome, prediction error was strongly associated with the family being predicted and weakly associated with the technique used to generate the prediction. Overall, these results suggest practical limits to the predictability of life outcomes in some settings and illustrate the value of mass collaborations in the social sciences.";Measuring the predictability of life outcomes with a scientific mass collaboration;Salganik, M., Lundberg, I. & Kindel, A. et al.;2020;Hogan, B.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
This paper examines the stability of egocentric networks as reported over time using a novel touchscreen-based participant-aided sociogram. Past work has noted the instability of nominated network alters, with a large proportion leaving and reappearing between interview observations. To explain this instability of networks over time, researchers often look to structural embeddedness, namely the notion that alters are connected to other alters within egocentric networks. Recent research has also asked whether the interview situation itself may play a role in conditioning respondents to what might be the appropriate size and shape of a social network, and thereby which alters ought to be nominated or not. We report on change in these networks across three waves and assess whether this change appears to be the result of natural churn in the network or whether changes might be the result of factors in the interview itself, particularly anchoring and motivated underreporting. Our results indicate little change in average network size across waves, particularly for indirect tie nominations. Slight, significant changes were noted between waves one and two particularly among those with the largest networks. Almost no significant differences were observed between waves two and three, either in terms of network size, composition, or density. Data come from three waves of a Chicago-based panel study of young men who have sex with men.;Assessing the stability of egocentric networks over time using the digital participant-aided sociogram tool Network Canvas;Hogan, B., Janulis, P. & Phillips, G. et al.;2019;Hogan, B.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
"This paper focuses on the use of mixed method social network analysis to understand how people’s conversations might influence their energy practices and attitudes to energy conservation. Eighty-five qualitative interviews were conducted with individuals living in six different communities across the United Kingdom. Our analysis sheds new light on who people discuss energy issues with; the social contexts where energy is discussed; and some of the factors that ‘open up’ or ‘close down’ energy conversations. We compare the influence of low and zero carbon technologies, and other interventions, on people’s energy conversations, and examine how perceived stigmas about discussing energy can be interpreted as ‘policing’ which can, in turn, inhibit further conversations about energy. We discuss the role that community-based organisations and other non-governmental agencies could play in potentially ‘normalising’ energy conversations, with the aspiration that such normalisation may influence the adoption of low and zero carbon practices.";Conversations about conservation? Using social network analysis to understand energy practices;Hamilton, J., Hogan, B. & Lucas, K. et al.;2019;Hogan, B.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
Lesbian, gay, bisexual, transgender, and queer (LGBTQ) youth and young adults almost inevitably “come out”, or self-disclose their identity to others. Some LGBTQ youth are more uniformly “out”, while others may disclose to some groups but not others. This selective disclosure is complicated on real name social media sites, which tend to encourage a unified presentation of self across social contexts. We explore these complications with a cohort of LBGTQ youth on Facebook (N = 199, Mage = 24.13). Herein we ask: How do LBGTQ youth manage the disclosure of their sexual orientation and/or gender identity to different people in their lives? Further, are there identifiable differences in the online social network structure for LGBTQ youth who manage outness in different ways? Finally, how do LGBTQ young people describe their experiences on Facebook? We answer these questions using a mixed methods approach, combining statistical cluster analysis, network visualization, and qualitative data. Our findings illustrate patterns in network structure by outness cluster type, highlighting both the work involved in managing one’s online identity as well as the costs to (semi-) closeted individuals including a considerably lower overall network connectivity. In particular, outness to family characterized LGBTQ young people’s experiences on Facebook.;“Everybody Puts Their Whole Life on Facebook”: Identity Management and the Online Social Networks of LGBTQ Youth;McConnell, E., Néray, B. & Hogan, B. et al.;2018;Hogan, B.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
"The early aughts saw an explosion of interest in social network sites. Many such sites, including Facebook, LinkedIn, and Twitter expanded to become ""platforms,"" meaning they are both websites and distributors of data. These data would typically be distributed to third parties in privacy-sensitive ways and regulated by the platform. Such access is based on balancing three issues: user privacy, generativity (i.e., capacity for novelty) for third parties, and control for platforms. Platforms appear to be seeking progressively more control at the cost of generativity by severely restricting third-party access to data about the user's friends. The reductions or outright lack of access means that the insights from our digital traces are no longer as knowable to either third parties or users. This article unpacks this shift by clarifying some of the technical issues involved (particularly APIs, the main means of external data access). The case study of social network visualization is used to exemplify how social network sites seek control at the expense of generativity. The article notes how this shift was done with little oversight.";Social Media Giveth, Social Media Taketh Away: Facebook, Friendships, and APIs;Hogan, B.;2016;Hogan, B.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
This study examined the impact of technology on couples in committed relationships through the lens of the couple and technology framework. Specifically, we used data from 6,756 European couples to examine associations between online boundary crossing, online intrusion, relationship satisfaction, and partner responsiveness. The results suggest that participants’ reports of online boundary crossing were linked with lower relationship satisfaction and partner responsiveness. Also, lower relationship satisfaction and partner responsiveness were associated with increased online boundary crossing. The results suggest that men, but not women, who reported greater acceptability for online boundary crossing were more likely to have partners who reported lower relationship satisfaction in their relationships. Implications for clinicians, relationship educators, and researchers are discussed.;Computer‐Mediated Communication in Intimate Relationships: Associations of Boundary Crossing, Intrusion, Relationship Satisfaction, and Partner Responsiveness;Norton, A., Baptist, J. & Hogan, B.;2017;Hogan, B.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
"While much social network data exists online, key network metrics for high-risk populations must still be captured through self-report. This practice has suffered from numerous limitations in workflow and response burden. However, advances in technology, network drawing libraries and databases are making interactive network drawing increasingly feasible. We describe the translation of an analog-based technique for capturing personal networks into a digital framework termed netCanvas that addresses many existing shortcomings such as: 1) complex data entry; 2) extensive interviewer intervention and field setup; 3) difficulties in data reuse; and 4) a lack of dynamic visualizations. We test this implementation within a health behavior study of a high-risk and difficult-to-reach population. We provide a within--subjects comparison between paper and touchscreens. We assert that touchscreen-based social network capture is now a viable alternative for highly sensitive data and social network data entry tasks.";Evaluating the Paper-to-Screen Translation of Participant-Aided Sociograms with High-Risk Participants;Hogan, B., Melville, J. & Phillips, G. et al.;2016;Hogan, B.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
The accelerated development of Information and Communications Technology has had a profound impact on the education system, both online and offline. In a South East European country such as Serbia, new technologies shape information, communication, and collaboration dynamics while contributing to a persistent digital divide regarding the skills necessary to obtain, process, evaluate, and communicate information. In this article, we explore digital divides between students and teachers in higher education with a focus on tensions surrounding digital literacy and collaboration. We apply Weber’s theory of stratification to an empirical case study of the digital divide in higher education in Serbia. We draw upon international indicators, secondary statistical sources, and primary semi-structured interviews with students and teachers in higher education. Through this analysis, we illustrate how forms of stratification intervene when trying to integrate new technologies and technology-oriented practices into the classroom. We consider stratification in terms of the traditional digital divide of access as well as stratification along lines of status, politics, and motivations. We interpret educators’ reluctance to adopt new technology as a reaction to the technology’s capacity to challenge the educators’ legitimacy, expertise, and preferred teaching materials. Students compound this situation with both greater familiarity and yet less focus on source credibility.;Overcoming digital divides in higher education: Digital literacy beyond Facebook;Radovanović, D., Hogan, B. & Lalić, D.;2015;Hogan, B.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
There are now more than 3 billion Internet users on our planet. The connections afforded to all of those people, in theory, allow for an unprecedented amount of communication and public participation. The goal of this article is to examine how those potentials match up to actual patterns of participation. By focusing on Wikipedia, the world's largest and most used repository of user-generated content, we are able to gain important insights into the geographies of voice and participation. This article shows that the relative democratization of the Internet has not brought about a concurrent democratization of voice and participation. Despite the fact that it is widely used around the world, Wikipedia is characterized by highly uneven geographies of participation. The goal of highlighting these inequalities is not to suggest that they are insurmountable. Our regression analysis shows that the availability of broadband is a clear factor in the propensity of people to participate on Wikipedia. The relationship is not a linear one, though. As a country approaches levels of connectivity above about 450,000 broadband Internet connections, the ability of broadband access to positively affect participation keeps increasing. Complicating this issue is the fact that participation from the world's economic peripheries tends to focus on editing about the world's cores rather than their own local regions. These results ultimately point to an informational magnetism that is cast by the world's economic cores, virtuous and vicious cycles that make it difficult to reconfigure networks and hierarchies of knowledge production.;Digital Divisions of Labor and Informational Magnetism: Mapping Participation in Wikipedia;Graham, M., Straumann, R. & Hogan, B.;2015;Hogan, B.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing; Digital Economies; Information Geography and Inequality"
This essay draws upon the social life of vinyl records as a means to consider social media as a set of many-to-many affordances rather than a suite of technologies. It defines affordances and draws upon their earlier history from cognitive science.;Mixing in Social Media;Hogan, B.;2015;Hogan, B.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
Research in computer-mediated communication has consistently asserted that Facebook use is positively correlated with social capital. This research has drawn primarily on Williams’ (2006) bridging and bonding scales as well as behavioral attributes such as civic engagement. Yet, as social capital is inherently a structural construct, it is surprising that so little work has been done relating social capital to social structure as captured by social network site (SNS) Friendship networks. Facebook is particularly well-suited to support the examination of structure at the ego level since the networks articulated on Facebook tend to be large, dense, and indicative of many offline foci (e.g., coworkers, friends from high school). Assuming that each one of these foci only partially overlap, we initially present two hypotheses related to Facebook social networks and social capital: more foci are associated with perceptions of greater bridging social capital and more closure is associated with greater bonding social capital. Using a study of 235 employees at a Midwestern American university, we test these hypotheses alongside self-reported measures of activity on the site. Our results only partially confirm these hypotheses. In particular, using a widely used measure of closure (transitivity) we observe a strong and persistent negative relationship to bonding social capital. Although this finding is initially counter-intuitive it is easily explained by considering the topology of Facebook personal networks: networks with primarily closed triads tend to be networks with tightly bound foci (such as everyone from high school knowing each other) and few connections between foci. Networks with primarily open triads signify many crosscutting friendships across foci. Therefore, bonding social capital appears to be less tied to local clustering than to global cohesion.;Assessing structural correlates to social capital in Facebook ego networks;Brooks, B., Hogan, B. & Ellison, N. et al.;2014;Hogan, B.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
In this report we detail the activities and research insights of the funded project “Who represents the Arab world online? Using the case of Wikipedia to map and measure local knowledge production and representation in the Middle East and North Africa.” We highlight the persistent information asymmetries between the MENA region and much of the developed world. We explore this through the use of data from Wikipedia and the Wikimedia foundation that has been extensively processed using a variety of tools from computer science. We augment this analysis with a series of qualitative insights discerned from two workshops with active Wikipedians in the MENA region. These workshops also served as forms of capacity building. In general, we believe that editing Wikipedia is an intensive task that is dominated by the global North and replete with a great deal of barriers to entry. It is nevertheless a key activity for future development of a stable information ecology within the MENA region. This is especially valid given how extensively Wikipedia is used across the web in sites such as Facebook and Google. We believe that capacity building among key Wikipedians can create greater understanding and offset much of the emotional labour required to sustain activity on the site in the face of intense arguments and ideological biases. However, we also believe that a distinct lack of sources both owning to a lack of legitimacy for MENA journalism and a paucity of open access government documents inhibit further growth in this area. Future work should be dedicated to these issues of support for active existing Wikipedians and knowledge sharing of Canada’s best practices in information sharing to facilitate accelerated growth of geographic content in the MENA region. But the ultimate difference will come from increased diffusion of broadband internet technology as demonstrated through several statistical models. We articulate local exceptions to this rule, but remain committed to an overall strategy of capacity building and broadband diffusion.;Uneven Openness: Barriers to MENA Representation on Wikipedia;Graham, M. & Hogan, B.;2014;Hogan, B.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing; Digital Economies; Information Geography and Inequality"
Geographies of codified knowledge have always been characterized by stark core–periphery patterns, with some parts of the world at the center of global voice and representation and many others invisible or unheard. Many have pointed to the potential for radical change, however, as digital divides are bridged and 2.5 billion people are now online. With a focus on Wikipedia, which is one of the world's most visible, most used, and most powerful repositories of user-generated content, we investigate whether we are now seeing fundamentally different patterns of knowledge production. Even though Wikipedia consists of a massive cloud of geographic information about millions of events and places around the globe put together by millions of hours of human labor, the encyclopedia remains characterized by uneven and clustered geographies: There is simply not a lot of content about much of the world. The article then moves to describe the factors that explain these patterns, showing that although just a few conditions can explain much of the variance in geographies of information, some parts of the world remain well below their expected values. These findings indicate that better connectivity is only a necessary but not a sufficient condition for the presence of volunteered geographic information about a place. We conclude by discussing the remaining social, economic, political, regulatory, and infrastructural barriers that continue to disadvantage many of the world's informational peripheries. The article ultimately shows that, despite many hopes that a democratization of connectivity will spur a concomitant democratization of information production, Internet connectivity is not a panacea and can only ever be one part of a broader strategy to deepen the informational layers of places.;Uneven Geographies of User-Generated Information: Patterns of Increasing Informational Poverty;Graham, M., Hogan, B. & Straumann, R. et al.;2014;Hogan, B.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing; Digital Economies; Information Geography and Inequality"
One of the many barriers to the incorporation of local and community actors in emerging energy governance structures and policy delivery mechanisms is the lack of thorough understanding of how they work in practice, and how best to support and develop effective local energy governance. Taking a meso-level perspective and a network approach to governance, this paper sheds some new light on this issue, by focusing on the relation, channels of communication and interactions between low carbon community groups (LCCGs) and other actors. Based on data gathered from LCCGs in Oxfordshire, UK, via network survey and interviews the research maps the relations in terms of the exchanges of information and financial support, and presents a relation-based structure of local energy governance. Analysis reveals the intensity of energy related information exchanges that is taking place at the county level and highlights the centrality of intermediary organization in facilitating information flow. The analysis also identifies actors that are not very dominant in their amount of exchanges, but fill ‘weak-tie’ functions between otherwise disconnected LCCGs or other actors in the network. As an analytical tool the analysis could be useful for various state and non-state actors that want to better understand and support – financially and otherwise – actors that enable energy related local action.;Network approach for local and community governance of energy: The case of Oxfordshire;Parag, Y., Hamilton, J. & White, V. et al.;2013;Hogan, B.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
Online forums are rich sources of information about user communication activity over time. Finding temporal patterns in online forum communication threads can advance our understanding of the dynamics of conversations. The main challenge of temporal analysis in this context is the complexity of forum data. There can be thousands of interacting users, who can be numerically described in many different ways. Moreover, user characteristics can evolve over time. We propose an approach that decouples temporal information about users into sequences of user events and inter-event times. We develop a new feature space to represent the event sequences as paths, and we model the distribution of the inter-event times. We study over 30,000 users across four Internet forums, and discover novel patterns in user communication. We find that users tend to exhibit consistency over time. Furthermore, in our feature space, we observe regions that represent unlikely user behaviors. Finally, we show how to derive a numerical representation for each forum, and we then use this representation to derive a novel clustering of multiple forums.;A time decoupling approach for studying forum dynamics;Kan, A., Chan, J. & Hayes, C. et al.;2013;Hogan, B.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
"Twitter tends to represent an unfounded resurgence of methodological realism, yet the medium is not merely a conduit for messages that reflect the true sentiment of all actors involved. Rather it is a structuring device for communication based on the idea of a ""network"" a priori. It is also a business. In this article, I discuss the technical and practical issues that simultaneously make Twitter feel like a profound window to the social world while masking many absences and inequalities. I critique the absence of lurkers, the focus on streaming data and the emphasis on personalization. Greater care must be made to triangulate Twitter data with traditional social science theories and methods.";Comment on elena pavan/1 considering platforms as actors;Hogan, B.;2013;Hogan, B.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
As rental markets move online, techniques to assess racial/ethnic rental housing discrimination should keep pace. We demonstrate an audit method for assessing discrimination in Toronto's online rental market. As a multicultural city with less segregation and more diverse visible minorities than most US cities, Toronto lends itself to multiname audit studies. We sent 5,620 fictitious email inquiries to landlords offering apartments on Craigslist, a popular Internet classifieds service. Each landlord received one inquiry each from five racialized groups—Caucasian, Black, E/SE Asian, Muslim/Arabic, and Jewish. In our experiments, “opportunity denying” discrimination (exclusion through nonresponse) was 10 times as common as “opportunity diminishing” discrimination (e.g., additional rental conditions). We estimate Muslim/Arabic–racialized men face the greatest resistance, with discrimination occurring in 12 percent of experiments. The level of discrimination is modest but significant for Asian men (7 percent), Blacks (5 percent), and Muslim/Arabic women (5 percent). Discrimination was evenly spread throughout the city.;Racial and Ethnic Biases in Rental Housing: An Audit Study of Online Apartment Listings;Hogan, B. & Berry, B.;2011;Hogan, B.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
There are numerous indirect ways in which socioeconomic status (SES) can advantage or disadvantage people in developing social capital. Specifically, SES affects the access individuals have to beneficial resources that indirectly affects the social capital benefits individuals receive from personal and group social networks. With the advent of social network sites like Facebook, the ability to quantify and measure the effects of SES on social capital benefits is possible to a much greater degree than ever before. This study of undergraduate students focuses on the relationship between SES and social capital. We examine the relationship between SES and three structural measures of students' social capital using online social network data: general social capital (network size), bridging social capital (number of clusters), and bonding social capital (average degree). According to our results, higher SES relates to larger and denser networks, but not to networks with more clusters. These findings suggest that SES is not related to greater opportunities for networking, but better capitalization of existing network contexts. In addition to the novel substantive contribution, this paper offers a methodological advance in the structural study of social capital, which has previously been limited in size or complexity due to recall.;SOCIOECONOMIC STATUS UPDATES: Family SES and emergent social capital in college student Facebook networks;Brooks, B., Welser, H. & Hogan, B. et al.;2011;Hogan, B.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
Presentation of self (via Goffman) is becoming increasingly popular as a means for explaining differences in meaning and activity of online participation. This article argues that self-presentation can be split into performances, which take place in synchronous “situations,” and artifacts, which take place in asynchronous “exhibitions.” Goffman’s dramaturgical approach (including the notions of front and back stage) focuses on situations. Social media, on the other hand, frequently employs exhibitions, such as lists of status updates and sets of photos, alongside situational activities, such as chatting. A key difference in exhibitions is the virtual “curator” that manages and redistributes this digital content. This article introduces the exhibitional approach and the curator and suggests ways in which this approach can extend present work concerning online presentation of self. It introduces a theory of “lowest common denominator” culture employing the exhibitional approach.;The Presentation of Self in the Age of Social Media: Distinguishing Performances and Exhibitions Online;Hogan, B.;2010;Hogan, B.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
"In ""Star Trek,"" Scotty suggests that Transwarp beaming is ""like trying to hit a bullet with a smaller bullet, whilst wearing a blindfold, riding a horse."" The study of social media faces similar challenges because new tools are developed at a rapid pace and existing tools are constantly being updated with new features, policies, and applications. Users tend to migrate, in often unpredictable ways, to new tools as well as to adopt multiple tools simultaneously, without showing consistent media preferences and habits. As a result, for scholars it sometimes feels as if the social media landscape changes too quickly to fully grasp and leaves scholars permanently lagging behind. The authors argue in this article that beyond the ebb and flow of everyday events and seemingly idiosyncratic usage, trends exist underlying long-term trajectories, persistent social practices, and discernible cultural patterns. The goal of this article on persistence and change in social media is to take the findings from the articles compiled in this special issue and extend these claims with an eye toward the aspects of social media that may persist for years, if not decades. They hope that their discussion of social media practice will provide an overarching framework from which future research can draw.";Persistence and Change in Social Media;Hogan, B. & Quan-Haase, A.;2010;Hogan, B.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
The intuitive appeal of force-directed layouts is due in part to their ability to represent underlying community structures. Such diagrams show dense pockets of nodes with bridges connecting across clusters. Yet, it is possible to start, rather than end, with community structure. This is a “pinwheel” diagram using the author’s Facebook personal network (captured July 15, 2009). Nodes represent the author’s friends and links represent friendships among them. The author is not shown. Each ‘wing’ radiating outwards is a partition using a greedy community detection algorithm (Wakita and Tsurumi, 2007). Wings are manually labelled. Node ordering within each wing is based on degree. Node color and size is also based on degree. Nodes position is based on a polar coordinate system: each node is on an equal angle of n/360º with a radius being a log-scaled measure of betweenness. Higher values are closer to the center indicating a sort of cross-partition ‘gravity’.;Pinwheel Layout to Highlight Community Structure;Hogan, B.;2010;Hogan, B.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
This paper explores the relationship between travel behaviour, ICT use and social networks. Specifically, we outline a theory of social action that can inform how ICTs relates to social activity travel and explore the efficacy of this theory in an empirical setting. We begin by outlining two factors that influence the propensity to travel: an individual's will to initiate events with members of one's social network, referred to as agency, and the social accessibility of network members themselves. Social accessibility defines a series of practical constraints for social‐activity travel and agency defines the extent to which an individual will actively work within these constraints to maintain their social network. The theoretical section first unpacks these concepts while embedding them in the research literature, finishing with an operationalisation of agency and social accessibility. Using this theory, the empirical section investigates the relationship between agency, social accessibility, and factors associated with both the respondents and their personal networks. More specifically, we examine how agency levels of interaction are related to differences in demographics, global measures of network structure and composition, and measures of media use, particularly of Internet and telephone. We conclude that individuals who are proximate or more active are more likely to maintain reciprocal relationships, and that more distant or infrequent ties require greater maintenance on the individual's part. We believe that studies of activity‐travel and ICTs will benefit from a theoretical lens that articulates some of the transformative effects of ICTs on travel vis‐à‐vis its effects on social life. Social accessibility and agency can help focus that lens thereby enabling researchers to make potentially more elaborate and realistic models that move beyond the spatial and temporal dimensions into social dimensions.;AGENCY IN SOCIAL ACTIVITY INTERACTIONS: THE ROLE OF SOCIAL NETWORKS IN TIME AND SPACE;Carrasco, J., Hogan, B. & Wellman, B. et al.;2009;Hogan, B.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
"This paper presents a data collection effort designed to incorporate the social dimension in social activity-travel behavior by explicitly studying the link between individuals' social activities and their social networks. The main hypothesis of the data collection effort is that individuals' travel behavior is conditional upon their social networks; that is, a key cause of travel behavior is the social dimension represented by social networks. With this hypothesis in mind, and using survey and interview instruments, the respondents' social networks are collected using an egocentric approach that is constituted by the interplay between their individual social structures and their social activity behavior. More explicitly, individuals' networks are a context within which to elicit social activity-travel generation, spatial distribution, and information communication and technology use. The resultant dataset links aspects, in novel ways, that have been rarely studied together, and provides a sound base of theory and method to study and potentially give new insights about social activity-travel behavior.";Collecting Social Network Data to Study Social Activity-Travel Behavior: An Egocentric Approach;Carrasco, J., Hogan, B. & Wellman, B. et al.;2008;Hogan, B.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
We describe an interview-based data-collection procedure for social network analysis designed to aid gathering information about the people known by a respondent and reduce problems with data integrity and respondent burden. This procedure, a participant-aided network diagram (sociogram), is an extension of traditional name generators. Although such a diagram can be produced through computer-assisted programs for interviewing (CAPIs) and low technology (i.e., paper), we demonstrate both practical and methodological reasons for keeping high technology in the lab and low technology in the field. We provide some general heuristics that can reduce the time needed to complete a name generator. We present findings from our Connected Lives field study to illustrate this procedure and compare to an alternative method for gathering network data.;Visualizing Personal Networks: Working with Participant-aided Sociograms;Hogan, B., Carrasco, J. & Wellman, B.;2007;Hogan, B.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
Social network analysis investigates relationships between people and information created by people. The field is presently in flux due to the increasing amount of available data and the concomitant interest in networks across many disciplines. This article reviews some of the recent advances in the field, such as p* modeling and community detection algorithms alongside some of the societal transitions that facilitate these advances. The latter third of the article raises some issues for data engineers to consider given the state of the field. These issues focus mainly on querying and managing large and complex datasets, such as those commonly found through online scraping. ;Using Information Networks to Study Social Behavior: An Appraisal ;Hogan, B.;2007;Hogan, B.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
Apart from conventional outsourcing and offshoring between firms, work is now also sourced from individuals working as online contractors through platforms such as Odesk. We examine what implications this new form of organizing has to the nature of the work, focusing on how online contractors’ geographic origin influences their labour market outcomes. Online contracting could potentially greatly increase the earnings of sufficiently skilled workers in low-income countries. We document that this not necessarily the case. Analysing transaction records from Odesk, a leading platform for online contract work, we suggest and empirically test for the presence of three mechanisms which depress developing countries’ online contractors’ wages. These are statistical discrimination, taste based discrimination, and lower reservation wages compared to contractors from developed countries. We discuss our findings in the context of “liabilities of origin”.;“Not a Lot of People Know Where It Is”: Liabilities of Origin in Online Contract Work;Kässi, O., Lehdonvirta, V. & Graham, M. et al.;2016;Hjorth, I.;"Digital Knowledge and Culture; Digital Economies; Information Geography and Inequality"
This paper highlights the strengths and benefits of the Global Production Network (GPN) approach. However, it also raises a concern that the manner in which embeddedness has been utilized within this paradigm is possibly ill-suited to arenas of virtual production in which commodification is heightened. In fact, to fully account for the uniqueness of virtual production we demonstrate that the GPN model requires significant extension and reformulation. This is the aim of this paper and in doing so we develop a new model of 'Virtual Production Networks' (VPNs). At the core of our VPN model is an original account of the distinctive manner in which online outsourcing platforms organize, commodify and disembed labor. Drawing upon detailed empirical research we conceptually map VPNs providing an original typology and highlighting the ways in which online outsourcing platforms are engineered and framed in an effort to disembed labor from the laws, institutions and norms. Although VPNs are disembedded they are not immaterial or operating in some kind of ethereal alternative dimension. Spatio-temporal fixes provide an alternative to the existing GPN use of embeddedness. We elucidate the manner in which disembedded - i.e. highly commodified - virtual production is fixed within regional, nation and local social networks. In fact, these spatio-temporal fixes enable the overcoming of a number of contradictions created by commodification. We conclude by considering the implications this has for social upgrading.;Virtual Production Networks: Fixing Commodification and Disembeddedness;Wood, A., Hjorth, I. & Graham, M.;2016;Hjorth, I.;"Digital Knowledge and Culture; Digital Economies; Information Geography and Inequality"
The Internet has long been predicted to become a shortcut that allows workers to work for any employer regardless of their location. To some extent this has now become a reality, with the rise of “online labour markets” that bring together buyers and sellers of intangible knowledge and service work from around the world. Practical considerations such as language and time zone differences can be expected to shape those markets, but a long tradition of research in international business finds that foreignness per se can also be a liability. In this paper, we use data from the largest global online marketplace to examine the degree to which geography still matters, not just for practical matters but also in terms of a persisting perception of foreignness in the online space. We find that the market is highly international, with most employers residing in rich countries and most workers in poor countries. However, workers are more likely to find work in their domestic markets. Moreover, domestic contractors get paid more than international contractors for the same type of work. Our analysis suggests that this bias against international contractors is not only due to practical factors such as time zone differences and language-based communication difficulties, but especially to what can be termed a “liability of foreignness”. We conclude that while the Internet can bridge physical distance, there are also other geographically conditioned barriers to trade that digital connectivity doesn’t necessarily address.;Online labour markets - levelling the playing field for international service markets?;Lehdonvirta, V., Graham, M. & Hjorth, I.;2014;Hjorth, I.;"Digital Knowledge and Culture; Digital Economies; Information Geography and Inequality"
As ever more policy-makers, governments and organisations turn to the gig economy and digital labour as an economic development strategy to bring jobs to places that need them, it becomes important to understand better how this might influence the livelihoods of workers. Drawing on a multi-year study with digital workers in Sub-Saharan Africa and South-east Asia, this article highlights four key concerns for workers: bargaining power, economic inclusion, intermediated value chains, and upgrading. The article shows that although there are important and tangible benefits for a range of workers, there are also a range of risks and costs that unduly affect the livelihoods of digital workers. Building on those concerns, it then concludes with a reflection on four broad strategies – certification schemes, organising digital workers, regulatory strategies and democratic control of online labour platforms – that could be employed to improve conditions and livelihoods for digital workers.;Digital labour and development: impacts of global digital labour platforms and the gig economy on worker livelihoods;Graham, M., Hjorth, I. & Lehdonvirta, V.;2017;Hjorth, I.;"Digital Knowledge and Culture; Digital Economies; Information Geography and Inequality"
There are many societal concerns that emerge as a consequence of Future Internet (FI) research and development. A survey identified six key social and economic issues deemed most relevant to European FI projects. During a SESERV-organized workshop, experts in Future Internet technology engaged with social scientists (including economists), policy experts and other stakeholders in analyzing the socio-economic barriers and challenges that affect the Future Internet, and conversely, how the Future Internet will affect society, government, and business. The workshop aimed to bridge the gap between thosewho study andthose who build the Internet. This chapter describes the socio-economic barriers seen by the community itself related to the Future Internet and suggests their resolution, as well as investigating how relevant the EU Digital Agenda is to Future Internet technologists.;Cross-Disciplinary Lessons for the Future Internet;Oostveen, A-M., Hjorth, I. & Pickering, B. et al.;2012;Hjorth, I.;"Digital Knowledge and Culture; Digital Politics and Government; Information Governance and Security"
In this chapter, Meyer argues that a main contribution of social informatics is to look at the hyphen in socio-technical, privileging neither the social nor the technical a priori when examing socio-technical systems. This has proven useful in several areas of research and teaching.;Examining the Hyphen: The Value of Social Informatics for Research and Teaching;Meyer, E.;2014;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
This chapter examines the challenges faced by academic libraries and archives as e-Research and digital scholarship mature. In particular, it discusses the tensions between open access and intellectual property rights, the way informal scholarly communications are increasingly being made public on the Internet, and the transition toward decentralized production and storage of data. The chapter also looks at important examples of the types of digital resources that libraries should consider to assist research in the sciences and humanities, explains how data webs can be used to integrate image and other data distributed across several heterogeneous repositories or databases, and considers the ways information and communication technologies (ICTs) are assisting scholars who work on ancient documents. Finally, it explores improving techniques of digital image capture for research purposes, and the opportunities and constraints associated with these technological developments.;Digital Resources and the Future of Libraries;Meyer, E., McCarthy-Madsen, C. & Fry, J.;2010;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
Enabling or Mediating the Social Sciences? The Opportunities and Risks of Bottom-up Innovation William H. Dutton and Eric T. Meyer The social sciences have sought to apply developments in e-research to social, behavioral , and policy research. In addition, they have provided insights on the legal, ethical , and other social factors shaping the use and implications of e-infrastructures across the disciplines. However, advanced Internet and Grid technologies have experienced slower uptake and diffusion in the social sciences and humanities—including in some data-rich areas where information and communication technologies (ICTs) might seem particularly relevant—than in the natural and physical sciences. Skeptics argue that most foreseeable communication and computational needs can be accommodated by the use of state-of-the-practice ICTs, such as software packages installed on personal computers linked to the Internet. Advocates of e–social science, in contrast, believe that advances in Internet and Grid technologies will enable innovations in the capacity and quality of research that would be impossible to contemplate using only the traditional tools of the field (essay 6.1). This vision has supported the emergence of what has become known as “e–social science”: efforts by the social sciences to join with computer science and engineering to launch national initiatives that seek to employ many of the same innovations in e-research that have been fostered in the “hard” sciences as e-science initiatives (chapter 2). Growing evidence of interest in e-research within the social sciences is tied to innovative applications across a range of quantitative and qualitative fields (Dutton and Meyer 2009). However, the emergence of e–social science as a distinctive area of research has raised concerns over the impact of network-enabled research on and in the social sciences . As discussed later in the chapter, social scientists’ traditional roles are being drawn into question by e-research infrastructures that may lead to changes in the basic ways in which scholarly work is conducted (Borgman 2007). This chapter begins by providing an overview of the use of ICTs within the social sciences, followed by a discussion on the nature and scope of e–social science. It distinguishes between research on the social shaping of technology and the application of emerging technologies, such as Grid-enabled data sets and tools. Early exploratory 166 William H. Dutton and Eric T. Meyer research on awareness and attitudes toward e-research in the social sciences suggests there is a growing base of support for its innovations, which is tied to a history of bottom-up processes shaping its uptake. The chapter then takes a critical look at the implications for social scientists of the wider diffusion of e-research tools, as well as at the potential and risk that use of these tools poses for the quality of research in this field. Our analysis highlights the need for research on how social, ethical, and institutional factors are shaping the ways in which e-research is applied across the disciplines. Use of e-Research in the Social Sciences Computing and telecommunications have long been central to the conduct of social science research. The first nonmilitary applications of computing in government after the Second World War were for the census as well as in finance and accountancy.1 Initial use in the social sciences was anchored in mainframe computers. These computers supported statistical analyses, often using software written in house by statisticians and social scientists, but increasingly based on “packaged” programs (e.g., SPSS).2 By the mid-1970s, interactive minicomputers enabled researchers to conduct more research online via telecommunications and to begin exploiting email and, occasionally, computer conferencing systems for collaboration.3 The personal computing revolution of the early 1980s led to new journals focused on the use of computers in the social sciences, such as the Social Science Computer Review, and a new perspective on the way the personal computer would empower individual social scientists to free themselves from queuing for the mainframe and conduct their research from their desktop at any time, writing many of their own programs.4 During the 1990s, social scientists found that their personal computers and standard software packages became their own workplace equivalent of the spinning jenny and handloom ,5 bringing the ability to address from their local machine(s) nearly every phase of their research, from analysis to publication;Enabling or Mediating the Social Sciences? The Opportunities and Risks of Bottom-up Innovation;Meyer, E. & Jeffreys, P.;2012;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
One of the challenges of building collaborative information systems for scientific and social scientific data is that many new projects are actually extensions of existing projects, often going back decades, which have embedded logic and work practices that are highly resistant to change. This resistance to change cannot, however, simply be attributed to conservatism on the part of individual scientists. On the contrary, many of the scientists that are discussed in this chapter are enthusiastic about the idea of contributing data to larger collaborations in exchange for the additional data that they will, in turn, have available to them. In practice, however, protocols that are the result of years of cumulative decisions at the local level have resulted in information storage systems that are highly idiosyncratic and often resistant to federation. To demonstrate this point, I report on two projects in very different domains which nevertheless share similar barriers to building a collaborative infrastructure.;Moving from Small Science to Big Science: Social and Organizational Impediments to Large Scale Data Sharing;Meyer, E.;2009;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
In this chapter, the author examines a single computerization movement: digital photography. During the period 1991-2004, the technology in digital cameras was being refined by manufacturers and the advantages and limitations of digital cameras were discussed widely in the popular and trade-oriented media. The author explores this period of technological change using the concept of computerization movements, particularly drawing on how computerization movements use technological action frames, which reflect composite understandings of a technology's function and use that are built up in the language about a technology. Content analysis data of major media sources shows a storyline developing alongside technology developments: The earliest digital cameras were widely hyped as potentially revolutionary but seriously limited in capability, and during the late 1990s concern was expressed regarding a perceived lack of sufficient technological progress toward acceptable quality technology. Once the technological limitations were overcome, widespread adoption was viewed as inevitable, even while concerns about such issues as digital manipulation remained. This simple story, however, does not reflect the more complex tensions experienced in many domains that rely heavily on photography. To explore this more complex story, the chapter also includes a more detailed look at a specialized domain, police and legal photography, where the adoption of digital photography followed a rockier course than the popular media accounts would suggest.;Framing the Photographs: Digital Photography as a Computerization Movement;Meyer, E.;2008;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
Digital photography is a relatively new topic for scholarly study in the area of computer mediated communication. Photographic technologies were only first computerized in the 1990s, but have rapidly supplanted older film technologies for a majority of professional uses. Digital photography has not simply substituted silicon chips for film, however, but has brought about rapid changes throughout the photographic process as photography entered the realm of information technology. This chapter presents a typology for approaching the study of photography as a form of computer mediated communication, and then presents several examples illustrating the consequences digital photography has for amateurs and professionals. Examples include photojournalism, scientific photography, photography in the legal system, and personal photography. The chapter ends with a call for additional research into the social aspects of this ubiquitous form of computer mediated communication.;Digital Photography;Meyer, E.;2008;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
The Socio-Technical Interaction Network (STIN) strategy for social informatics research was published late in Rob Kling’s life, and as a result, he did not have time to pursue its continued development. This paper aims to summarize existing work on STINs, identify key themes, strengths, weaknesses and limitations, and to suggest trajectories for the future of STIN research. The STIN strategy for research on socio-technical systems offers the potential for useful insights into the highly intertwined nature of social factors and technological systems, however a number of areas of the strategy remain underdeveloped and offer the potential for future refinement and modification.;Socio-Technical Interaction Networks: A Discussion of the Strengths, Weaknesses and Future of Kling’s STIN Model;Meyer, E.;2006;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
Automation of jobs is discussed as a threat to many job occupations, but in the UK healthcare sector many view technology and automation as a way to save a threatened system. However, existing quantitative models that rely on occupation-level measures of the likelihood of automation suggest that few healthcare occupations are susceptible to automation. In order to improve these quantitative models, we focus on the potential impacts of task-level automation on health work, using qualitative ethnographic research to understand the mundane information work in general practices. By understanding the detailed tasks and variations of information work, we are building a more complete and accurate understanding of how healthcare staff work and interact with technology and with each other, often mediated by technology.;Work that Enables Care: Understanding Tasks, Automation, and the National Health Service;Willis, M. & Meyer, E.;2018;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
A decade after the passing of Rob Kling, social informatics scholars come together on this panel to discuss the past and current state of social informatics research. This panel will provide an overview of the trajectory of social informatics from its emergence in Norway in the early 1980s to its current status as a scientific and intellectual movement uniquely positioned to investigate computerization and society. From the late 1970s, Kling and his colleagues engaged in the study of the social aspects of computing (Dutton, 2005) that would become arguably the most significant version of social informatics when labeled as such in 1996. Social informatics was defined as “the interdisciplinary study of the design, uses and consequences of information technology that takes into account their interaction with institutional and cultural contexts” (Kling, 1999). At that time, the call to incorporate the social, political, and cultural dimensions of ICT design and use threw down the gauntlet, moving the study of computerization and society in a decidedly social direction. Early social informatics research was primarily ethnographic and site specific or based on limited discourse analysis involving smaller case studies. However, in the last decade, the rise and pervasiveness of social media, has intensified the need to understand the interaction between the social and the technical making social informatics more relevant than ever. Recently, some social informatics scholars have heeded the call and have become engaged more intensively in case studies and comparative analysis while others are conducting large‐scale research, involving big data and quantitative techniques. Both types of research are driving social informatics into the next stage of its evolution. The panelists will present various perspectives on the past as well as current and future directions in social informatics research. Howard Rosenbaum will offer a history of social informatics. Madelyn Sanfilippo and Pnina Fichman will describe the evolution of social informatics research. Kristin Eschenfelder will use her research on social science data repositories to illustrate how case study approach is informative in social informatics research. Pnina Fichman and Noriko Hara will present a comparative analysis to demonstrate how this approach can be beneficial in social informatics research. Eric Meyer will describe how social informatics has shaped his approach to understanding e‐science, including the latest shifts toward ‘big data’ in the social sciences, and offers a useful framework for researching technology in use. Steve Sawyer, will discuss future paths for social informatics.;Social informatics: Now and then;Fichman, P., Rosenbaum, H. & Eschenfelder, K. et al.;2014;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
To determine actual attitudes and practices of those in the Future Internet industry towards user involvement, delegates at the 2012 FIA participated in a focus group and a survey. Continuous user involvement is highly valued and expected to maximise the societal benefits of FI applications. However, just over half of the FI projects apply a user-centred approach, and a large number of survey respondents admitted to being not very knowledgeable about standard user-centred design tools or techniques.;User Involvement in Future Internet Projects;Oostveen, A-M., Meyer, E. & Pickering, B.;2013;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing; Digital Politics and Government; Information Governance and Security"
This paper uses traditional scientometric and also webometric measures to gauge the prominence of e-Research across the globe. We have used the same keywords for both samples in an analysis of e-Research, using a wide variety of labels to capture the field as widely as possible. Thus we are able to compare scientometrics (broadly speaking, publication outputs) and webometrics (again, broadly, web presence), and this comparison makes for an interesting investigation into the global distribution of e-Research. This research also confirms contrasts in terms of the different labels used in different countries, such as the greater prominence of 'cyberinfrastructure' in the US and of 'e-Science' in the UK. Apart from this expected result, there are a number of surprising findings when we examine the global rank order of countries but also the relative prominence of publications as against online presence. Clearly, e-Research has recently emerged as a distinctive field which can be charted more easily than other areas of research. At the same time, there are a number of challenges inherent in scoping e-Research by the visibility of publications and online presence. In the conclusion, we discuss some of these, and indicate the usefulness and limits of research of this type.;Mapping global e-research: scientometrics and webometrics;Meyer, E., Park, H. & Schroeder, R.;2009;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
This article examines the shift to online knowledge in research. In recent years there has been a major transformation in how formal and informal science communication is disseminated by electronic means. At the same time, researchers’ practices in accessing knowledge and information have changed, particularly in the use of search engines and digitized resources apart from traditional journals. While we still know little about how this affects the nature of research, particularly in light of disciplinary differences, we reject here the idea that the simple growth of outputs and proliferation of outputs also leads straightforwardly to a richer and more diverse information and knowledge environment. Instead, we argue that gatekeepers such as search engines which shape online visibility, combined with competition for limited attention space at the leading edge of research, leads to a different model of how access to knowledge and information is being shaped.;The World Wide Web of Research and Access to Knowledge;Meyer, E. & Schroeder, R.;2009;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
"Objectives: To assess genetic vulnerability factors related to comorbid mood disorder and alcohol dependence. Methods: We analyzed family studies of bipolar affective disorder for familiality of comorbid alcohol dependence. We then analyzed family studies of alcohol dependence for familiality (and genetic linkage characteristics) of comorbid depression and mania. Results: Alcohol dependence was increased in subjects with mood disorder diagnoses within a series of families ascertained for bipolar illness. Comorbid alcohol dependence was found to be familial (Relative Risk = 1.5, p < .002). Similarly, comorbid depression and mania are familial within a series of families ascertained for alcohol dependence. Alcohol dependence and depression appear to be linked to markers on chromosome 1p; they are also associated with the gene coding for the M2 muscarinic receptor on chromosome 7q (CHRM2). Conclusions: There are specific genetic vulnerability factors related to comorbid alcohol dependence and depression. Family studies suggest that such specific factors may also be identified for alcohol dependence and mania.";Genetics of Comorbid Mood Disorder and Alcohol Dependence;Nurnberger, J., Kuperman, S. & Flury-Wetherill, L. et al.;2007;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
"This paper defines and extends Kling’s concept of a communication regimes by identifying the concept’s origins and offering a definition that will allow further research using this framework. The terminology used here originates in political science; in translating these concepts for information science, however, much of the original meaning can be maintained and fruitfully applied. The paper outlines the definition and illustrates it using examples from photojournalism as a communication regime undergoing change. A communication regime is: 1) a loosely coupled social network in which the communication and the work system are highly coupled; 2) a system with a set of implicit or explicit principles, norms, rules, and decision-making procedures around which actors’ expectations converge; 3) a system in which the types of communication are tightly coupled to the production system in which they are embedded; 4) a system with institutions which help to support and to regulate the regime; and 5) a system within which there are conflicts over control, over who enforces standards, over who bears the costs of change and who reaps the benefits of change. This example suggests other areas where a communication regime framework may help understand the relationship between IT and social change. ";Communication Regimes: A conceptual framework for examining IT and social change in organizations ;Meyer, E.;2006;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
The Collaborative Study on the Genetics of Alcoholism (COGA) seeks to identify genes contributing to alco­ holism and related traits (i.e., phenotypes), including depression. Among alcoholic subjects the COGA study found an increased prevalence of depressive syndrome (i.e., depression that may or may not occur in conjunc­ tion with increased drinking). This combination of alcoholism and depression tends to run in families. Comorbid alcoholism and depression occurred substan­ tially more often in first-degree relatives of COGA participants with alcoholism than in relatives of control participants. Based on these data, COGA investigators defined three phenotypes—“alcoholism,” “alcoholism and depression,” and “alcoholism or depression”—and analyzed whether these phenotypes were linked to spe­ cific chromosomal regions. These analyses found that the “alcoholism or depression” phenotype showed sig­ nificant evidence for genetic linkage to an area on chromosome 1. This suggests that a gene or genes on chromosome 1 may predispose some people to alco­ holism and others to depression (which may be alcohol induced). ;Is There a Genetic Relationship Between Alcoholism and Depression? ;Nurnberger, J., Foroud, T. & Flury-Wetherill, L. et al.;2002;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
" Recent advances in technology have reopened an old debate on which sectors will be most affected by automation. This debate is ill served by the current lack of detailed data on the exact capabilities of new machines and how they are influencing work. Although recent debates about the future of jobs have focused on whether they are at risk of automation, our research focuses on a more fine-grained and transparent method to model task automation and specifically focus on the domain of primary health care. This protocol describes a new wave of intelligent automation, focusing on the specific pressures faced by primary care within the National Health Service (NHS) in England. These pressures include staff shortages, increased service demand, and reduced budgets. A critical part of the problem we propose to address is a formal framework for measuring automation, which is lacking in the literature. The health care domain offers a further challenge in measuring automation because of a general lack of detailed, health care–specific occupation and task observational data to provide good insights on this misunderstood topic. This project utilizes a multimethod research design comprising two phases: a qualitative observational phase and a quantitative data analysis phase; each phase addresses one of the two project aims. Our first aim is to address the lack of task data by collecting high-quality, detailed task-specific data from UK primary health care practices. This phase employs ethnography, observation, interviews, document collection, and focus groups. The second aim is to propose a formal machine learning approach for probabilistic inference of task- and occupation-level automation to gain valuable insights. Sensitivity analysis is then used to present the occupational attributes that increase/decrease automatability most, which is vital for establishing effective training and staffing policy. Our detailed fieldwork includes observing and documenting 16 unique occupations and performing over 130 tasks across six primary care centers. Preliminary results on the current state of automation and the potential for further automation in primary care are discussed. Our initial findings are that tasks are often shared amongst staff and can include convoluted workflows that often vary between practices. The single most used technology in primary health care is the desktop computer. In addition, we have conducted a large-scale survey of over 156 machine learning and robotics experts to assess what tasks are susceptible to automation, given the state-of-the-art technology available today. Further results and detailed analysis will be published toward the end of the project in early 2019. We believe our analysis will identify many tasks currently performed manually within primary care that can be automated using currently available technology. Given the proper implementation of such automating technologies, we expect considerable staff resources to be saved, alleviating some pressures on the NHS primary care staff.";The Future of Health Care: Protocol for Measuring the Potential of Task Automation Grounded in the National Health Service Primary Care System;Willis, M., Duckworth, P. & Coulter, A. et al.;2019;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
In the Introduction to this special issue on the Social Informatics of Knowledge, the editors of the issue reflect on the history of the term “social informatics” and how the articles in this issue both reflect and depart from the original concept. We examine how social informatics researchers have studied knowledge, computerization, and the workplace, and how all of those have evolved over time. We describe the process by which articles were included, how they help us understand the field of social informatics scholarship today, and reflect briefly on what the future of the field holds.;The social informatics of knowledge;Meyer, E., Shankar, K. & Willis, M. et al.;2019;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
While talk of “Big Data” is now prevalent in many sectors, there are still relatively few examples of Big Data being used to shape public policy. This article reports an international study of Big Data for policy initiatives to understand the role played by data‐driven approaches in the policy process. Drawing on evidence (including policy analysis and interviews with stakeholders) from 58 initiatives, we find that some policy areas, notably efforts to improve government transparency, are far more represented than others, such as use of social media data for policy evaluation. We also find Big Data used more often in the policy cycle for foresight and agenda setting, or interim evaluation and monitoring, rather than for policy implementation and ex post evaluation. Many different types of data are used in the policy process, with traditional sources such as government statistics still favored over new and emerging sources. We find that use of Big Data for public policy is therefore at an early stage, with expectations far outstripping the current reality.;Big Data for Policymaking: Great Expectations, but with Limited Progress?;Poel, M., Meyer, E. & Schroeder, R.;2018;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
In the current hyperconnected era, modern Information and Communication Technology (ICT) systems form sophisticated networks where not only do people interact with other people, but also machines take an increasingly visible and participatory role. Such Human-Machine Networks (HMNs) are embedded in the daily lives of people, both for personal and professional use. They can have a significant impact by producing synergy and innovations. The challenge in designing successful HMNs is that they cannot be developed and implemented in the same manner as networks of machines nodes alone, or following a wholly human-centric view of the network. The problem requires an interdisciplinary approach. Here, we review current research of relevance to HMNs across many disciplines. Extending the previous theoretical concepts of socio-technical systems, actor-network theory, cyber-physical-social systems, and social machines, we concentrate on the interactions among humans and between humans and machines. We identify eight types of HMNs: public-resource computing, crowdsourcing, web search engines, crowdsensing, online markets, social media, multiplayer online games and virtual worlds, and mass collaboration. We systematically select literature on each of these types and review it with a focus on implications for designing HMNs. Moreover, we discuss risks associated with HMNs and identify emerging design and development trends.;Understanding Human-Machine Networks: A Cross-Disciplinary Survey;Tsvetkova, M., Yasseri, T. & Meyer, E. et al.;2017;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
In June 2015, the Computer Laboratory at University of Cambridge hosted the inaugural “Data for Policy” conference that was supported by a large network of key U.K. stakeholders including the Centre for Science and Policy (CSaP), Office for National Statistics, Royal Statistical Society, LSE Department of Methodology, Imperial Data Science Institute, and other academic and nonprofit organizations, and government departments. The organizers of the conference were keen to bring together academics, data practitioners, and government policy experts to discuss how to better link up the opportunities for using data to inform public policy with actual examples of successes and lessons from failures. The conference attracted 170 delegates, who made 90 presentations over the three days of the conference. More information can be found on the conference organization's website: dataforpolicy.org. From those 90 presentations which covered a gamut of topics, we selected a number of papers which were particularly suited for this special section of Policy & Internet and invited submissions from the authors. The four papers that successfully completed the peer review process are included here.;Data for Public Policy;Meyer, E., Crowcroft, J. & Engin, Z. et al.;2017;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
In this article, we examine the growth of the Internet as a research topic across the disciplines and the embedding of the Internet into the very fabric of research. While this is a trend that ‘everyone knows’, prior to this study, no work had quantified the extent to which this common sense knowledge was true or how the embedding actually took place. Using scientometric data extracted from Scopus, we explore how the Internet has become a powerful knowledge machine which forms part of the scientific infrastructure across not just technology fields, but also right across the social sciences, sciences and humanities.;The net as a knowledge machine: How the Internet became embedded in research;Meyer, E., Schroeder, R. & Cowls, J.;2016;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
"Big data has been widely promoted across disciplines and sectors for its potential to enhance lives and promote knowledge discovery. However, challenges arise at all stages of the data lifecycle due to the complexity of interactions between data and the contexts within which they are collected and managed, which has implications for interpretations of this data and eventual use of information and the creation of knowledge products from these data. Starting from the perspective of social informatics, this panel will discuss: the reciprocal relationships between data and context; specific challenges in distinct stages of data generation, data repository implementation, data curation, data use, and data reproducibility; and the implications of these challenges and their potential solutions for both social informatics research and society in general.";Social informatics of data norms;McCoy, C., Marcinkowski, M. & Sawyer, S. et al.;2016;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
In this paper we outline an initial typology and framework for the purpose of profiling human-machine networks, that is, collective structures where humans and machines interact to produce synergistic effects. Profiling a human-machine network along the dimensions of the typology is intended to facilitate access to relevant design knowledge and experience. In this way the profiling of an envisioned or existing human-machine network will both facilitate relevant design discussions and, more importantly, serve to identify the network type. We present experiences and results from two case trials: a crisis management system and a peer-to-peer reselling network. Based on the lessons learnt from the case trials we suggest potential benefits and challenges, and point out needed future work.;Human-Machine Networks: Towards a Typology and Profiling Framework;Eide, A., Pickering, J. & Yasseri, T. et al.;2016;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
In this essay, the role of human expertise in the face of technological advance is discussed. There are many examples of technology that have become sufficiently advanced that the previous need to develop expert-level skills before being able to perform at a high level is either vastly reduced or eliminated. For instance, digital cameras can create sharp, beautiful photos with essentially zero technical skill, and high-definition video recordings are available on smartphones and iPads. The question is not only whether these technologies eliminate the need for expertise (thus substituting engineering for expertise) but also if in doing so they foster the development of new types of creative expertise (such as an ability to use photography as part of a social media strategy, for instance). The article concludes by arguing that machines contribute to an increasingly capable constellation of people and machines, which together allow more people to develop their talents into expressions of creative expertise.;The expert and the machine: Competition or convergence?;Meyer, E.;2015;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
"This paper is the product of a workshop that brought together practitioners, researchers, and data experts to discuss how big data is becoming a resource for positive social change in low‐ and middle‐income countries (LMICs). We include in our definition of big data sources such as social media data, mobile phone use records, digitally mediated transactions, online news media sources, and administrative records. We argue that there are four main areas where big data has potential for promoting positive social change: advocacy; analysis and prediction; facilitating information exchange; and promoting accountability and transparency. These areas all have particular challenges and possibilities, but there are also issues shared across them, such as open data and privacy concerns. Big data is shaping up to be one of the key battlefields of our time, and the paper argues that this is therefore an opportune moment for civil society groups in particular to become a larger part of the conversation about the use of big data, since questions about the asymmetries of power involved are especially urgent in these uses in LMICs. Civil society groups are also currently underrepresented in debates about privacy and the rights of technology users, which are dominated by corporations, governments and nongovernmental organizations in the Global North. We conclude by offering some lessons drawn from a number of case studies that represent the current state‐of‐the‐art.";Big Data and Positive Change in the Developing World;Taylor, L., Cowls, J. & Schroeder, R. et al.;2014;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
"The web encourages the constant creation and distribution of large amounts of information; it is also a valuable resource for understanding human behavior and communication. To take full advantage of the web as a research resource that extends beyond the consideration of snapshots of the present, however, it is necessary to begin to take web archiving much more seriously as an important element of any research program involving web resources. The ephemeral character of the web requires that researchers take proactive steps in the present to enable future analysis. Efforts to archive the web or portions thereof have been developed around the world, but these efforts have not yet provided reliable and scalable solutions. This article summarizes the current state of web archiving in relation to researchers and research needs. Interviews with researchers, archivists, and technologists identify the differences in purpose, scope, and scale of current web archiving practice, and the professional tensions that arise given these differences. Findings outline the challenges that still face researchers who wish to engage seriously with web content as an object of research, and archivists who must strike a balance reflecting a range of user needs.";Community, tools, and practices in web archiving: The state‐of‐the‐art in relation to social science and humanities research needs;Dougherty, M. & Meyer, E.;2014;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
"Although the terminology of Big Data has so far gained little traction in economics, the availability of unprecedentedly rich datasets and the need for new approaches – both epistemological and computational – to deal with them is an emerging issue for the discipline. Using interviews conducted with a cross-section of economists, this paper examines perspectives on Big Data across the discipline, the new types of data being used by researchers on economic issues, and the range of responses to this opportunity amongst economists. First, we outline the areas in which it is being used, including the prediction and ‘nowcasting’ of economic trends; mapping and predicting influence in the context of marketing; and acting as a cheaper or more accurate substitute for existing types of data such as censuses or labour market data. We then analyse the broader current and potential contributions of Big Data to economics, such as the ways in which econometric methodology is being used to shed light on questions beyond economics, how Big Data is improving or changing economic models, and the kinds of collaborations arising around Big Data between economists and other disciplines.";Emerging practices and perspectives on Big Data analysis in economics: Bigger and better or more of the same?;Taylor, L., Schroeder, R. & Meyer, E.;2014;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
"This article underlines some aspects that relate, on the one side, to the technological devices necessary to photography production and, on the other, the kind of practices that shape and are shaped by those devices. It discusses how those relationships have shaped different visual regimes. Based on theoretical approaches like Science and Technology Studies (STS) and the Socio-technical Interactions Network (STIN) perspective, the article starts with a brief historical description focusing on the production of photos as a three-step process: 1) infrastructural elements of image production; 2) technologies of processing images; and 3) distribution/showing of images. It is proposed that photography has had four moments in this history. Finally, the article discusses the latest socio-technological practices, and proposes that the iPhone is the best example of the kind of devices that are possibly opening a fifth moment in photography technologies.";Creation and Control in the Photographic Process: iPhones and the emerging fifth moment of photography;Gomez-Cruz, E. & Meyer, E.;2012;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
The Oxford e-Social Science (OeSS) project investigated the uses and impacts of digital research – what others have called e-Research or e-Science – from the perspective of the social sciences. The study examined the factors shaping new approaches to digital research across the sciences and humanities as well as its implications for the nature and quality of research, in addition to the ethical, legal, and institutional issues it raises in particular research areas. Appendix A provides an overview of the OeSS project. As the OeSS project came to an end after six years of research across two phases, project members hosted two days of events focused on the challenges presented by digital research, and the issues raised for policy and practice. Key issue areas emerged in discussion of the social shaping and implications of digital research. They include the growing wealth of digital data, the potential for digital collaboration, new forms of scholarly communication, the ethical challenges of digital research, the reshaping of institutional boundaries, and the need for digital curricula. While there are many other issues, these surfaced as key in discussion about digital researchers and colleagues from business, industry and the policy communities with an interest in the vitality of this burgeoning area of research. This report discusses these issues in turn before concluding with thoughts on moving ahead. Appendix A provides an overview of the Oxford e-Social Science Project, which provided a foundation for the forum discussion, and Appendix B a list of participants in the policy forum that informs this report.;Key Issues for Digital Research: A Social Science Perspective on Policy and Practice;Dutton, W., Jirotka, M. & Meyer, E. et al.;2012;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
We argue that high-resolution naturalistic digital images of physical objects are oriented to in a very different manner than other visual representations such as ‘inscriptions’ which are manufactured by black-box devices in order to transform phenomena into diagrams, or ‘rendering practices’ where scientists visually transform the meaning of objects and events using representational techniques to select information and simplify its presentation. We show that medieval music scholars engage with high-resolution images of physical objects through crossmodal practices relying upon the interconnected senses to examine a variety of properties held within physical objects when they are displayed within digital images.;Interpreting Digital Images Beyond Just the Visual: Crossmodal Practices in Medieval Musicology;Eden, G., Jirotka, M. & Meyer, E.;2012;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
In recent years, many studies have highlighted the changing nature of scholarly research, reflecting the new digital tools and techniques that have been developed. But researcher uptake of these tools is strongly influenced by existing information behaviour, itself affected by a number of factors, particularly discipline. This article outlines findings from a recent study which used six case studies to look at the information behaviours of researchers working in different disciplinary fields or academic departments, or using specific tools. The study suggested that researchers’ uses of, and attitudes towards, digital technologies are affected by existing disciplinary habits and preconceptions. Furthermore, it found that the computational and collaborative complexity of the tools that researchers used was linked to their disciplinary backgrounds.;Discipline matters: Technology use in the humanities;Collins, E., Bulger, M. & Meyer, E.;2011;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
In many ways, the physical sciences are at the forefront of using digital tools and methods to work with information and data. However, the fields and disciplines that make up the physical sciences are by no means uniform, and physical scientists find, use, and disseminate information in a variety of ways. This report examines information practices in the physical sciences across seven cases, and demonstrates the richly varied ways in which physical scientists work, collaborate, and share information and data. This report details seven case studies in the physical sciences. For each case, qualitative interviews and focus groups were used to understand the domain. Quantitative data gathered from a survey of participants highlights different information strategies employed across the cases, and identifies important software used for research. Finally, conclusions from across the cases are drawn, and recommendations are made. This report is the third in a series commissioned by the Research Information Network (RIN), each looking at information practices in a specific domain (life sciences, humanities, and physical sciences). The aim is to understand how researchers within a range of disciplines find and use information, and in particular how that has changed with the introduction of new technologies.;Collaborative Yet Independent: Information Practices in the Physical Sciences;Meyer, E., Bulger, M. & Kyriakidou-Zacharoudiou, A. et al.;2012;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
This panel is comprised of international scholars studying how the information practices of researchers have been changing both the habits of individuals and the research directions of disciplines as research is increasingly reliant on digital tools and data. The panel will include short presentations, followed by substantial time for discussion and interaction with the audience. Among the key themes to be addressed are the new research questions that emerge as information becomes digital, the continuity and persistence of disciplinary habits, and the risks of certain types of research being excluded because it is non‐digital.;Researchers' information uses in a digital world: The big picture;Meyer, E., Harley, D. & Antonijevic, S. et al.;2012;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
In this report, the authors consider the possible future uses of web archives. This report is structured first, to engage in some speculative thought about the possible futures of the web as an exercise in prompting us to think about what we need to do now in order to make sure that we can reliably and fruitfully use archives of the web in the future. Next, we turn to considering the methods and tools being used to research the live web, as a pointer to the types of things that can be developed to help understand the archived web. Then, we turn to a series of topics and questions that researchers want or may want to address using the archived web. In this final section, we tentatively identify some of the short, medium and long term challenges individuals, organizations, and international bodies can target to increase our ability to explore these topics and answer these questions.;Web Archives: The Future(s);Meyer, E., Thomas, A. & Schroeder, R.;2011;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
Digitised materials representing the world’s cultural heritage are part of a growing trend towards a world in which knowledge is digitally stored, available on demand, and constantly growing. As the world becomes digital and the globally connected “digital brain” holds the shared knowledge of the world, the materials of the past need to be included in order to ensure that our collective memory online encompasses not just the present and the future, but also the past. This report is an effort to begin to synthesize the evidence available under the JISC digitisation and eContent programmes to better understand the patterns of usage of digitised collections in research and teaching, in the UK and beyond. JISC has invested heavily in eContent and digitisation, funding dozens of projects of varying size since 2004. However, until recently, the value of these efforts has been mostly either taken as given, or asserted via anecdote. By drawing on evidence of the various impacts of twelve digitised resources, we can begin to build a base of evidence that moves beyond anecdotal evidence to a more empirically-based understanding on a variety of impacts that have been measured by qualitative and quantitative methods.;Splashes and Ripples: Synthesizing the Evidence on the Impacts of Digital Resources;Meyer, E.;2011;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
With the evolution of the Internet from a controlled research network to a worldwide social and economic platform, the initial assumptions regarding stakeholder cooperative behavior are no longer valid. Conflicts have emerged in situations where there are opposing interests. Previous work in the literature has termed these conflicts tussles. This article presents the research of the SESERV project, which develops a methodology to investigate such tussles and is carrying out a survey of tussles identified within the research projects funded under the Future Networks topic of the FP7. Selected tussles covering both social and economic aspects are analyzed also in this article.;An Approach to Investigating Socio-economic Tussles Arising from Building the Future Internet;Kalogiros, C., Courcoubetis, C. & Stamoulis, G. et al.;2011;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
In this report, we summarize the state of the art of web archiving in relationship to researchers and research needs. This is a different focus than much of the earlier work in this area, including the JISC PoWR report which focused on institutional strategies for archiving web resources (JISC, 2008). It is important to note that this report focuses on the uses and needs of individual researchers. Research groups are also important, as some of the challenges that face individual researchers can quickly spiral into deeply complex tangles when dealing with collaboratories. For instance, national selection policies and national copyright rules can stand in the way of international projects, even if there are sound academic reasons to pursue international collaboration. While these issues are addressed here when appropriate, the bulk of the report focuses on individual researchers and institutions.;Researcher Engagement with Web Archives: State of the Art;Dougherty, M., Meyer, E. & Madsen, C. et al.;2010;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
This report, which should be read in conjunction with its companion, “Researcher Engagement with Web Archives: State of the Art” (Dougherty, et al., 2010), looks beyond the current state of web archives, and the uses made of them, to expand on some of the challenges identified there, and to point out some of the important opportunities which exist for funding bodies to add considerable value to the investments made to date, as well as to move web archiving technology and practices to the next level of comprehensiveness and usefulness.;Researcher Engagement with Web Archives: Challenges and Opportunities for Investment;Thomas, A., Meyer, E. & Dougherty, M. et al.;2010;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
Objective: Family studies have suggested that postpartum mood symptoms might have a partly genetic etiology. The authors used a genome-wide linkage analysis to search for chromosomal regions that harbor genetic variants conferring susceptibility for such symptoms. The authors then fine-mapped their best linkage regions, assessing single nucleotide polymorphisms (SNPs) for genetic association with postpartum symptoms.  Method: Subjects were ascertained from two studies: the NIMH Genetics Initiative Bipolar Disorder project and the Genetics of Recurrent Early-Onset Depression. Subjects included women with a history of pregnancy, any mood disorder, and information about postpartum symptoms. In the linkage study, 1,210 women met criteria (23% with postpartum symptoms), and 417 microsatellite markers were analyzed in multipoint allele sharing analyses. For the association study, 759 women met criteria (25% with postpartum symptoms), and 16,916 SNPs in the regions of the best linkage peaks were assessed for association with postpartum symptoms.  Results: The maximum linkage peak for postpartum symptoms occurred on chromosome 1q21.3-q32.1, with a chromosome-wide significant likelihood ratio Z score (Z  LR ) of 2.93 (permutation p=0.02). This was a significant increase over the baseline Z  LR of 0.32 observed at this locus among all women with a mood disorder (permutation p=0.004). Suggestive linkage was also found on 9p24.3-p22.3 (Z  LR =2.91). In the fine-mapping study, the strongest implicated gene was  HMCN1 (nominal p=0.00017), containing four estrogen receptor binding sites, although this was not region-wide significant. Conclusions: This is the first study to examine the genetic etiology of postpartum mood symptoms using genome-wide data. The results suggest that genetic variations on chromosomes 1q21.3-q32.1 and 9p24.3-p22.3 may increase susceptibility to postpartum mood symptoms.;Genome-Wide Linkage and Follow-Up Association Study of Postpartum Mood Symptoms;Mahon, P., Payne, J. & Dean, M. et al.;2009;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
e‐Research initiatives have been launched around the world, but have they captured the imagination of researchers across the disciplines? This paper reports on a web‐based survey designed to gauge awareness of and support for e‐Research initiatives. Early adoption and interest in e‐Research practices represent a wide range of methodological traditions, but those most interested in e‐Research tend to be among a cohort of more recent graduates of doctoral programmes. However, greater certainty and support is driven largely by proximity to e‐Research. This finding reinforces the value of efforts to engage more social scientists and other researchers in e‐Research, such as through demonstrations, training or other ways of providing hands‐on involvement. Doctoral and early career training might be the most fruitful arenas for engagement.;Experience with New Tools and Infrastructures of Research: An Exploratory Study of Distance From, and Attitudes Toward, e‐Research;Dutton, W. & Meyer, E.;2009;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
Are advances in ICTs enabling positive transformations in academic research practices? This paper explores key themes emerging from a 2008 survey of researchers aimed at identifying use and non‐use of advanced ICTs for research. Deterministic perspectives on e‐Research suggest that new e‐infrastructures will reshape research in predictable ways, such as by fostering more collaboration. In contrast, social shaping perspectives lead to the expectation that existing practices and institutions will shape the ways in which e‐Research is employed. This exploratory study found that individual variations across researchers cluster into identifiable groupings of research practices, which help to illuminate involvement in e‐Research, and differences in methods and research software used across the disciplines. In line with a social shaping perspective, the findings suggest that, in the social sciences, top‐down visions of e‐infrastructure development need to be tied to the realities of bottom‐up patterns of innovation in approaches to e‐Research.;Top‐Down e‐Infrastructure Meets Bottom‐Up Research Innovation: The Social Shaping of e‐Research;Meyer, E. & Dutton, W.;2009;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
e-Research is a rapidly growing research area, both in terms of publications and in terms of funding. In this article we argue that it is necessary to reconceptualize the ways in which we seek to measure and understand e-Research by developing a sociology of knowledge based on our understanding of how science has been transformed historically and shifted into online forms. Next, we report data which allows the examination of e-Research through a variety of traces in order to begin to understand how knowledge in the realm of e-Research has been and is being constructed. These data indicate that e-Research has had a variable impact in different fields of research. We argue that only an overall account of the scale and scope of e-Research within and between different fields makes it possible to identify the organizational coherence and diffuseness of e-Research in terms of its socio-technical networks, and thus to identify the contributions of e-Research to various research fronts in the online production of knowledge.;Untangling the web of e-Research: Towards a sociology of online knowledge;Meyer, E. & Schroeder, R.;2009;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
The experience of presence is at the core of everyday life and interactions. How we experience and make sense of the world plays a crucial role for how we think about and act upon it. Introducing new technologies like brain-computer interfaces and mixed reality applications to change, augment, substitute, or reconfigure this process is therefore bound to raise a number of fundamental social, ethical, and legal issues. To better understand the role of presence technologies in social interactions, this report engages in an in-depth analysis and discussion of four presence related research projects under the PEACH program (PRESENCCIA, IPCity, PASION, and IMMERSENCE) and also reports on the state of the debate in the PEACH community. The analysis reveals a rather diverse set of issues and concerns that run along a number of dimensions, including context of use, degree of uncertainty involved, and level of immersion experienced by users. Mapping the social, ethical, and legal issues, the report suggests that presence technologies not only raise new problems, but also added new qualities to existing ones. Besides questions of research ethics especially with regard to increasingly invasive techniques, concerns revolve - among other things - around the physical and mental well-being of users, issues of identity, deception, manipulation, and potential abuse, questions about privacy and the treatment of large amounts of personal data, the involvement of non-users, the difficult transition between different experiences of presence, and challenges to existing legal frameworks.;Social, Ethical, and Legal Issues in Presence Research and Applications;Schroeder, R., Meyer, E. & Ziewitz, M.;2009;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
This essay examines how researchers gain access to knowledge at a time when scholarly communication and materials are increasingly moving online. This topic has so far mainly been discussed in terms of journal publication and readership. Here we take a broader view, including a variety of areas where knowledge production and dissemination is broader than journal publications and includes data and tools. A second reason to take a broader view extends the horizon still further, since scientific communication and collaboration are not just undergoing change within the research community, but also depend on wider changes such as the use of search engines and how they affect what can be found online generally. New search behaviours are particularly evident among a new generation of scholars and potential scholars. Hence we will look at changes in research as well as in the realm of online knowledge more broadly.;Sifting Through the Online Web of Knowledge;Meyer, E. & Schroeder, R.;2009;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
The FKBP5 gene product forms part of a complex with the glucocorticoid receptor and can modulate cortisol-binding affinity. Variations in the gene have been associated with increased recurrence of depression and with rapid response to antidepressant treatment. We sought to determine whether common FKBP5 variants confer risk for bipolar disorder. We genotyped seven tag single-nucleotide polymorphisms (SNPs) in FKBP5, plus two SNPs previously associated with illness, in 317 families with 554 bipolar offspring, derived primarily from two studies. Single marker and haplotypic analyses were carried out with FBAT and EATDT employing the standard bipolar phenotype. Association analyses were also conducted using 11 disease-related variables as covariates. Under an additive genetic model, rs4713902 showed significant overtransmission of the major allele (P=0.0001), which was consistent across the two sample sets (P=0.004 and 0.006). rs7757037 showed evidence of association that was strongest under the dominant model (P=0.001). This result was consistent across the two datasets (P=0.017 and 0.019). The dominant model yielded modest evidence for association (P<0.05) for three additional markers. Covariate-based analyses suggested that genetic variation within FKBP5 may influence attempted suicide and number of depressive episodes in bipolar subjects. Our results are consistent with the well-established relationship between the hypothalamic–pituitary–adrenal (HPA) axis, which mediates the stress response through regulation of cortisol, and mood disorders. Ongoing whole-genome association studies in bipolar disorder and major depression should further clarify the role of FKBP5 and other HPA genes in these illnesses.;Family-based association of FKBP5 in bipolar disorder;Willour, V., Chen, H. & Toolan, J. et al.;2008;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
We sought to determine whether premenstrual mood symptoms exhibit familial aggregation in bipolar disorder or major depression pedigrees. Two thousand eight hundred seventy-six women were interviewed with the Diagnostic Interview for Genetic Studies as part of either the NIMH Genetics Initiative Bipolar Disorder Collaborative study or the Genetics of Early Onset Major Depression (GenRED) study and asked whether they had experienced severe mood symptoms premenstrually. In families with two or more female siblings with bipolar disorder (BP) or major depressive disorder (MDD), we examined the odds of having premenstrual mood symptoms given one or more siblings with these symptoms. For the GenRED MDD sample we also assessed the impact of personality as measured by the NEO-FFI. Premenstrual mood symptoms did not exhibit familial aggregation in families with BP or MDD. We unexpectedly found an association between high NEO openness scores and premenstrual mood symptoms, but neither this factor, nor NEO neuroticism influenced evidence for familial aggregation of symptoms. Limitations include the retrospective interview, the lack of data on premenstrual dysphoric disorder, and the inability to control for factors such as medication use.;Premenstrual mood symptoms: study of familiality and personality correlates in mood disorder pedigrees;Payne, J., Klein, S. & Zamoiski, R. et al.;2009;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
This paper examines the shift to online knowledge in research. In recent years there has been a major transformation in how formal and informal science communication is disseminated by electronic means. At the same time, researchers’ practices in accessing knowledge and information have changed, particularly in the use of search engines and digitized resources apart from traditional journals. While we still know little about how this affects the nature of research, particularly in light of disciplinary differences, we reject here the idea that the simple growth of outputs and proliferation of outputs also leads straightforwardly to a richer and more diverse information and knowledge environment. Instead, we argue that gatekeepers such as search engines which shape online visibility, combined with competition for limited attention space at the leading edge of research, leads to a different model of how access to knowledge and information is being shaped.;The world wide web of research and access to knowledge;Meyer, E. & Schroeder, R.;2009;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
"Objective. Brain-derived neurotrophic factor (BDNF) plays an important role in the survival, differentiation, and outgrowth of select peripheral and central neurons throughout adulthood. Growing evidence suggests that BDNF is involved in the pathophysiology of mood disorders. Methods. Ten single nucleotide polymorphisms (SNPs) across the BDNF gene were genotyped in a sample of 1749 Caucasian Americans from 250 multiplex bipolar families. Family-based association analysis was used with three hierarchical bipolar disorder models to test for an association between SNPs in BDNF and the risk of bipolar disorder. In addition, an exploratory analysis was performed to test for an association of the SNPs in BDNF and the phenotypes of rapid cycling and episode frequency. Results. Evidence of association (P<0.05) was found with several of the SNPs using multiple models of bipolar disorder; one of these SNPs also showed evidence of association (P<0.05) with rapid cycling. Conclusion. These results provide further evidence that variation in BDNF affects the risk for bipolar disorder.";Evidence of association between brain-derived neurotrophic factor gene and bipolar disorder;Liu, L., Foroud, T. & Xuei, X. et al.;2008;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
This paper examines the impact of e-Research, particularly in the social sciences, by doing a meta-analysis of existing sources and complementing this with new data. Sources include two online surveys of social scientists which gauged uses and awareness of e-Research tools, reports from national e-Social Science initiatives, and scientometric analyses of e-Social Sciences. The paper will undertake a preliminary evaluation of how different measures and indications capture the impact of e-Social Science. The paper argues that only a multidimensional analysis can do justice to understanding how social science is or is not being transformed via e-Research.;Gauging the Impact of e-Research in the Social Sciences;Meyer, E. & Schroeder, R.;2008;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
During the late 1990s, photography moved from being a primarily analogue medium to being an almost entirely digital medium. The development of digital cameras and software for working with photographs has led to the wholesale computerization of photography in many different domains. This paper reports on the findings of a study of the social and organizational changes experienced by marine mammal scientists who have changed from film‐based photography to digital photography. This technical change might be viewed as a simple substitution of a digital for an analogue camera, with little significance for how scientists do what they do. However, a perspective anchored in social informatics leads to the expectation that such incremental technical changes can have significant outcomes, changing not only how scientists work, but also the outcomes of their research. This present study finds that key consequences of this change have been the composition of the personnel working on the scientific research teams for marine biology projects and the ways in which these scientists allocate their time.;Technological Change and the Form of Science Research Teams: Dealing with the Digitals;Meyer, E.;2008;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
"Electroencephalographic (EEG) measures of hemispheric asymmetry in anterior brain activity have been related to a variety of indices of psychopathology and emotionality. However, little is known about patterns of frontal asymmetry in alcohol‐dependent (AD) samples. It is also unclear whether psychiatric comorbidity in AD subjects accounts for additional variance in frontal asymmetry, beyond a diagnosis of AD alone. We compared 193 AD subjects with 108 control subjects on resting brain activity in anterior and posterior regions, as indexed by asymmetries in α band power in the left and right hemispheres. Within the AD group alone, we examined whether comorbid major depressive disorder (MDD) or antisocial personality disorder (ASPD) had effects on regional asymmetry. Compared with control subjects, AD subjects exhibited lower left, relative to right, cortical activation in anterior regions. Evidence that comorbidity in AD subjects accounted for further variance in EEG asymmetry was mixed; AD subjects with comorbid ASPD were not significantly different from those without ASPD, while AD subjects with a lifetime history of MDD showed less asymmetry in anterior regions than those without MDD. Our findings indicate that AD subjects exhibit a pattern of frontal asymmetry similar to that found in other psychiatric groups. Results examining the effects of comorbidity in AD on EEG asymmetry were inconclusive. The implications of our findings for future work are described.";Patterns of Regional Brain Activity in Alcohol‐Dependent Subjects;Hayden, E., Wiegand, R. & Meyer, E. et al.;2006;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
Photoblogging, photo-sharing, and other internetactivities geared toward enabling photography as a centralpurpose have been growing in number and popularity inthe last several years. Photographs have been part of websites since the beginning of the graphical internet eraushered in by the Netscape browser. However, onlyrecently have sites appeared that are geared toward sharingphotographs as a central concern rather than just oneelement of a website really started to spring up.Flickr.com, for instance, was founded in 2004 andphotoblogs.org was founded in 2002. Most other photo-sharing websites were started in the last 2-3 years (see Table 1 below for details).Due to their novelty, relatively little research has beendone into the nature of photo-centric websites. Inparticular, there has not yet been 1) a systematicaccounting of the landscape of photo-centric websites anda description of how they are being used, 2) an analysiscomparing photoblogging and more generic photo-sharingsites, nor 3) a discussion of whether these represent apotential long-term computerization movement inphotography or a short-lived fad. This paper is an earlyattempt to address these issues by presenting somepreliminary research data on photoblogger’s behaviors.;How Photobloggers are Framing a New Computerization Movement;Meyer, E., Rosenbaum, H. & Hara, N. et al.;2005;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
"Alcohol dependence tends to aggregate within families. We analyzed data from the family collection of the Collaborative Study on the Genetics of Alcoholism to quantify familial aggregation using several different criterion sets. We also assessed the aggregation of other psychiatric disorders in the same sample to identify areas of possible shared genetic vulnerability. Age-corrected lifetime morbid risk was estimated in adult first-degree relatives of affected probands and control subjects for selected disorders. Diagnostic data were gathered by semistructured interview (the Semi-Structured Assessment for the Genetics of Alcoholism), family history, and medical records. Rates of illness were corrected by validating interview and family history reports against senior clinicians’ all sources best estimate diagnoses. Sex, ethnicity, comorbidity, cohort effects, and site of ascertainment were also taken into account. Including data from 8296 relatives of alcoholic probands and 1654 controls, we report lifetime risk rates of 28.8% and 14.4% for DSM-IV alcohol dependence in relatives of probands and controls, respectively; respective rates were 37.0% and 20.5% for the less stringent DSM-III-R alcohol dependence, 20.9% and 9.7% for any DSM-III-R diagnosis of nonalcohol nonnicotine substance dependence, and 8.1% and 5.2% for antisocial personality disorder. Rates of specific substance dependence were markedly increased in relatives of alcohol-dependent probands for cocaine, marijuana, opiates, sedatives, stimulants, and tobacco. Aggregation was also seen for panic disorder, obsessive-compulsive disorder, posttraumatic stress disorder, and major depression. Conclusions:  The risk of alcohol dependence in relatives of probands compared with controls is increased about 2-fold. The aggregation of antisocial personality disorder, drug dependence, anxiety disorders, and mood disorders suggests common mechanisms for these disorders and alcohol dependence within some families. These data suggest new phenotypes for molecular genetic studies and alternative strategies for studying the heterogeneity of alcohol dependence. The findings are presented in rank order of nominal significance. Several of these regions have been previously implicated in independent studies of either bipolar disorder or schizophrenia. The strongest finding is at 16p13 at D16S748 with an NPL of 3.3, there is evidence of epistasis between this locus and 9q21. Application of conditional analyses is potentially useful in larger sample collections to identify susceptibility genes of modest influence that may not be identified in a genome-wide scan aimed to identify single gene effects.";A Family Study of Alcohol Dependence: Coaggregation of Multiple Disorders in Relatives of Alcohol-DependentProbands;Nurnberger, J., Wiegand, R. & Bucholz, K. et al.;2004;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
"In 1989 the National Institute of Mental Health began a collaborative effort to identify genes for bipolar disorder. The first 97 pedigrees showed evidence of linkage to chromosomes 1, 6, 7, 10, 16, and 22 (Nurnberger et al 1997). An additional 56 bipolar families have been genotyped, and the combined sample of 153 pedigrees studied. Three hierarchical affection status models were analyzed with 513 simple sequence repeat markers; 298 were common across all pedigrees. The primary analysis was a nonparametric genome-wide scan. We performed conditional analyses based on epistasis or heterogeneity for five regions. One region, on 16p13, was significant at the genome-wide p < .05 level. Four additional chromosomal regions (20p12, 11p15, 6q24, and 10p12) showed nominally significant linkage findings ( p ≤ .01). Conditional analysis assuming epistasis identified a significant increase in linkage at four regions. Families linked to 6q24 showed a significant increase in nonparametric logarithms of the odds (NPL) scores at 5q11 and 7q21. Epistasis also was observed between 20p12 and 13q21, and 16p13 and 9q21.";Genome-wide scan and conditional analysis in bipolar disorder: evidence for genomic interaction in the National Institute of Mental Health genetics initiative bipolar pedigrees;McInnis, M., Dick, D. & Willour, V. et al.;2003;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
We conducted genomewide linkage analyses on 1,152 individuals from 250 families segregating for bipolar disorder and related affective illnesses. These pedigrees were ascertained at 10 sites in the United States, through a proband with bipolar I affective disorder and a sibling with bipolar I or schizoaffective disorder, bipolar type. Uniform methods of ascertainment and assessment were used at all sites. A 9-cM screen was performed by use of 391 markers, with an average heterozygosity of 0.76. Multipoint, nonparametric linkage analyses were conducted in affected relative pairs. Additionally, simulation analyses were performed to determine genomewide significance levels for this study. Three hierarchical models of affection were analyzed. Significant evidence for linkage (genomewide P<.05) was found on chromosome 17q, with a peak maximum LOD score of 3.63, at the marker D17S928, and on chromosome 6q, with a peak maximum LOD score of 3.61, near the marker D6S1021. These loci met both standard and simulation-based criteria for genomewide significance. Suggestive evidence of linkage was observed in three other regions (genomewide P<.10), on chromosomes 2p, 3q, and 8q. This study, which is based on the largest linkage sample for bipolar disorder analyzed to date, indicates that several genes contribute to bipolar disorder.;Genomewide Linkage Analyses of Bipolar Disorder: A New Sample of 250 Pedigrees from the National Institute of Mental Health Genetics Initiative;Dick, D., Foroud, T. & Flury, L. et al.;2003;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
The Collaborative Study on the Genetics of Alcoholism (COGA) seeks to identify genes contributing to alcoholism and related traits (i.e., phenotypes), including depression. Among alcoholic subjects the COGA study found an increased prevalence of depressive syndrome (i.e., depression that may or may not occur in conjunction with increased drinking). This combination of alcoholism and depression tends to run in families. Comorbid alcoholism and depression occurred substantially more often in first–degree relatives of COGA participants with alcoholism than in relatives of control participants. Based on these data, COGA investigators defined three phenotypes—“alcoholism,” “alcoholism and depression,” and “alcoholism or depression”—and analyzed whether these phenotypes were linked to specific chromosomal regions. These analyses found that the “alcoholism or depression” phenotype showed significant evidence for genetic linkage to an area on chromosome 1. This suggests that a gene or genes on chromosome 1 may predispose some people to alcoholism and others to depression (which may be alcohol induced).;Is There a Genetic Relationship Between Alcoholism and Depression?;Nurnberger, J., Foroud, T. & Flury-Wetherill, L. et al.;2003;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
"Depression (major depression or depressive syndrome) is more prevalent in alcoholic than in nonalcoholic subjects in families with multiple members with alcoholism studied as part of the Collaborative Study on the Genetics of Alcoholism (National Institute on Alcohol Abuse and Alcoholism). First-degree relatives of probands with comorbid alcoholism and depression have a higher prevalence of both disorders than relatives of probands with alcoholism alone, and both groups have a higher prevalence than the relatives of comparison subjects selected without regard to psychopathology. Data from the collaborative study were used to test three phenotypes (comorbid alcoholism and depression, alcoholism or depression, and depression) for genetic linkage.  Genome-wide sibling-pair linkage analyses were performed with the phenotypes comorbid alcoholism and depression, alcoholism or depression, and depression (major depression or depressive syndrome). Analyses were performed in two data sets (initial and replication data sets) from subject groups ascertained with identical criteria, as well as in the combined data set. Peak lod scores on chromosome 1 (near 120 centimorgan) for the alcoholism or depression phenotype were 5.12, 1.52, and 4.66 in the initial, replication, and combined data sets, respectively. The corresponding lod scores on chromosome 2 were 2.79, 0.20, and 3.26; on chromosome 6, they were 3.39, 0.00, and 0.92; and on chromosome 16, they were 3.13, 0.00, and 2.06. Lod scores on chromosome 2 for the comorbid alcoholism and depression phenotype in the three data sets were 0.00, 4.12, and 2.16, respectively. The results suggest that a gene or genes on chromosome 1 may predispose some individuals to alcoholism and others to depression (which may be alcohol induced). Loci on other chromosomes may also be of interest.";Evidence for a Locus on Chromosome 1 That Influences Vulnerability to Alcoholism and Affective Disorder;Nurnberger, J., Foroud, T. & Flury-Wetherill, L. et al.;2001;Meyer, E.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
The current conversation around automation and artificial intelligence technologies creates a future vision where humans may not possibly compete against intelligent machines, and that everything that can be automated through deep learning, machine learning, and other AI technologies will be automated. In this article, we focus on general practitioner documentation of the patients’ clinical encounters, and explore how these work practices lend themselves to automation by AI. While these work practices may appear perfect to automate, we reveal potential negative consequences to automating these tasks, and illustrate how AI may render important aspect of this work invisible and remove critical thinking. We conclude by highlighting the specific features of clinical documentation work that could leverage the benefits of human-AI symbiosis.;Automating Documentation: A Critical Perspective into the Role of Artificial Intelligence in Clinical Documentation;Willis, M. & Jarrahi, M.;2019;Willis, M.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
In this brief poster abstract we explore the finding from previous research that distributed teams collaborating on research use email to an overwhelming degree. This email is the source of collaboration and one of the central documents in the practice of doing science. We present an early idea of email focused ethnography and using visualizations to assist in the qualitative exploration of analyzing email communications. Of interest is the utility of different visualizations to inform follow up interviews of longitudinal fieldwork and data collection. Two such visualizations are presented and described. Along with the benefits of the techniques we describe some of the challenges. ;Using an Ethnography of Email to Understand Distributed Scientific Collaborations ;Sharma, S., Willis, M. & Snyder, J. et al.;2015;Willis, M.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
We ask the question: What document infrastructures do scientists build to support their virtual organizing and documenting practices? Cyberinfrastructure (CI) is seen by many as playing a critical role in the future of social, behavioral, and economic sciences (SBE) by enabling innovation and scientific discovery. However, little is known about SBE scientists' distributed collaboration, a vital practice that CI must support for the doing of science. To provide insight into this question we interviewed 12 scientists regarding their work practices as they pursue joint research projects with colleagues from other universities. We identify the most frequently used physical and digital tools for SBE science and collaboration and characterize commonplace scientific practices in this domain with a paradigmatic example.;Documents and distributed scientific collaboration;Willis, M., Sharma, S. & Snyder, J. et al.;2014;Willis, M.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
The present paper is the second in a series published at I/ITSEC that seeks to explain the efficacy of multirole experiential learning employed to create engaging game-based training methods transitioned to the U.S. Army, U.S. Army Special Forces, Civil Affairs, and Psychological Operations teams. The first publication (I/ITSEC 2009) summarized findings from a quantitative study that investigated experiential learning in the multi-player, PC-based game module transitioned to PEO-STRI, DARWARS Ambush! NK (non-kinetic). The 2009 publication reported that participants of multi-role (Player and Reflective Observer/Evaluator) game-based training reported statistically significant learning and engagement. Additionally when the means of the two groups (Player and Reflective Observer/Evaluator) were compared, they were not statistically significantly different from each other. That is to say that both playing as well as observing/evaluating were engaging learning modalities. The Observer/Evaluator role was designed to provide an opportunity for real-time reflection and meta-cognitive learning during game play. Results indicated that this role was an engaging way to learn about communication, that participants learned something about cultural awareness, and that the skills they learned were helpful in problem solving and decision-making. The present paper seeks to continue to understand what and how users of non-kinetic game-based missions learn by revisiting the 2009 quantitative study with further investigation such as stochastic player performance analysis using latent semantic analyses and graph visualizations to correlate against human coder ratings and pre- and post-test self-analysis. The results are applicable to First-Person game-based learning systems designed to enhance trainee intercultural communication, interpersonal skills, and adaptive thinking. In the full paper, we discuss results obtained from data collected from 78 research participants of diverse backgrounds who trained by engaging in tasks directly, as well as observing and evaluating peer performance in real-time. The goal is two-fold. One is to quantify and visualize detailed player performance data coming from game play transcription to give further understanding to the results in the 2009 I/ITSEC paper. The second is to develop a set of technologies from this quantification and visualization approach into a generalized application tool to be used to aid in future games development of player/learner models and game adaptation algorithms.;Beyond game effectiveness. Part II, a qualitative study of multi-role experiential learning;Willis, M., Tucker, E. & Raybourn, E. et al.;2010;Willis, M.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
Mobile devices can help digital students reach out across cultures to develop intercultural competence, improve learning, and provide course support for a variety of course topics. Intercultural competence is expressed through openness, cognitive adaptability, and behavioral flexibility toward unfamiliar cultures. Digital students demonstrate a behavioral flexibility toward technology use that can be leveraged to encourage students to embrace cultures different from their own. This paper explores the feasibility of using mobile devices as viable options for course support by utilizing traditional learning styles and cultural learning styles. From the conducted survey preferred networks are identified for creating a community to support mobile learning.;Leveraging Mobile Devices to Develop Intercultural Competency for Digital Students;Willis, M. & Raybourn, E.;2009;Willis, M.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
Personal health information management (PHIM) is a broad endeavor that requires the patient to navigate many different types of information. Including patients performing a variety of tasks and roles to make information useful. I ask the question: what practices constitute a patient’s personal health information management socio-technical assemblage? By doing this I am interested in understanding how PHIM is an assemblage of different actors, tools, technologies, information, and materialities that form a heterogeneous network to motivate the patient’s health maintenance and wellbeing. I describe information practices, planning and sense making practices that patients engage to begin to define this assemblage, and the social actors and materialities that manifest and stabilize. Then, I discuss three key commitments learned from this approach, namely: the personal aspect of PHIM, the role of physical and digital materials on PHIM, and the role of information practice materialities.;I’m Trying to Find my Way of Staying Organized: the Socio-Technical Assemblages of Personal Health Information Management;Willis, M.;2019;Willis, M.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
"Researchers, governments, and funding agencies are calling on research disciplines to embrace open data—data that anyone can access and use. They have done so based on the premise that research efforts can draw and generate several benefits from open data because it might provide further insight and enable individuals to replicate and extend current knowledge in different contexts. These potential benefits, coupled with a global push towards open data policies, bring open data into the agenda of research disciplines, which includes information systems (IS). In this paper, we respond to these developments as follows. We outline themes in the ongoing discussion around open data in the IS discipline. The themes fall into two clusters: 1) the motivation for open data includes themes of mandated sharing, benefits to the research process, extending the life of research data, and career impact; and 2) the implementation of open data includes themes of governance, socio-technical system, standards, data quality, and ethical considerations. In this paper, we outline the findings from a pre-ICIS 2016 workshop on the topic of open data. The workshop discussion confirmed themes and identified issues that require attention in terms of the approaches that IS researchers currently use. The IS discipline offers a unique knowledge base, tools, and methods that can advance open data across disciplines. Based on our findings, we provide suggestions on how IS researchers can drive the open data conversation. Further, we provide advice for adopting and establishing procedures and guidelines for archiving, evaluating, and using open data.";Contemporary Issues of Open Data in Information Systems Research: Considerations and Recommendations;Link, G., Lumbard, K. & Conboy, K. et al.;2017;Willis, M.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
"Objective To use a human factors perspective to examine how older adult patients with heart failure use cognitive artifacts for medication management. We performed a secondary analysis of data collected from 30 patients and 14 informal caregivers enrolled in a larger study of heart failure self-care. Data included photographs, observation notes, interviews, video recordings, medical record data, and surveys. These data were analyzed using iterative content analysis. Findings revealed that medication management was complex, inseparable from other patient activities, distributed across people, time, and place, and complicated by knowledge gaps. We identified 15 types of cognitive artifacts including medical devices, pillboxes, medication lists, and electronic personal health records used for: 1) measurement/evaluation; 2) tracking/communication; 3) organization/administration; and 4) information/sensemaking. These artifacts were characterized by fit and misfit with the patient's sociotechnical system and demonstrated both advantages and disadvantages. We found that patients often modified or “finished the design” of existing artifacts and relied on “assemblages” of artifacts, routines, and actors to accomplish their self-care goals. Cognitive artifacts are useful but sometimes poorly designed or not used optimally. If appropriately designed for usability and acceptance, paper-based and computer-based information technologies can improve medication management for individuals living with chronic illness. These technologies can be designed for use by patients, caregivers, and clinicians; should support collaboration and communication between these individuals; can be coupled with home-based and wearable sensor technology; and must fit their users’ needs, limitations, abilities, tasks, routines, and contexts of use.";Medication-related cognitive artifacts used by older adults with heart failure;Mickelson, R., Willis, M. & Holden, R.;2015;Willis, M.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
"An internet survey and follow-up mail survey were conducted in order to (a) determine New Mexico state legislators' preferred sources for information when making decisions about healthcare policies and to (b) assess the state legislators' attitudes toward e-mail more generally. Legislators were found to privilege expert colleagues and constituents over mass media as healthcare policy information sources; additionally, face-to-face encounters with constituents were preferred over e-mail, although respondents largely felt positive about using e-mail with both constituents and colleagues. These preferences regarding information sourcing and delivery indicate that public relations practitioners' (PRPs) continuing tendency to communicate with state legislators using traditional media-centric methods are problematic. Results suggest legislators could be reached with greater effect if PRPs were to supplement their use of mass media channels with more personal, symmetrical relationship management strategies.";Influencing Healthcare Policy: Implications of State Legislator Information Source Preferences for Public Relations Practitioners and Public Information Officers;Weiss, D., McIntosh-White, J. & Stohr, R. et al.;2015;Willis, M.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
Substance use and mental health problems are often underdiagnosed and undertreated in primary care. Veterans affairs facilities are using the Behavioral Telehealth Center (BTC) to provide evidence-based assessments for primary care patients via telephone. Whether participation in BTC services is associated with (1) increases in healthcare utilization and (2) decreases in symptoms based on behavioral health screening instruments, post-BTC services compared with pre-BTC services were investigated. Retrospective data were extracted for 1,820 patients who were referred to the BTC. Differences in utilization rates and symptom scores pre- and post-BTC services were tested using repeated measures analysis of covariance while controlling for relevant sociodemographic variables. Participants (1) utilized significantly more substance use and mental health treatment services and (2) had significantly lower alcohol and depression screening scores post-BTC services compared with pre-BTC services. This initial evaluation provides support that BTC services are associated with increased healthcare utilization and decreased alcohol and depressive symptoms.;Healthcare Utilization and Symptom Variation Among Veterans Using Behavioral Telehealth Center Services;Possemato, K., Bishop, T. & Willis, M. et al.;2013;Willis, M.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
Consumer and patient participation proved to be an effective approach for medical pictogram design, but it can be costly and time-consuming. We proposed and evaluated an inexpensive approach that crowdsourced the pictogram evaluation task to Amazon Mechanical Turk (MTurk) workers, who are usually referred to as the “turkers”. To answer two research questions: (1) Is the turkers’ collective effort effective for identifying design problems in medical pictograms? and (2) Do the turkers’ demographic characteristics affect their performance in medical pictogram comprehension? We designed a Web-based survey (open-ended tests) to ask 100 US turkers to type in their guesses of the meaning of 20 US pharmacopeial pictograms. Two judges independently coded the turkers’ guesses into four categories: correct, partially correct, wrong, and completely wrong. The comprehensibility of a pictogram was measured by the percentage of correct guesses, with each partially correct guess counted as 0.5 correct. We then conducted a content analysis on the turkers’ interpretations to identify misunderstandings and assess whether the misunderstandings were common. We also conducted a statistical analysis to examine the relationship between turkers’ demographic characteristics and their pictogram comprehension performance. The survey was completed within 3 days of our posting the task to the MTurk, and the collected data are publicly available in the multimedia appendix for download. The comprehensibility for the 20 tested pictograms ranged from 45% to 98%, with an average of 72.5%. The comprehensibility scores of 10 pictograms were strongly correlated to the scores of the same pictograms reported in another study that used oral response–based open-ended testing with local people. The turkers’ misinterpretations shared common errors that exposed design problems in the pictograms. Participant performance was positively correlated with their educational level. The results confirmed that crowdsourcing can be used as an effective and inexpensive approach for participatory evaluation of medical pictograms. Through Web-based open-ended testing, the crowd can effectively identify problems in pictogram designs. The results also confirmed that education has a significant effect on the comprehension of medical pictograms. Since low-literate people are underrepresented in the turker population, further investigation is needed to examine to what extent turkers’ misunderstandings overlap with those elicited from low-literate people.;Crowdsourcing Participatory Evaluation of Medical Pictograms Using Amazon Mechanical Turk;Yu, B., Willis, M. & Sun, P. et al.;2013;Willis, M.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
The election of U.S. President Obama thrust healthcare reform onto the legislative agenda and sent public information officers and public relations practitioners scrambling to more effectively influence legislators in order to benefit employers and clients. In previous studies, legislators placed mass media last on lists of sources preferred for informing policymaking decisions. This study replicates and extends previous research, filling a 20-year gap and focusing exclusively on healthcare policy information sources preferred by New Mexico legislators. An Internet survey indicated legislators preferred to get healthcare decision-making information first from expert colleagues, second from constituents. Mass media ranked at the bottom of their lists of preferred sources, although legislators did not entirely discount media usefulness. Respondents also felt while media can help build consensus, they usually promote adversarial relationships between lawmakers and their publics. Synthesis of results suggests that those seeking to influence legislators should divide their efforts more evenly between authoring journalist information subsidies and building personal relationships with legislators they seek to influence.;Legislators' reliance on mass media as information sources: Implications for symmetrical communication between public information officers, public relations practitioners and policymakers;McIntosh-White, J., Willis, M. & Stohr, R.;2013;Willis, M.;"Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
In the dot-com boom of the late 1990s, employees of Internet startups took risks—left well-paying jobs for the chance of striking it rich through stock options (only to end up unemployed a year later), relocated to areas that were epicenters of a booming industry (which shortly went bust), chose the opportunity to be creative over the stability of a set schedule. This book investigates choices such as these made by high-tech workers in New York City’s “Silicon Alley” in the 1990s. Why did these workers exhibit entrepreneurial behavior in their jobs—investing time, energy, and other personal resources that the author terms “venture labor”—when they themselves were employees and not entrepreneurs? The author argues that this behavior was part of a broader shift in society in which economic risk shifted away from collective responsibility toward individual responsibility. In the new economy, risk and reward took the place of job loyalty, and the dot-com boom helped glorify risks. Company flexibility was gained at the expense of employee security. Through extensive interviews, the author finds not the triumph of the entrepreneurial spirit but a mixture of motivations and strategies, informed variously by bravado, naïveté, and cold calculation. She connects these individual choices with larger social and economic structures, making it clear that understanding venture labor is of paramount importance for encouraging innovation and, even more important, for creating sustainable work environments which support workers.;Venture Labor: Work and the Burden of Risk in Innovative Industries;Neff, G.;2012;Neff, G.;Education, Digital Life and Wellbeing
The Internet and digital media are increasingly seen as having enormous potential for solving problems facing healthcare systems. This chapter traces emerging “digital health” uses and applications, focusing on the political economy of data. For many people, the ability to access their own data through social media and connect with people with similar conditions holds enormous potential to empower them and improve healthcare decisions. For researchers, digital health tools present new forms of always-on data that may lead to major discoveries. Technology and telecommunications companies hope their customers? data can answer key health questions or encourage healthier behavior. At the same time, Gina Neff argues that digital health raises policy and social equity concerns regarding sensitive personal data, and runs a risk of being seen as a sort of silver bullet instead of mere technological solutionism.;The Political Economy of Digital Health;Neff, G.;2019;Neff, G.;Education, Digital Life and Wellbeing
"Recent advancements in technology challenge our fundamental notions of human power and agency. Tools and techniques including machine learning, artificial intelligence, and chatbots may be capable of exercising complex “agentic” behaviors (Dhar, 2016). Advanced technologies are capable of communicating with human beings in an increasingly sophisticated manner. Ranging from artificial chat partners through the commercial algorithms of social media to cutting-edge robots, these encounters with interactive machines often result in a complex and intimate relationship between users and technologies (Finn, 2017). For instance, people now may have their own virtual assistants such as Apple’s Siri and Amazon’s Alexa. Other commercial technological agents “help” people find new movies on Netflix or friends on Facebook. Robotic companions, like Huggable developed by Cynthia Lynn Breazeal at MIT, can read human emotions and react to them accordingly. Chatbots have long evoked reactions from people that can be used therapeutically for psychological counselling and now these tools are being rolled out as apps to help people cope with anxiety and depression (Lien, 2017; Neff and Nagy, 2016). Such developments present a quandary for scholars of communication. Does the agency of the people and, increasingly, of things that we chose to communicate with matter? This question prompts us to urge communication scholars to develop a better definition of agency and more clarity on how agency is enacted in practice within complex, technologically-mediated interactions.";Agency in the digital age: Using symbiotic agency to explain human-technology interaction;Neff, G. & Nagy, P.;2018;Neff, G.;Education, Digital Life and Wellbeing
This chapter argues that the concerns of propaganda, voice, and democracy that characterized the rise of communication and media studies as disciplines were anchored in a set of twentieth-century liberal ideals that presumed the key role that information plays in people’s lives. This chapter argues that media and communication scholars need to update their theories for the twenty-first century. Both the election of Trump and the ‘Brexit’ referendum in the UK are case studies how twentieth century ideas about information, media and democracy are no longer sufficient to anchor contemporary media and communication scholarship. This chapter suggests a corrective by means an early twentieth century thinker who has not been used widely in media and communication, Emile Durkheim. By reintroducing the metaphor of organic and mechanical solidarity, this chapter argues that empathy and social cohesion might be alternates for intellectual anchors for our field for the future.;The Potential of Networked Solidarity;Neff, G.;2018;Neff, G.;Education, Digital Life and Wellbeing
The startups working in biosensing and self-tracking present a case to examine the role that power plays in the discursive process of framing new technologies. One frame often used for defining new data tools and services include “disruption,” or the perceived ability of technologies to upend the status quo of power within established industries or social institutions. In this chapter we present research findings on the start-up environment in consumer wellness and the more closely regulated field of mobile medical applications. We use two cases of health data innovation to show how discourses might affect how people design and use new technologies. The disruption discourse limits the possibilities for people to imagine technologies bridging existing social contexts and categories. Disruption limits such vision by overlooking the distinct roles for, and relationships around, data as it moves across contexts. Disruption helps to reproduce existing discourses of institutional power, even as it purports to change, replace, or disrupt those same power arrangements.;Disruption and the Political Economy of Biosensor Data;Fiore-Gartland, B. & Neff, G.;2016;Neff, G.;Education, Digital Life and Wellbeing
How has the internet influenced economic organization? Rather than approach this question by examining the productivity gains produced by internet technologies or the roles that dot-coms play in the economy this chapter examines how the process of technological change influences the organization of economic life. The social effects of the internet include emerging organizational innovations in addition to technological innovations. Although skeptics may point to failed dot-coms or the internet's broken promises of productivity gains, the values embedded in widely used information technologies have become encoded into the routines of the market and into organizational forms.;Permanently Beta: Responsive Organization in the Internet Era;Neff, G. & Stark, D.;2004;Neff, G.;Education, Digital Life and Wellbeing
This exploratory research examines how we might nudge consumers towards making healthier food choices in online grocery shopping or other digitally mediated food consumption contexts. Our pilot study investigated how different forms of social comparisons could be used to encourage consumers to reduce the number of calories contained in their online grocery basket. Our findings show that participants who were less interested in trying new diets were more willing to reduce calories when presented with a comparison to people unlike them, an out-group member comparison, while those who were interested in trying new diets were more willing to reduce calories regardless of social comparison type. These findings imply that one size does not fit all when nudging. More research is needed to see how social comparisons influence the effectiveness of digital health behavior projects.;Using Social Comparisons to Facilitate Healthier Choices in Online Grocery Shopping Contexts;DiCosola, B. & Neff, G.;2020;Neff, G.;Education, Digital Life and Wellbeing
Computing affects how scientific knowledge is constructed, verified, and validated. Rapid changes in hardware capability, and software flexibility, are coupled with a volatile tool and skill set, particularly in the interdisciplinary scientific contexts of oceanography. Existing research considers the role of scientists as both users and producers of code. We focus on how an intentional, individually-initiated but socially-situated, process of uptake influences code written by scientists. We present an 18-month interview and observation study of four oceanography teams, with a focus on ethnographic shadowing of individuals undertaking code work. Through qualitative analysis, we developed a framework of deliberate individual change, which builds upon prior work on programming practices in science through the lens of sociotechnical infrastructures. We use qualitative vignettes to illustrate how our theoretical framework helps to understand changing programming practices. Our findings suggest that scientists use and produce software in a way that deliberately mitigates the potential pitfalls of their programming practice. In particular, the object and method of visualization is subject to restraint intended to prevent accidental misuse.;Deliberate Individual Change Framework for Understanding Programming Practices in four Oceanography Groups;Kuksenok, K., Aragon, C. & Fogarty, J. et al.;2017;Neff, G.;Education, Digital Life and Wellbeing
High performance buildings—buildings with the aim of reduced energy and resource use— require that engineering analysis be at the center of an iterative and complex design process that assesses trade-offs, goals, and priorities across engineering and other fields of expertise. It has been observed that teams rarely get this right. Historical, cultural, and technical issues all get in the way of open communication and the integration of technical analysis. In this research, we ask what organizational and communication practices are needed for engineering to translate and design teams to synthesize complex energy modeling into the design of hospital buildings? In this paper we introduce a detailed ethnography of energy modeling during the conceptual phase of a new hospital design where energy modeling falls short of its potential. With cross case comparison, we found that a technically-knowledgeable boundary spanner in the owner organization enriches collaboration between the design team and the owner organization for more accurate and impactful energy modeling and improved translation of the model between team and owner. The energy modeling process became almost more important than the results of the energy model wherein the owner and design team had design-critical conversations about the model inputs and clear knowledge about the owner’s goals for the data. We propose that it is in this socially constructed knowledge where real high performance design can occur.;Technical Boundary Spanners and Translation: A Study of Energy Modeling for High Performance Hospitals;Neff, G.;2016;Neff, G.;Education, Digital Life and Wellbeing
Engaging the need to better understand the problems of high performance energy design in AEC collaborative practices and delivery methods, this study tested a schema that differentiated between the micro level of everyday design decisions, the meso level of project organization that guides project delivery, and the macro level of institutions—professions, disciplines, and firms — within which AEC practice takes place. Based in observations and interviews of two large projects in a U.S. architectural firm, we used a comparative case study to develop a series of analytical themes that located where issues of meso and macro level forces impacted micro level energy design decisions. This study found that the architect’s disciplinary vision and project management styles were very influential over energy design accomplishment, while firm attitudes promoting high performance design had little effect. Overall, we found no example of micro level design decisions that did not implicate some type of meso or macro level influence. This suggests that industry guides emphasizing technical solutions achieved at the micro level are not adequate for the needs of evolving AEC integrated practices;Finding connections between design processes and institutional forces on integrated AEC teams for high performing energy design;Monson, C., Neff, G. & Dossick, C. et al.;2016;Neff, G.;Education, Digital Life and Wellbeing
The study and analysis of large and complex data sets offer a wealth of insights in a variety of applications. Computational approaches provide researchers access to broad assemblages of data, but the insights extracted may lack the rich detail that qualitative approaches have brought to the understanding of sociotechnical phenomena. How do we preserve the richness associated with traditional qualitative methods while utilizing the power of large data sets? How do we uncover social nuances or consider ethics and values in data use? These and other questions are explored by human-centered data science, an emerging field at the intersection of human-computer interaction (HCI), computer-supported cooperative work (CSCW), human computation, and the statistical and computational techniques of data science. This workshop, the first of its kind at CSCW, seeks to bring together researchers interested in human-centered approaches to data science to collaborate, define a research agenda, and form a community.;Developing a Research Agenda for Human-Centered Data Science;Aragon, C., Hutto, C. & Echenique, A. et al.;2016;Neff, G.;Education, Digital Life and Wellbeing
High-performance (HP) buildings are known for the holistic approach to design and construction project delivery, which encompasses various performance goals, such as energy efficiency, environmental considerations, and occupants' well-being. Compared to traditional buildings, HP projects require closer integration in the design and construction process. Researchers have used conventional qualitative analysis to identify successful strategies in facilitating integration, and quantitative methods of research to rank such techniques in order of effectiveness. However, there have not been significant attempts in evaluating the joint causal effect of such strategies and uncovering the causal mechanisms that underlie successful HP projects. Utilizing a new methodology called fuzzy set-qualitative comparative analysis (fs-QCA), we analyzed the configurations of integration strategies used in more than 30 HP projects. We found that the four elements of setting ambitious environmental goals, having committed and trustworthy team members, using advanced information management technologies, and early and frequent involvement of the key team members create a sufficient recipe for achieving a high level of energy efficiency. Our analysis also shows that in the presence of these elements, contractual agreements do not produce a necessary element, as contractual barriers can be overcome with leadership, information systems, and work processes that engender an environment of trust, openness, and ambition.;Achieving Higher Energy Efficiency in High-Performance Buildings Using Integrated Practices: A Fuzzy Set- Qualitative Comparative Analysis Approach;Homayouni, H., Dossick, C. & Neff, G.;2014;Neff, G.;Education, Digital Life and Wellbeing
Despite the transition from paper to digital media, hand-off of data and documents from construction to operations and facilities management is still cumbersome and often requires manual entry and duplication of effort. This paper presents initial findings from an ongoing pilot project that began in spring 2011 on a digital information exchange standard called COBie (Construction Operations Building Information Exchange). Through interviews with key participants, we analyze existing practices as well as proposed changes to be made to these practices. Across a large organization, digital information is not trusted—nor is information neutral. Information is connected to particular jurisdictions who currently control the creation and management of their own datasets. We found that despite availability of digital information, people generally prefer to obtain information from colleagues with direct knowledge of the project or from paper documents. Digital information was considered to be either too difficult to access or not viewed as trustworthy since digital data was not consistently maintained. As more digital information is amassed, including information from COBie and building information models, organizational cultures and practices need to be developed around these new datasets.;Construction to Operations Exchange: Challenges of Implementing COBie and BIM in a Large Owner Organization;Anderson, A., Marsters, A. & Dossick, C. et al.;2012;Neff, G.;Education, Digital Life and Wellbeing
"Researchers have found successful collaboration that spans organizational boundaries enhances the productivity of the design and construction process. Researchers and practitioners alike argue using Building Information Modeling (BIM) should lead to tighter collaboration and closer communication among project participants working in cross-organizational environments. Using data from observations over fifteen months of the integrated design process of a laboratory building project, we build a typology of the strategies of successful collaboration within the AEC industry. Then, using interview data from 70 architects, engineers and general contractors from across the U.S., we test our proposed typology to suggest how the collaboration process is implicated in inter-organizational BIM integration. We find that inter-organizational BIM-enabled projects and successful inter-organizational collaboration have shared theoretical categories: fostering integrated teams; implementing tools and strategies to encourage clear communication across the team; and developing transparent technology use. We argue that attempts at either will not be successful without first establishing the right social and organizational foundation that supports both collaboration and successful technology implementation.";Theoretical Categories of Successful Collaboration and BIM Implementation within the AEC Industry;Homayouni, H., Neff, G. & Dossick, C.;2010;Neff, G.;Education, Digital Life and Wellbeing
In this paper, we will report on how the introduction of a new technology, Building Information Models (BIM), is in the process of changing collaboration among architects, engineers, and builders. Using a multi-method study of comparative case studies and triangulation interviews, we have observed two building projects over an eight-month period, interviewed architects, engineers, general contractors and subcontractors, and we are able to create generalizable grounded theory about technology-supported collaboration. This rich ethnographic data enables the analysis of the ramifications of the existing frameworks of standards of practice and occupational boundaries for collaboration, and allowed us to identify the potential of new technology to change these frameworks. Technology is always embedded in a social context, and its successful adoption depends upon that context. Some in the building design and construction industry are pushing for new technological advancements along side collaborative delivery methods, but there remain a number of organizational questions that must be addressed, particularly in how to engage second and third tier consultants, suppliers and subconstractors who are not part of the primary architect-owner-contractor agreements. Where delivery methods alone will not address the inter-organizational challenges, strategies such as co-location support a stronger team orientation to the project through informal communication.;The Realities of Building Information Modeling for Collaboration in the AEC Industry;Dossick, C., Neff, G. & Homayouni, H.;2009;Neff, G.;Education, Digital Life and Wellbeing
Highly energy efficient (HEE) buildings require a whole-system approach to building design. Scholars have suggested many tools, techniques, and processes to address the cross-disciplinary complexities of such an approach, but how these elements might be best combined to lead to better project outcomes is yet unknown. To address this, we surveyed architects associated with 33 AIA-COTE award-winning projects on the social, organizational, and technological elements of whole-system design (WSD) practices. We then used fuzzy sets-qualitative comparative analysis (fsQCA) to analyze the interdependencies among those elements. We found three distinct pathways for the design and construction of HEE buildings: information-driven, process-driven, or organization-driven. We also found that HEE buildings share some conditions for success, including having shared goals, owners engagement in the design process, and frequent and participatory interorganizational meetings. Our findings can help practitioners strategize and make decisions on incorporating WSD elements associated with their project types. Moreover, these results provide a launchpad for scholars to investigate complementarities among elements facilitating the design and construction process of HEE projects.;Three Pathways to Highly Energy Efficient Buildings: Assessing Combinations of Teaming and Technology;Homayouni, H., Dossick, C. & Neff, G.;2021;Neff, G.;Education, Digital Life and Wellbeing
This forum is dedicated to personal health in all its many facets: decision making, goal setting, celebration, discovery, reflection, and coordination, among others. We look at innovations in interactive technologies and how they help address current critical healthcare challenges.;Who does the work of data?;Møller, N., Bossen, C. & Pine, K. et al.;2020;Neff, G.;Education, Digital Life and Wellbeing
Sex-for-rent schemes have emerged on online sites as rental options. We analyzed 583 advertisements that were posted on Craigslist in London and Los Angeles and interviewed 34 women who were or had been in these arrangements. This research yielded four key tensions: (1) navigating innuendo (mis)interpretation versus preserving arranged ambiguity, (2) the guise of amateurism and romance versus persistent specificity, (3) calculated sacrifice versus narrative of a better life, and (4) consent versus consensual non-consent. Findings attest to the affordances online platforms offer by connecting geographically dispersed parties in a low risk, anonymous forum. Furthermore, present research joins discourses on the commercialization of intimacy and forms of precarious, gendered labor while asserting Internet features are pivotal in facilitating these arrangements. We propose gendered affordances to conceptualize how individual aspirational labor efforts, combined with platform affordances, commodify intimacy for sale on the moral marketplace.;The gendered affordances of Craigslist “new-in-town girls wanted” ads;Schwartz, B. & Neff, G.;2019;Neff, G.;Education, Digital Life and Wellbeing
Through the study of visualizations, virtual worlds and information exchange, the purpose of this paper is to reveal the complex connections between technology and the work of design and construction. The authors apply the sociotechnical view of technology and the ramifications this view has on successful use of technology in design and construction. This is a discussion paper reviewing over a decade of research that connects three streams of research on architecture, engineering and construction (AEC) teams as these teams grappled with adapting work practices to new technologies and the opportunities these technologies promised. From studies of design and construction practices with building information modeling and energy modeling, the authors show that given the constructed nature of models and the loose coupling of project teams, these team organizational practices need to mirror the modeling requirements. Second, looking at distributed teams, whose interaction is mediated by technology, the authors argue that virtual world visualizations enhance discovery, while distributed AEC teams also need more traditional forms of 2D abstraction, sketching and gestures to support integrated design dialogue. Finally, in information exchange research, the authors found that models and data have their own logic and structure and, as such, require creativity and ingenuity to exchange data across systems. Taken together, these streams of research suggest that process innovation is brought about by people developing new practices. In this paper, the authors argue that technology alone does not change practice. People who modify practices with and through technology create process innovation.;Innovation through practice: The messy work of making technology useful for architecture, engineering and construction teams;Dossick, C., Osburn, L. & Neff, G.;2019;Neff, G.;Education, Digital Life and Wellbeing
"What would data science look like if its key critics were engaged to help improve it, and how might critiques of data science improve with an approach that considers the day-to-day practices of data science? This article argues for scholars to bridge the conversations that seek to critique data science and those that seek to advance data science practice to identify and create the social and organizational arrangements necessary for a more ethical data science. We summarize four critiques that are commonly made in critical data studies: data are inherently interpretive, data are inextricable from context, data are mediated through the sociomaterial arrangements that produce them, and data serve as a medium for the negotiation and communication of values. We present qualitative research with academic data scientists, “data for good” projects, and specialized cross-disciplinary engineering teams to show evidence of these critiques in the day-to-day experience of data scientists as they acknowledge and grapple with the complexities of their work. Using ethnographic vignettes from two large multiresearcher field sites, we develop a set of concepts for analyzing and advancing the practice of data science and improving critical data studies, including (1) communication is central to the data science endeavor; (2) making sense of data is a collective process; (3) data are starting, not end points, and (4) data are sets of stories. We conclude with two calls to action for researchers and practitioners in data science and critical data studies alike. First, creating opportunities for bringing social scientific and humanistic expertise into data science practice simultaneously will advance both data science and critical data studies. Second, practitioners should leverage the insights from critical data studies to build new kinds of organizational arrangements, which we argue will help advance a more ethical data science. Engaging the insights of critical data studies will improve data science. Careful attention to the practices of data science will improve scholarly critiques. Genuine collaborative conversations between these different communities will help push for more ethical, and better, ways of knowing in increasingly datum-saturated societies.";Critique and Contribute: A Practice-Based Framework for Improving Critical Data Studies and Data Science;Neff, G., Tanweer, A. & Fiore-Gartland, B. et al.;2017;Neff, G.;Education, Digital Life and Wellbeing
This short essay concludes a Special Section on the book Venture Labor and reflects on the directions for future research for media and communication scholars. The essay argues that new ways of studying the political economy of information-intensive industries stretch the traditional scope of media and communication studies. This “informational economy” relies ever more on the production and circulation of commodified and monetized values emerging within the media industry and media practices broadly construed. This essay concludes a collection of articles proposing theoretical frameworks and empirical examples to deal with the transformations of work and economic value within the media and communication field. Collectively, the authors in this special issue address how media workers are responding to technological and economic change and how new communication technologies influence the production of economic value. This essay argues that the field of media and communication studies can help scholars understand the practices of the informational economy, the status of workers within this economy, and their resistance to its exploitative tendencies.;Conclusion: Agendas for studying communicative capitalism;Neff, G.;2017;Neff, G.;Education, Digital Life and Wellbeing
In 2016, Microsoft launched Tay, an experimental artificial intelligence chat bot. Learning from interactions with Twitter users, Tay was shut down after one day because of its obscene and inflammatory tweets. This article uses the case of Tay to re-examine theories of agency. How did users view the personality and actions of an artificial intelligence chat bot when interacting with Tay on Twitter? Using phenomenological research methods and pragmatic approaches to agency, we look at what people said about Tay to study how they imagine and interact with emerging technologies and to show the limitations of our current theories of agency for describing communication in these settings. We show how different qualities of agency, different expectations for technologies, and different capacities for affordance emerge in the interactions between people and artificial intelligence. We argue that a perspective of “symbiotic agency”—informed by the imagined affordances of emerging technology—is required to really understand the collapse of Tay.;Talking to Bots: Symbiotic Agency and the Case of Tay;Neff, G. & Nagy, P.;2016;Neff, G.;Education, Digital Life and Wellbeing
Quantified Self (QS) is a group that coordinates a global set of in-person meetings for sharing personal experiences and experiments with self-tracking behaviours, moods, and activities. Through participation in US-based QS events and watching online QS presentations from around the globe, we identify a function of ambiguous valuation for supporting sharing communities. Drawing on Stark's (2011) theory of heterarchy, we argue that the social and technical platforms supporting sharing within the QS community allow for multiple, sometimes conflicting, sets of community and commercial values. Community cohesion benefits from ambiguity over which values set is most important to QS members. Ambiguity is promoted by sharing practices through at least two means, the narrative structure of members' presentations, and what counts as tracking. By encouraging members to adhere to a three-question outline, the community ensures that multiple values are always present. Thus, it becomes a question of which values this sharing community emphasizes, not which value sets members present, at any given time. By leaving the tools and methods of tracking open − from sophisticated wearables and data analysis to pen-and-paper and storytelling − the community creates space for and embraces self-trackers with a broad spectrum of technological proficiency and interest. QS as a group capitalizes on circulation of knowledge valued somewhat ambiguously to sustain and grow the community, both encouraging and supporting the commercialization of self-tracking technologies while keeping technology developer interests from overwhelming community-building interests. This, we argue, has implications for researchers hoping to understand online communities and the ‘sharing economy' more generally.;Technologies for Sharing: lessons from Quantified Self about the political economy of platforms;Barta, K. & Neff, G.;2015;Neff, G.;Education, Digital Life and Wellbeing
"In this essay, we reconstruct a keyword for communication—affordance. Affordance, adopted from ecological psychology, is now widely used in technology studies, yet the term lacks a clear definition. This is especially problematic for scholars grappling with how to theorize the relationship between technology and sociality for complex socio-technical systems such as machine-learning algorithms, pervasive computing, the Internet of Things, and other such “smart” innovations. Within technology studies, emerging theories of materiality, affect, and mediation all necessitate a richer and more nuanced definition for affordance than the field currently uses. To solve this, we develop the concept of imagined affordance. Imagined affordances emerge between users’ perceptions, attitudes, and expectations; between the materiality and functionality of technologies; and between the intentions and perceptions of designers. We use imagined affordance to evoke the importance of imagination in affordances—expectations for technology that are not fully realized in conscious, rational knowledge. We also use imagined affordance to distinguish our process-oriented, socio-technical definition of affordance from the “imagined” consensus of the field around a flimsier use of the term. We also use it in order to better capture the importance of mediation, materiality, and affect. We suggest that imagined affordance helps to theorize the duality of materiality and communication technology: namely, that people shape their media environments, perceive them, and have agency within them because of imagined affordances.";Imagined Affordance: Reconstructing a Keyword for Communication Theory;Nagy, P. & Neff, G.;2015;Neff, G.;Education, Digital Life and Wellbeing
Communication technologies increasingly mediate data exchanges rather than human communication. We propose the term data valences to describe the differences in expectations that people have for data across different social settings. Building on two years of interviews, observations, and participation in the communities of technology designers, clinicians, advocates, and users for emerging mobile data in formal health care and consumer wellness, we observed the tensions among these groups in their varying expectations for data. This article identifies six data valences (self-evidence, actionability, connection, transparency, “truthiness,” and discovery) and demonstrates how they are mediated and how they are distinct across different social domains. Data valences give researchers a tool for examining the discourses around, practices with, and challenges for data as they are mediated across social settings.;Communication, Mediation, and the Expectations of Data: Data Valences Across Health and Wellness Communities ;Fiore-Gartland, B. & Neff, G.;2015;Neff, G.;Education, Digital Life and Wellbeing
This article briefly reviews theories of materiality emerging in communication technology studies and organizational communication and then suggests three ways that journalism scholars might apply these theories to studies of news production. How journalists work, how journalism is shaped within newsrooms, the ways the news industry is changing, and ultimately, the effects of digital transitions can all benefit from including a focus on the ‘objects of journalism’. First, objects, such as documents, help scholars describe the social settings where objects are found. Second, the objects of journalism help scholars uncover lines of authority, contexts of news routines, and richness and persistence of news practices. Third, studying the objects of journalism can help explain the persistence of so-called residual practices that might otherwise seem dysfunctional in digital news. Materiality theories can help journalism scholars explain the impact of the transition to digital news on the work and practices of journalists and the news industry as a whole.;Learning from documents: Applying new theories of materiality to journalism;Neff, G.;2014;Neff, G.;Education, Digital Life and Wellbeing
Engineering teams collaborating in virtual environments face many technical, social, and cultural challenges. In this paper we focus on distributed teams making joint unanticipated discoveries in virtual environments. We operationalize a definition of “messy talk” as a process in which teams mutually discover issues, critically engage in clarifying and finding solutions to the discovered issues, exchange their knowledge, and resolve the issue. Can globally distributed teams use messy talk via virtual communication technology? We analyzed the interactions of four distributed student teams collaborating on a complex design and planning project using building information models (BIMs) and the cyber-enabled global research infrastructure for design (CyberGRID), a virtual world specifically developed for collaborative work. Their interactions exhibited all four elements of messy talk, even though resolution was the least common. Virtual worlds support real-time joint problem solving by (1) providing affordances for talk mediated by shared visualizations, (2) supporting team perceptions of building information models that are mutable, and (3) allowing transformations of those models while people were together in real time. Our findings suggest that distributed team collaboration requires technologies that support messy talk—and iterative trial and error—for complex multidimensional problems.;Messy Talk in Virtual Teams: Achieving Knowledge Synthesis through Shared Visualizations;Dossick, C., Anderson, A. & Azari, R. et al.;2015;Neff, G.;Education, Digital Life and Wellbeing
"Increasingly, communication researchers are issuing calls for attention to the role materiality plays in communication processes (e.g., Boczkowski, 2004; Boczkowski & Lievrouw, 2008; Leonardi & Barley, 2008; Leonardi, Nardi, & Kallinikos, 2013; Lievrouw, 2013). Resulting in part from the challenges of studying new communication and information technologies, this new focus on materiality offers opportunities for communication researchers to theorize beyond communication through, with, and, in some cases, without a medium to think about the material structures of mediation itself. In this chapter we propose a model for thinking through the communicative roles and functions of the materiality of everyday objects, by using one type of objects, documents, as an extended theoretical example of the importance of materiality for communication.";Materiality: Challenges and Opportunities for Communication Theory;Neff, G., Fiore-Silfvast, B. & Dossick, C.;2014;Neff, G.;Education, Digital Life and Wellbeing
I would like to propose a set of methodological and phenomenological entry points into Sonia’s last intellectual charge to communication scholars. Participation and emancipatory knowledges resonate with many of us as admirable values to support in the digital age, but as a discipline we should expand our tool kits to address the “emancipatory visions” that motivate many of us. First, I would argue that we cannot truly understand participation in the digital age without richer theories of the role of materiality in communication. Look no further than the rhetoric of the coming “Internet of Things” to find that how we talk about agency is shifting drastically away from human-centered power and action. The legacy in our field of focusing on meaning making does not help us here, where what counts as communication is black-boxed within complex sociotechnical systems that are intelligible to few and in which human participation is passive at best. We must expand our notion of communicative actors, and a growing number of communication scholars are doing that through attention to the multiple and complex roles for materiality. When material processes have been addressed by our field, it usually has been through the lens of the social and cultural meanings and framings for those objects (Lievrouw, 2014). This leaves us with a paucity of theories to anchor studies of the emerging new types of participation in our civic and social lives and communication networks. How can we think about participation and knowledge in online political discourse when Twitter bots are designed to muddle political conversations, torment candidates, and misdirect debate (Howard, 2014)? What are the implications for equity when posts on Twitter are coded in large-scale big data sets as a proxy for participation in a public as a citizen, as Crawford (2013) has shown with government responses to Hurricane Katrina? My own recent research has been puzzling over the problem of how we can think responsibly about the power of things to structure the possibilities of and for communication in group settings. Within the teams we studied, documents functioned in material ways that were not necessarily aligned with and, indeed, were often orthogonal to their textual and symbolic meanings (Neff, Fiore-Silfvast, & Dossick, 2014). If communication scholars are serious about their commitment to the concept of participation, we must expand our theories and methods for thinking through what participates in our discourse and how action and voice form within those settings. Second, if we are truly committed to understanding participation within global circuits of digital knowledge production, then we must continue to, as Sonia suggests, bridge concepts of larger political economy, cultural/everyday and global/local contexts. We need to push analyses beyond texts’ meaning and significance—admirable goals, of course—and continue to investigate the ways in which institutional contexts and social/organizational settings determine what is communicatively possible. Thomas Streeter’s (2011) work on the ideological foundations that make the rise of the Internet possible is a key example of this kind of research into the social structures that shape communication practices. My own attempt to understand the formation of media workers' professional identities within large-scale economic shifts is another (Neff, 2012). Doing work in communication and media studies on the social structural scale is certainly not easy, but the field will benefit from more studies that use rich evidence drawn from social life to build compelling arguments about the scope of possibilities in which such texts and technologies can reside. Finally, and implied by my other two points, we need expanded theories of communicative agency and power. In many of the scholarly circles I am privy to, the term the digital has come to stand in for and represent a condition or subjectivity akin to modernity—broad enough to evoke sweeping change but too vague in its ability to describe those changes. Carefully teasing apart where agency and power reside in the sociotechnical communicative systems that increasingly shape our lives will help scholars avoid an elitist trap of “digital dualism” in which we pretend “we are in some special, elite group with access to the pure offline” (Jurgenson, 2012, para. 18). Such studies will let us frame what it is to be human—not as possessing some special sort of agency or intelligence that we imagine our systems do not yet have nor formed within some kind of magical, mythical moment of face-to-face, human-to-human, “pure” communication unsullied by mediation. Rather, communication scholars have an urgent responsibility to position our research foci within the sets of strong social structures that have always co-constituted human agency and to recognize our disciplinarily informed ability to name, define, and analyze them, especially as such structures become digital. Doing so will inevitability shift our notions of human subjectivity and agency within such structures, but hopefully will also expand our visions of emancipatory horizons. ;Participations: Dialogues on the Participatory Promise of Contemporary Culture and Politics Part 4: Knowledge and Education ;Bird, E., Couldry, N. & Hepp, A. et al.;2014;Neff, G.;Education, Digital Life and Wellbeing
Data as a discursive concept in and around data‐intensive health and wellness communities evokes multiple social values and social lives for data. Drawing on two years of qualitative, ethnographic observations, participation, and interviews in these communities, our work explores the gap between discourses of data, the practices with and around data, and the contexts in which data “live.” Across the communities of technology designers, “e‐health” providers and advocates, and users of health and wellness data, we find that tensions emerge not around the meaning or legitimacy of particular data points, but rather around how data is expected to perform socially, organizationally and institutionally, what we term data valences. Our paper identifies data valences in health and wellness data, shows how these valences are mediated, and demonstrates that distinct data valences are more apparent in the interstitial interactions occurring in the spaces between institutions or among powerful stakeholder groups.;What we talk about when we talk data: Valences and the social performance of multiple metrics in digital health;Fiore-Silfvast, B. & Neff, G.;2014;Neff, G.;Education, Digital Life and Wellbeing
The biggest challenge for the use of “big data” in health care is social, not technical. Data-intensive approaches to medicine based on predictive modeling hold enormous potential for solving some of the biggest and most intractable problems of health care. The challenge now is figuring out how people, both patients and providers, will actually use data in practice. “I FOUND THE BUZZ AS FEVERISHLY LOUD AROUND HEALTH INFORMATION INNOVATION AS IT WAS DURING MY RESEARCH ON THE FIRST DOT-COM BOOM.” ;Why Big Data Won't Cure Us;Neff, G.;2013;Neff, G.;Education, Digital Life and Wellbeing
Culture Digitally is a collective of scholars, gathered by Tarleton Gillespie (Cornell University) and Hector Postigo (Temple University). With the generous funding of the National Science Foundation, the group supports scholarly inquiry into new media and cultural production through numerous projects, collaborations, a scholarly blog, and annual workshops. For more information on projects and researchers affiliated with Culture Digitally, visit culturedigitally.org or follow @CultureDig).;Affordances, Technical Agency, and the Politics of Technologies of Cultural Production;Neff, G., Jordan, T. & McVeigh-Schultz, J.;2012;Neff, G.;Education, Digital Life and Wellbeing
In the 2012 ASA Communication and Information Technologies Special Issue, we are pleased to bring together eight articles revealing the importance of social engagement on the web. This issue contains research speaking to scholarship dealing with four key topics: governance, exchange, participation, and engagement. Taken together, these eight articles raise far-reaching questions about the intersection of new media and various social formations and processes. Each of these contributions addresses fundamental issues both about the social matrix in which the internet continues to evolve and the effects of technologically mediated communication and interaction on contemporary social life. As a whole, the articles point to the coadaptation between social structures and actions on the one hand and technologically mediated communication on the other hand. This coadaptation process confronts new media scholars with challenging questions about the forces, factors, and processes at play in various domains of economic, cultural, and political practice. The articles remind us that the complex articulations between technology and social life are evolving and changing at a rapid pace along with the tools themselves. Moreover, sometimes this coadaptation fails to forestall consequential conflict and tensions between the technology and the social processes and social actors who are deploying it.;THE SOCIAL MATRIX OF THE EMERGENT WEB: GOVERNANCE, EXCHANGE, PARTICIPATION, & ENGAGEMENT;Neff, G. & Robinson, L.;2012;Neff, G.;Education, Digital Life and Wellbeing
We studied the organizational practices around Building Information Modelling, or BIM, in inter-organizational collaborations among architects, engineers and construction professionals in order to theorize how communi- cation supports technology adoption. Using ethnographic observation and one-on-one interviews with project participants, we observed five teams on three different commercial and institutional building projects that each collaborated over periods of 8–10 months. In this paper, we argue that the dynamic complexity of design and construction processes requires what we call ‘messy talk’—conversations neither about topics on meeting agendas, nor on specified problems or specific queries for expertise. In messy talk interactions, AEC professionals contributed to innovation and project cohesion by raising and addressing issues not known by others. The communicative ‘affordances and constraints’ of BIM structured meeting conversations away from less structured, open-ending problem-solving and towards agenda-driven problem-solving around already identified problems. In other words, using BIM to make information exchange more efficient and effec- tive worked only for certain tasks. We found BIM supports the exchange of explicit knowledge, but not necess- arily informal, active and flexible conversations and exchange of tacit knowledge through messy talk. Although messy talk is perceived as more inefficient, it ultimately makes inter-organizational teams more effective. ;Messy talk and clean technology: communication, problem-solving and collaboration using Building Information Modelling ;Dossick, C. & Neff, G.;2011;Neff, G.;Education, Digital Life and Wellbeing
When can digital artefacts serve to bridge knowledge barriers across epistemic communities? There have been many studies of the roles new information and communication technologies play within organizations. In our study, we compare digital and non-digital methods of inter-organizational collaboration. Based on ethnographic fieldwork on three construction projects and interviews with 65 architects, engineers, and builders across the USA, we find that IT tools designed to increase collaboration in this setting instead solidify and make explicit organizational and cultural differences between project participants. Our study suggests that deeply embedded disciplinary thinking is not easily overcome by digital representations of knowledge and that collaboration may be hindered through the exposure of previously implicit distinctions among the team members’ skills and organizational status. The tool that we study, building information modelling, reflects and amplifies disciplinary representations of the building by architects, engineers, and builders instead of supporting increased collaboration among them. We argue that people sometimes have a difficult time overcoming the lack of interpretive flexibility in digital coordinating tools, even when those tools are built to encourage interdisciplinary collaboration.;A CASE STUDY OF THE FAILURE OF DIGITAL COMMUNICATION TO CROSS KNOWLEDGE BOUNDARIES IN VIRTUAL CONSTRUCTION;Neff, G., Fiore-Silfvast, B. & Dossick, C.;2010;Neff, G.;Education, Digital Life and Wellbeing
Proponents claim that the adoption of building information modeling (BIM) will lead to greater efficiencies through increased collaboration. In this paper, we present research that examines the use of BIM technologies for mechanical, electrical, plumbing, and fire life safety systems (often referred to as MEP) coordination and how the introduction of BIM influences collaboration and communication. Using data from over 12 months of ethnographic observations of the MEP coordination process for two commercial construction projects and interviews with 65 industry leaders across the United States, we find that BIM-enabled projects are often tightly coupled technologically, but divided organizationally. This means that while BIM makes visible the connections among project members, it is not fostering closer collaboration across different companies. We outline the competing obligations to scope, project, and company as one cause for this division. Obligations to an individual scope of work or to a particular company can conflict with project goals. Individual leadership, especially that of the MEP coordinator in the teams we studied, often substitutes for stronger project cohesion and organization. Organizational forces and structures must be accounted for in order for BIM to be implemented successfully.;Organizational Divisions in BIM-Enabled Commercial Construction;Dossick, C. & Neff, G.;2010;Neff, G.;Education, Digital Life and Wellbeing
"For many Americans, the promise of thriving in the new economy has been replaced with the realities of survival. The dot-com boom of the late 1990s marked the coming of age of the much-heralded new economy, an economic, technological, and social transformation that had been in progress for decades. The same entrepreneurial spirit that characterized the stock market frenzy is still expected of high-tech employees. A highly mobile and in many cases highly compensated workforce face a multitude of new risks: Jobs are no longer secure nor insulated from global competition, employer-provided health benefits are drying up, and retirement planning is almost entirely the responsi-bility of employees themselves. These changes are not restricted to the high-tech elite. American workers now face a restructured labor market that asks individuals to bear more responsibility for their jobs, training, and benefits; a global labor market that pushes real wages down; and a broken social contract that replaces the promise of security with the hollow rhetoric of ownership. This book brings together people who are thinking about the challenges that workers face in this new economic environment.";Surviving in the New Economy: Sharecroppers in the Ownership Society;Amman, J., Carpenter, T. & Neff, G.;2006;Neff, G.;Education, Digital Life and Wellbeing
This article compares the work of fashion models and “new media workers” (those who work in the relatively new medium of the Internet as dot-com workers) in order to highlight the processes of entrepreneurial labor in culture industries. Based on interviews and participant-observation in New York City, we trace how entrepreneurial labor becomes intertwined with work identities in cultural industries both on and off the job. While workers are drawn to the autonomy, creativity and excitement that jobs in these media industries can provide, they have also come to accept as normal the high risks associated with this work. Diffused through media images, this normalization of risk serves as a model for how workers in other industries should behave under flexible employment conditions. Using interview data from within the fashion media and the dot-com world, we discuss eight forces that give rise to the phenomenon of entrepreneurial labor: the cultural quality of cool, creativity, autonomy, self-investment, compulsory networking, portfolio evaluations, international competition, and foreshortened careers. We also provide a model of what constitutes the hierarchy of “good work” in cultural industries, and we conclude with implications of what entrepreneurial labor means for theories of work.;Entrepreneurial Labor among Cultural Producers: “Cool” Jobs in “Hot” Industries;Neff, G., Wissinger, E. & Zukin, S.;2006;Neff, G.;Education, Digital Life and Wellbeing
A key challenge to understanding the eruption of globalization protest since the late 1990s is the lack of data on the protesters themselves. Although scholars have focused increasingly on these large protest events and the transnational social movements that play a role organizing them, information about the protesters remains scant. We address this research gap by analyzing survey data collected from a random sample of protesters at five globalization protests in three countries. By disaggregating protesters from the local area and protesters who traveled to the protest event, the role that organizations play becomes clear: SMOs mobilize non-local participants and coordinate travel to protest events. These data also suggest answers to the broader questions that have emerged about global civil society. In contrast to the expectations in recent scholarship, we find very few protesters came from outside of the countries in which the protests were taking place. Instead, we conclude that SMOs use the Internet to connect domestically grounded activists to transnational struggles and to mobilize them to participate in large-scale protest events. In other words, organizations do, indeed, matter in the globalization movement and have significantly expanded the protesting population beyond local citizens.;How Do Organizations Matter? Mobilization and Support for Participants at Five Globalization Protests;Fisher, D., Stanley, K. & Berman, D. et al.;2014;Neff, G.;Education, Digital Life and Wellbeing
This article examines the role of place and placemaking within cultural industries in the digital era. The data for this article are drawn from a data set of attendance at more than nine hundred social networking events over a six-year period in New York City’s Internet, or “new media,” industry. These data confirm that place became more,not less,important to cultural production over this period. Networking, or the processes of the formation of social network ties, is concentrated in activities within narrow geographic clusters. This study suggests that the networking events within the industry—cocktail parties, seminars, ceremonies, and the like—mediate access to crucial resources within the industry.;The Changing Place of Cultural Production: The Location of Social Networks in a Digital Media Industry;Neff, G.;2005;Neff, G.;Education, Digital Life and Wellbeing
The work life in New York's Internet industry points to how employees have adapted to the new norms of flexible work. With an increased emphasis on entrepreneurialism at work–‐including risk‐taking and nonstandard work arrangements–‐work norms in this industry result in individualist solutions to organizational and industrial uncertainty. This trend among professional workers contributes to ideologies that support economic inequality and serves a powerful symbolic function of encouraging flexibility at other levels of the workforce.;Risk Relations: The New Uncertainties of Work;Neff, G.;2004;Neff, G.;Education, Digital Life and Wellbeing
In March 2018, to the surprise of many users, the largest Reddit forums related to darknet markets (DNM) were banned overnight. For users, whose trading activity relied heavily on these forums, the ban was a threat to the community as a whole. In this study we use a complete set of posts from the newly founded forums in the darknet-based “Dread” platform to examine key discussion topics and the sentiment of the community towards the ban. We look at the level of user engagement in the new forums, and the number of users who retain their old usernames. Applying topic modelling to posts on the new forum, we show that there are many overlapping themes across both the banned and new forums, and that discussions on drugs are the most prominent, followed by vendors, shipping reviews, and payment methods. We observe that the new community demonstrates negative sentiment toward the unexpected ban and the loss of accumulated information, but also holds a favourable view of Dread in the hopes that it will offer greater features and security for the users. Users across both platforms express attachment and affinity to the general DNM community, and demonstrate relation to it beyond the commercial purpose.;Into the Dark: A Case Study of Banned Darknet Drug Forums;Cho, S. & Wright, J.;2019;Wright, J.;Information Governance and Security
"Anecdotal evidence suggests an increasing number of people are turning to VPN services for the properties of privacy, anonymity and free communication over the internet. Despite this, there is little research into what these services are actually being used for. We use DNS cache snooping to determine what domains people are accessing through VPNs. This technique is used to discover whether certain queries have been made against a particular DNS server. Some VPNs operate their own DNS servers, ensuring that any cached queries were made by users of the VPN. We explore 3 methods of DNS cache snooping and briefly discuss their strengths and limitations. Using the most reliable of the methods, we perform a DNS cache snooping scan against the DNS servers of several major VPN providers. With this we discover which domains are actually accessed through VPNs. We run this technique against popular domains, as well as those known to be censored in certain countries; China, Indonesia, Iran, and Turkey. Our work gives a glimpse into what users use VPNs for, and provides a technique for discovering the frequency with which domain records are accessed on a DNS server.";Analysing Censorship Circumvention with VPNs Via DNS Cache Snooping;Farnan, O., Wright, J. & Darer, A.;2019;Wright, J.;Information Governance and Security
Censorship of the Internet is widespread around the world. As access to the web becomes increasingly ubiquitous, filtering of this resource becomes more pervasive. Transparency about specific content and information that citizens are denied access to is atypical. To counter this, numerous techniques for maintaining URL filter lists have been proposed by various individuals, organisations and researchers. These aim to improve empirical data on censorship for benefit of the public and wider censorship research community, while also increasing the transparency of filtering activity by oppressive regimes. We present a new approach for discovering filtered domains in different target countries. This method is fully automated and requires no human interaction. The system uses web crawling techniques to traverse between filtered sites and implements a robust method for determining if a domain is filtered. We demonstrate the effectiveness of the approach by running experiments to search for filtered content in four different censorship regimes. Our results show that we perform better than the current state of the art and have built domain filter lists an order of magnitude larger than the most widely available public lists as of April 2018. Further, we build a dataset mapping the interlinking nature of blocked content between domains and exhibit the tightly networked nature of censored web resources.;Automated Discovery of Internet Censorship by Web Crawling;Darer, A., Farnan, O. & Wright, J.;2018;Wright, J.;Information Governance and Security
We develop a means to detect ongoing per-country anomalies in the daily usage metrics of the Tor anonymous communication network, and demonstrate the applicability of this technique to identifying likely periods of internet censorship and related events. The presented approach identifies contiguous anomalous periods, rather than daily spikes or drops, and allows anomalies to be ranked according to deviation from expected behaviour. The developed method is implemented as a running tool, with outputs published daily by mailing list. This list highlights per-country anomalous Tor usage, and produces a daily ranking of countries according to the level of detected anomalous behaviour. This list has been active since August 2016, and is in use by a number of individuals, academics, and NGOs as an early warning system for potential censorship events. We focus on Tor, however the presented approach is more generally applicable to usage data of other services, both individually and in combination. We demonstrate that combining multiple data sources allows more specific identification of likely Tor blocking events. We demonstrate the our approach in comparison to existing anomaly detection tools, and against both known historical internet censorship events and synthetic datasets. Finally, we detail a number of significant recent anomalous events and behaviours identified by our tool.;On Identifying Anomalies in Tor Usage with Applications in Detecting Internet Censorship;Wright, J., Darer, A. & Farnan, O.;2018;Wright, J.;Information Governance and Security
Does recent growth of darknet markets signify a slow reorganisation of the illicit drug trade? Where are darknet markets situated in the global drug supply chain? In principle, these platforms allow producers to sell directly to end users, bypassing traditional trafficking routes. And yet, there is evidence that many offerings originate from a small number of highly active consumer countries, rather than from countries that are primarily known for drug production. In a large-scale empirical study, we determine the darknet trading geography of three plant-based drugs across four of the largest darknet markets, and compare it to the global footprint of production and consumption for these drugs. We present strong evidence that cannabis and cocaine vendors are primarily located in a small number of consumer countries, rather than producer countries, suggesting that darknet trading happens at the »last mile», possibly leaving old trafficking routes intact. A model to explain trading volumes of opiates is inconclusive. We cannot find evidence for significant production-side offerings across any of the drug types or marketplaces. Our evidence further suggests that the geography of darknet market trades is primarily driven by existing consumer demand, rather than new demand fostered by individual markets.;Platform Criminalism: The 'Last-Mile' Geography of the Darknet Market Supply Chain;Dittus, M., Wright, J. & Graham, M.;2018;Wright, J.;"Information Governance and Security; Digital Economies; Information Geography and Inequality"
"Various methods have been proposed for creating and maintaining lists of potentially filtered URLs to allow for measurement of ongoing internet censorship around the world. Whilst testing a known resource for evidence of filtering can be relatively simple, given appropriate vantage points, discovering previously unknown filtered web resources remains an open challenge. We present a novel framework for automating the process of discovering filtered resources through the use of adaptive queries to well-known search engines. Our system applies information retrieval algorithms to isolate characteristic linguistic patterns in known filtered web pages; these are used as the basis for web search queries. The resulting URLs of these searches are checked for evidence of filtering, and newly discovered blocked resources will be fed back into the system to detect further filtered content. Our implementation of this framework, applied to China as a case study, shows the approach is demonstrably effective at detecting significant numbers of previously unknown filtered web pages, making a significant contribution to the ongoing detection of internet filtering as it develops. When deployed, this system was used to discover 1355 poisoned domains within China as of Feb 2017 - 30 times more than in the most widely-used published filter list of the time. Of these, 759 are outside of the Alexa Top 1000 domains list, demonstrating the capability of this framework to find more obscure filtered content. Further, our initial analysis of filtered URLs, and the search terms that were used to discover them, gives further insight into the nature of the content currently being blocked in China.";FilteredWeb: A framework for the automated search-based discovery of blocked URLs;Darer, A., Farnan, O. & Wright, J.;2017;Wright, J.;Information Governance and Security
"One of the primary filtering methods that the Great Firewall of China (GFW) relies on is poisoning DNS responses for certain domains. When a DNS request is poisoned by the GFW, multiple DNS responses are received - both legitimate and poisoned responses. While most prior research into the GFW focuses on the poisoned responses, ours also considers the legitimate responses from the DNS servers themselves. We find that even when we ignored the immediate poisoned responses, the cache from the DNS servers themselves are also poisoned. We also find and discuss the IP addresses within the DNS responses we get; in particular 9 IP addresses that are returned as a result for many different poisoned domains. We present the argument that this type of attack may not be primarily targeted directly at users, but at the underlying DNS infrastructure within China.";Poisoning the Well: Exploring the Great Firewall's Poisoned DNS Responses;Farnan, O., Darer, A. & Wright, J.;2016;Wright, J.;Information Governance and Security
"We present a lightweight key management protocol that provides secured device registration and communication in federated sensor networks. The protocol is designed for zero configuration and use in small packet low power wireless networks; protocol messages may fit into single packets. We use the Casper security protocol analyser to examine the behaviour and security properties of the protocol model. Within the assumptions of the model, we demonstrate forward secrecy, security against man-in-the-middle attacks, and local network key protection, comparing favourably with related protocols. Our experimental analysis shows that the protocol may feasibly be deployed on current sensor platforms with 256-bit elliptic curve cryptography.";KEMF: Key Management for Federated Sensor Networks;O'Hanlon, P., Wright, J. & Brown, I. et al.;2014;Wright, J.;Information Governance and Security
The Minimal-Hitting-Set attack[10] (HS-attack)  is a well-known passive intersection attack against Mix-based anonymity systems, applicable in cases where communication behaviour is non-uniform and unknown. The attack allows an observer to identify uniquely the fixed set of communication partners of a particular user by observing the messages of all senders and receivers using a Mix. Whilst the attack makes use of a provably minimal number of observations, it also requires solving an NP-complete problem. No prior research, to our knowledge, analyses the average complexity of this attack as opposed to its worst case. We choose to explore the HS-attack, as opposed to statistical attacks, to provide a baseline metric and a practical attack for unambiguously identifying anonymous users. We show that the average complexity of the HS-attack can vary between a worst-case exponential complexity and a linear-time complexity according to the Mix parameters. We provide a closed formula for this relationship, giving a precise measure of the resistance of Mixes against the HS-attack in practice, and allowing adjustment of their parameters to reach a desired level of strength.;A Practical Complexity-Theoretic Analysis of Mix Systems;Pham, D., Wright, J. & Kesdogan, D.;2011;Wright, J.;Information Governance and Security
"Changing human behavior is essential for biodiversity conservation, but robust approaches for large scale change are needed. Concepts like repeat message exposure and social reinforcement, as well as mechanisms like online news coverage and targeted advertisements, are currently used by private and public sectors, and could prove powerful for conservation. Thus, to explore their potential in influencing wildlife consumption, we used online advertisements through Facebook, Google, and Outbrain, to promote news articles discussing the use of a Critically Endangered antelope (the Saiga tatarica) as a traditional Chinese medicine in Singapore. Our message, tailored to middle‐aged Chinese Singaporean women, framed saiga horn products as being no longer socially endorsed. Through advert performance and in‐depth analyses of Facebook user engagement, we assessed audience response. Our message pervaded Singapore's online media (e.g., our adverts were shown almost five million times; and the story ran on seven news outlets), and resulted in widespread desirable audience responses (e.g., 63% of Facebook users' engagements included identifiably positive features like calls for public action to reduce saiga horn consumption, anger at having unknowingly used a Critically Endangered species, and self‐pledges to no longer use it; only 13% of engagements included identifiably negative features). This work shows that targeted dissemination of online news articles can have promising results, and may have wide applicability to conservation.";Strategic advertising of online news articles as an intervention to influence wildlife product consumers;Doughty, H., Wright, J. & Veríssimo, D. et al.;2020;Wright, J.;Information Governance and Security
Human factors are increasingly recognised as central to conservation of biodiversity. Despite this, there are no existing systematic efforts to monitor global trends in perceptions of wildlife. With traditional news reporting now largely online, the internet presents a powerful means to monitor global attitudes towards species. In this work we develop a method using the Global Database of Events, Language, and Tone (GDELT) to scan global news media, allowing us to identify and download conservation-related articles. Applying supervised machine learning techniques, we filter irrelevant articles to create a continually updated global dataset of news coverage for seven target taxa: lion, tiger, saiga, rhinoceros, pangolins, elephants, and orchids, and observe that over two- thirds of articles matching a simple keyword search were irrelevant. We examine coverage of each taxa in different regions, and find that elephants, rhinos, tigers, and lions receive the most coverage, with daily peaks of around 200 articles. Mean sentiment was positive for all taxa, except saiga for which it was neutral. Coverage was broadly distributed, with articles from 73 countries across all continents. Elephants and tigers received coverage in the most countries overall, whilst orchids and saiga were mentioned in the smallest number of countries. We further find that sentiment towards charismatic megafauna is most positive in non-range countries, with the opposite being true for pangolins and orchids. Despite promising results, there remain substantial obstacles to achieving globally representative results. Disparities in internet access between low and high income countries and users is a major source of bias, with the need to focus on a diversity of data sources and languages, presenting sizable technical challenges. Tackling these will depend on the development of more advanced natural language tools in a wildlife conservation context, as well as the development of systems to access more, and more diverse, data sources. ;Online Monitoring of Global Attitudes Towards Wildlife ;Wright, J., Lennox, R. & Veríssimo, D.;2007;Wright, J.;Information Governance and Security
"Illegal wildlife trade is gaining prominence as a threat to biodiversity, but addressing it remains challenging. To help inform proactive policy responses in the face of uncertainty, in 2018 we conducted a horizon scan of significant emerging issues. We built upon existing iterative horizon scanning methods, using an open and global participatory approach to evaluate and rank issues from a diverse range of sources. Prioritized issues related to three themes: developments in biological, information, and financial technologies; changing trends in demand and information; and socioeconomic, geopolitical shifts and influences. The issues covered areas ranging from changing demographic and economic factors to innovations in technology and communications that affect illegal wildlife trade markets globally; the top three issues related to China, illustrating its vital role in tackling emerging threats. This analysis can support national governments, international bodies, researchers, and nongovernmental organizations as they develop strategies for addressing the illegal wildlife trade.";Emerging illegal wildlife trade issues: A global horizon scan;Esmail, N., Wintle, B. & Sas-Rolfes, M. et al.;2020;Wright, J.;Information Governance and Security
The Oxford Martin Programme on the Illegal Wildlife Trade (illegalwildlifetrade.net) has launched a key research brief, Evidence to Action: Research to Address Illegal Wildlife Trade (osf.i/reprint/ocarxi/5ndz). This brief, addressed to policy makers and practitioners, outlines areas where research evidence can support effective illegal wildlife trade policy, highlights critical uncertainties where research is required, and emphasizes the need for better design and evaluation of interventions that can help improve the effectiveness of efforts to combat illegal wildlife trade. Tools and expertise to improve the evidence base for national and international illegal wildlife trade policy already exist but are underutilized. Tapping into these resources could produce substantive benefits for wildlife conservation and associated sectors, enabling governments to fulfill their obligations under the Sustainable Development Goals and international biodiversity conventions. This could be achieved through enhanced funding support for inter-sectoral research collaborations, engaging researchers in priority setting and programme design, increasing developing country research capacity and engaging researchers and community voices in policy processes. The Evidence to Action brief is the first of a new set of tools and guidance for researchers and practitioners. The latest addition is a brief reviewing the scale of Darknet Usage in the Illegal Wildlife Trade (osf.i/reprint/ocarxi/gr9d), which includes recommendations for researchers and policymakers. The darknet is a network of websites that can be accessed only via special software that hides the details of the user's connection, and allows websites to be hosted without revealing their location or operator. Large-scale darknet marketplaces exist for illegal drugs, firearms, hacking tools, stolen identity documents, and a wide variety of other illicit goods. However, the darknet has not, to date, proven to be an attractive platform for the buying and selling of illegal wildlife products (see also Oryx, 51, 393–394). Despite this, it provides a marketplace of last resort that becomes increasingly attractive over other, more accessible, online services as law enforcement and platform operators enforce policies against trading in illegal wildlife products. This makes the ongoing study of darknet markets an important avenue for research as other policies against online illegal wildlife trading emerge.;Evidence to action: research to address illegal wildlife trade;Cugniere, L., Wright, J. & Milner-Gulland, E.;2019;Wright, J.;Information Governance and Security
Internet filtering in China is a pervasive and well-reported phenomenon and, as arguably the most extensive filtering regime in the world today, has been studied by a number of authors. Existing studies, however, have considered both the filtering infrastructure and the nation itself as largely homogeneous in this respect. This article investigates variation in filtering across China through direct access to internet services across the country. This is achieved through use of the Domain Name Service (DNS), which provides a mapping between human-readable names and machine-routable internet addresses, and is thus a critical component of internet-based communications. Manipulation of DNS is a common mechanism used by states and institutions to hamper access to internet services that have been deemed undesirable. Our experiments support the hypothesis that, despite typically being considered a monolithic entity, the Chinese filtering approach is better understood as a decentralized and semi-privatized operation in which low-level filtering decisions are left to local authorities and organizations. This article provides a first step in understanding how filtering affects populations at a fine-grained level, and moves towards a more subtle understanding of internet filtering than those based on the broad criterion of nationality. The techniques employed in this work, while here applied to geographic criteria, provide an approach by which filtering can be analysed according to a range of social, economic and political factors in order to more fully understand the role that internet filtering plays in China, and around the world.;Regional variation in Chinese internet filtering;Wright, J.;2013;Wright, J.;Information Governance and Security
We are witnessing the emergence of a ‘planetary labour market’ for digital work. Building on a five-year study of digital work in some of the world’s economic margins, we show a planetary labour market does not do away with geography, it rather exists to take advantage of it. Digital technologies have been deployed in order to bring into being a labour market that can operate at a planetary scale, and has particular affordances and limitations that rarely bolster both the structural and associational power of workers.;The global gig economy: Towards a planetary labour market?;Graham, M. & Anwar, M.;2019;Anwar, M.;"Digital Economies; Information Geography and Inequality"
The rise of the “gig economy” has enabled Internet users to find new work previously unavailable to them. The “online labor platforms”, which constitute a global remote gig economy, enable clients to access worldwide labor power. This chapter discusses how these platforms work. Two hundred and fifty remote gig economy workers across ten countries and four continents have been interviewed along with platform CEOs and government and trade union officials. Moreover, a survey encompassing 679 Asian and African workers has been conducted in addition to an analysis of transaction data and observation studies. The authors conclude by cautioning against having online gig work function as an unregulated labor market and propose some suggestions to improve relationships and conditions between the employing class, the governing class and the working class.;Minimum wages for online labor platforms?;Wood, A., Graham, M. & Anwar, M.;2019;Anwar, M.;"Digital Economies; Information Geography and Inequality"
Much has been written about information and communication technology (ICT) and its development potential in areas such as poverty reduction. Debates are framed around technocentric visions of development. This discourse on ICT-for-development (ICT4D) has opened the way for academic and policy debates surrounding ICT’s potential for development in Africa. By critically engaging with these debates, this chapter entangles key issues around ICT4D on the African continent to show how ICT might be implicated in uneven development. It then adopts the lens of South Africa to cover some ICT and poverty debates and shows that ICT is critical for development in South Africa, particularly at the individual level. The development policy prescription of the current South African government is heavily implicated towards this neoliberal line of thinking. Overall, there remains a need for further research to address how ICT can enable a change in the structural dynamics in the country that are key to poverty and inequality reduction.;Connecting South Africa: ICTs, Uneven Development and Poverty Debates;Anwar, M.;2018;Anwar, M.;"Digital Economies; Information Geography and Inequality"
"Millions of workers around the world join the so-called “sharing economy” every day to perform a variety of jobs. Most of these jobs are digitally mediated through internet-based platforms which connect buyers and sellers of goods and services. However, recent research has begun to highlight the many risks associated with jobs in the sharing economy (Scholz, 2016; Slee, 2016). Many such jobs are characterised by temporary contracts, long and irregular hours, low income, and are often unregulated. The work is highly commoditised, and a global market for this work means that many workers feel they are replaceable, with little bargaining power (Graham et al., 2017a). Workers are made to compete against each other which drives down wages. Thus, many workers will earn below the national minimum wage of their country of location. Since many of these jobs are small “tasks”, clients may have no formal or legal requirement to provide employment benefits to workers. In other words, many sharing economy work practices carry with them various forms of insecurities, and workers typically have less bargaining power than in standard labour markets. These risks are even more pronounced among workers in low and middle-income countries, where our research is situated. In this chapter, we discuss ways in which the sharing economy can contribute towards economic development by making its work practices fairer not just for workers in low and middle-income countries contexts, but also for those in other parts of the world. We first argue that there is a need to reframe work practices in the sharing economy. In some cases, this will mean ensuring that platforms are seen as employers (and workers are seen as employees rather than being seen as being self-employed) in cases where they exert a large amount of control over working lives. Secondly, a better understanding of the important nodes in sharing economy value chains (that is, points of influence and control) can help formulate strategies involving disruption and intervention by labour so that more value is captured for and by workers. This chapter introduces and reviews two models of cooperative working that could work in conjunction with each other to make the sharing economy fairer for workers around the world.";Two Models for a Fairer Sharing Economy;Graham, M. & Anwar, M.;2018;Anwar, M.;"Digital Economies; Information Geography and Inequality"
New forms of digital work have emerged which in theory can be done from anywhere. Does this mean that geography no longer matters to digital work? Not exactly. This chapter draws on our previous empirical research into digital labor to outline how geography still matters, and who it matters for in a world of increasingly digital work. The contemporary geography of digital labor can be used to exploit workers, but we also argue that it opens up distinct possibilities for digital workers to recreate their own worlds of work.;Digital Labour;Graham, M. & Anwar, M.;2018;Anwar, M.;"Digital Economies; Information Geography and Inequality"
The world of work is changing. Communications technologies and digital platforms have enabled some types of work to be delivered from anywhere in the world by anyone with a computer and an internet connection. This digitally-mediated work brings jobs to parts of the world traditionally characterized by low incomes and high unemployment rates. As such, it has been touted by governments, third-sector organizations, and the private sector as a novel strategy of economic development. Drawing on a four-year study with 65 workers in South Africa, Kenya, Nigeria, Ghana and Uganda, we examine the development implications of the gig economy on labour in Africa. We offer four analytical development dimensions through which platform-based remote work impacts the lives and livelihoods of African workers, i.e. freedom, flexibility, precarity and vulnerablity. We argue that these dimensions should be understood in a continuum to better explain the working conditions and lives of workers in the gig economy.;Between a rock and a hard place: Freedom, flexibility, precarity and vulnerability in the gig economy in Africa;Anwar, M. & Graham, M.;2020;Anwar, M.;"Digital Economies; Information Geography and Inequality"
The main aim of this briefing is to make visible the invisible and bring light to the role African workers are playing in developing key emergent and everyday digital technologies such as autonomous vehicles, machine learning systems, next-generation search engines and recommendations systems. Once we acknowledge that many contemporary digital technologies rely on a lot of human labour to drive their interfaces, we can begin to piece together what the new global division of labour for digital work looks like and build a greater socio-political response (both at the global and local scale) to make some of these value chains more transparent, ethical and rewarding.;Digital labour at economic margins: African workers and the global information economy;Anwar, M. & Graham, M.;2020;Anwar, M.;"Digital Economies; Information Geography and Inequality"
In this article, we examine how remote gig workers in Africa exercise agency to earn and sustain their livelihoods in the gig economy. In addition to the rewards reaped by gig workers, they also face significant risks, such as precarious working conditions and algorithmic workplace monitoring, thus constraining workers’ autonomy and bargaining power. Gig workers, as a result, are expected to have fewer opportunities to exert their agency – particularly so for workers in Africa, where the high proportion of informal economy and a lack of employment opportunities in local labour markets already constrain workers’ ability to earn livelihoods. Instead, we demonstrate how remote workers in Africa manage various constraints on one of the world’s biggest gig economy platforms through their diverse everyday resilience, reworking and resistance practices (after Katz, 2004). Drawing from a rich labour geography tradition, which considers workers to ‘actively produce economic spaces and scales’, our main theoretical contribution is to offer a reformulation of Katz’s notions of ‘resistance’, ‘resilience’ and ‘reworking’ as everyday practices of gig workers best understood as ‘hidden transcripts’ of the gig economy (Scott, 1990). The article draws on in-depth interviews (N=65) conducted with remote workers during the fieldwork in five selected African countries.;Hidden transcripts of the gig economy: labour agency and the new art of resistance among African gig workers;Anwar, M. & Graham, M.;2019;Anwar, M.;"Digital Economies; Information Geography and Inequality"
Business process outsourcing (BPO) is seen by the South African government as key to employment generation. BPO operations are reasonably footloose and the industry’s immense focus on workplace control and high levels of attrition means that the development potential of the BPO industry on workers in South Africa is a matter of critical concern. This article uses the global production network (GPN) framework to understand the developmental potentials of contact center jobs for workers in South Africa. The key argument is that economic upgrading among contact center firms can lead to both social upgrading and downgrading among workers.;Does economic upgrading lead to social upgrading in contact centers? Evidence from South Africa;Anwar, M. & Graham, M.;2019;Anwar, M.;"Digital Economies; Information Geography and Inequality"
Connectivity throughout the world is rapidly changing. Nowhere is this more apparent than in Sub-Saharan Africa. The region is quickly moving from a state of digital dis-connectivity, to a state where hundreds of millions of citizens are connected to the digital economy. This rapid change in connectivity has generated a lot of hope and excitement for the potentials of an emergent knowledge economy in the region. Sub-Saharan Africa can, in theory, compete in the production of all manner of digital goods and services with anywhere else in the world. This article surveys the current state of our ongoing multi-year research into the topic, based on empirical research into a range of sectors and domains (including computer code writing, online freelancing, business process outsourcing, and digital entrepreneurship).;Digital Connectivity and African Knowledge Economies;Graham, M., Ojanperä, S. & Anwar, M. et al.;2017;Anwar, M.;"Digital Economies; Information Geography and Inequality"
Special Economic Zones (SEZs) are important vectors of neoliberal globalization in India. Despite facing widespread resistance against the proposed land acquisition for these zones, they are still being promoted across the country. We argue that the wealth redistribution to the country's elites and the fractured resistance movements enable neoliberalism and its practices to grow in the countryside. Using a private sector SEZ in Gurgaon as a case study, this article explores how special economic zoning, as a neoliberal policy, has been implicated in the spatialized production of poverty. We also show that the main actors who promote neoliberalism in India (the state and the large‐scale urban private sector) have found a seemingly unlikely ally in rural India in the form of farmers with large landholdings, rural elites who are willing to let go of their land under certain conditions. The data for the article was collected in India in 2009–10.;Bringing globalization to the countryside: Special Economic Zones in India;Anwar, M. & Carmody, P.;2016;Anwar, M.;"Digital Economies; Information Geography and Inequality"
Two features of the globalising economy are its technological dynamism and its increasing service orientation. International tourism is an important element of the “new economy” of globalisation, as are new information and communication technologies. The relations between these two central elements of the reconfiguring world system have been under-theorised. Based on extensive primary fieldwork in Western Cape Province, South Africa, this paper explores the impact of new information and communication technologies on tourism development. It finds that, while these new technologies are extensively used in marketing and booking, in particular, foreign-owned websites have established a dominant command and control function, thereby replicating previous patterns of economic extraversion.;The Diffusion and Impacts of Information and Communication Technology on Tourism in the Western Cape, South Africa;Anwar, M., Carmody, P. & Surborg, B. et al.;2014;Anwar, M.;"Digital Economies; Information Geography and Inequality"
Foreign direct investments (FDI) into Africa from developing economies have grown substantially over the past decade. While the focus of the enquiry among the geographers has been the rise of Chinese investments in Africa, India has become an important ‘Asian driver’ within the ‘new scramble for Africa’. This article highlights the geography of Indian involvement in Africa in terms of its growing scale, new patterns and the emerging complex structure of Indian investments. The article finds that the nature of India-Africa trade relationship mirrors colonial trade relationships between India and the Great Britain. The Indian investments in Africa are resource- oriented and fused with geopolitical dynamics, driven by capitalistic agendas.;Indian foreign direct investments in Africa: a geographical perspective;Anwar, M.;2014;Anwar, M.;"Digital Economies; Information Geography and Inequality"
Special economic zones in India have gained prominence among the policy making circles in recent years. The argument by the policy makers was that these zones will allow industrialisation in India. This article reviews the emerging geography of SEZs (special economic zones) in India and the Indian government recent experiment with the SEZs as models of economic development. The article argues that current SEZ policy in India is designed along the lines of mainstream economic strategy for industrialisation of Washington Consensus, i.e. open economy with greater market freedom coupled with minimal government intervention leads to rapid economic growth and rising incomes. The evidence suggests that these zones are giving rise to uneven geographical development in India with certain regions, sectors and classes are deriving the benefits from this policy.;New modes of industrial manufacturing: India’s experience with special economic zones;Anwar, M.;2014;Anwar, M.;"Digital Economies; Information Geography and Inequality"
The South African government believes that the business processing industry is key to job creation. And the sector has indeed created thousands of jobs, in particular for the country’s youth – a group that bears a disproportionate share of joblessness. But unlike in India, the growth of South Africa’s business process outsourcing sector has not generated the kind of jobs that offer workers the chance of progressing up the career ladder. In South Africa, workers tend to get stuck in the same, low-level and poorly paid jobs with very few prospects of promotion or career development. Their working conditions are also generally poor. South Africa’s outsourcing industry grew significantly between 1997, when it hosted an estimated 185 contact centre operations, to 2010, when the figure was well over 1000. More recent data isn’t available. But data from 2017 estimated that 228 642 people were employed in the sector. Most business process services jobs such as in the call and contact centres are taken up by young workers. In the context of the country’s high youth unemployment of more than 52%, government sees the sector as one vehicle for absorbing young workers. However, the sector’s failure to create opportunities for career advancement is proving a trap for young people.;Most call centre jobs are a dead end for South Africa’s youth;Anwar, M.;2019;Anwar, M.;"Digital Economies; Information Geography and Inequality"
"The UK government’s recent review of modern employment practices defined gig economy as ‘people using apps [or platforms] to sell their labour’. An estimated 50-60 million workers are part of the global gig economy. The Online Labour Index, developed by researchers at the Oxford Internet Institute, University of Oxford, suggests that the demand for gig work or platform work grew by about 22 per cent since 2016. At the heart of the rapid growth of the gig economy (both place-based gig work, such as food delivery, care work and taxis; and remote gig work, like transcription, video editing, software development, and data entry) is the proliferation of internet-based apps or platforms, which facilitate most transactions in gig work. For the last three years, I have been studying platform-based workers in Africa, many of whom work well over 60 hours a week, some even working 100 hours weekly to make a living. While wages from platforms are higher for some workers compared to local alternatives, long, unsociable and unpredictable working hours, physical exhaustion and stress are also common. Many similar accounts from the experiences of platform workers in the UK give a disturbing picture of the global gig economy.";How Marx predicted the worst effects of the gig economy more than 150 years ago;Anwar, M.;2018;Anwar, M.;"Digital Economies; Information Geography and Inequality"
Faced with a growing economic crisis, South Africa’s new Finance Minister, Malusi Gigaba, has come up with a 14 point plan to turn the country’s economic fortunes around. Sibonelo Radebe asked Mohammad Amir Anwar to assess the plan.;Obsession with growth won’t help South Africa’s economic recovery;Anwar, M.;2017;Anwar, M.;"Digital Economies; Information Geography and Inequality"
The role of “white monopoly capital” in post-apartheid South Africa has been in the news lately. In the South African context, it can be understood as the white population’s extensive control over the country’s economy. The debate reflects a recanting view against the rainbow nation dream sold when the country gained political freedom 22 years ago. The idea is that white monopoly capital is the source of the problem of multiple failures of the South African political economy. The response has been a rising chorus of white monopoly capitalism deniers who argue that the governing African National Congress (ANC) is using the concept as a shield against criticism. Instead of addressing its failings such as a faltering economy, widening inequality, unemployment, corruption and incompetence, the argument goes, the ANC is deflecting attention for the country’s difficulties by blaming white monopoly capital. Some in this camp add that South Africa has recorded significant progress in redistributing the country’s wealth, mainly via the allocation of equity in formerly white companies to black economic empowerment groups. They quote figures that they say reflects rising levels of black ownership on the Johannesburg Stock Exchange.;White people in South Africa still hold the lion’s share of all forms of capital;Anwar, M.;2017;Anwar, M.;"Digital Economies; Information Geography and Inequality"
The global food price crises between 2008 and 2009 led countries that bore the brunt of the catastrophe to look elsewhere for agricultural land to mitigate the effects. In 2008 prices of some foods, including wheat, soared by 130% in a single year and the United Nation’s Food and Agriculture Organisation’s food price indexshot up 40%. The result was a frenzied scramble that saw countries acquire an estimated 40 million hectares of land in foreign countries, most of it in Africa. A great deal of attention has been paid to the role of the US, the largest investor in land in the world, China and Middle Eastern countries. Much less attention has been given to the role of India. A global land monitoring initiative, Land Matrix, ranks India as one of the top 10 investors in land abroad. It is the biggest investor in land in Ethiopia, with Indian companies accounting for almost 70% the land acquired by foreigners after 2008. Indian land deals in Ethiopia are the result of the strong convergence in the two countries’ domestic political-economic policies. Both advocate the privatisation of public assets and increasing reliance on free trade and open markets. India’s investment in land has been driven by the need to obviate the effects of spiralling food prices by outsourcing food supply. Ethiopia’s decisions are driven by its development policy based on commercialisation of agriculture and reliance on foreign investments. Rough estimates suggest Indian firms have acquired roughly 600 000 hectares of land in Ethiopia. This is more than ten times the size of land acquired by firms in India under the country’s special economic zones policy. India is followed closely by Saudi Arabian firms, with 500 000 hectares of land, in Ethiopia.;The lesser known story of India’s role in Ethiopian land deals;Anwar, M.;2015;Anwar, M.;"Digital Economies; Information Geography and Inequality"
Brazil, Russia, India and China have shown tremendous determination and co-operation on global issues in recent years. Their alliance epitomises south-south co-operation in the contemporary era. This is the notion of solidarity among developing countries through the exchange of goods, resources, technology and knowledge to meet their development goals. But what does this all mean for Africa? Is this bloc of countries committed to Africa or simply driven by self-interest? For the purposes of this article I refer to the bloc known as BRICS but I am excluding South Africa. The reason for this is that South Africa was a late addition to the group and was brought in to complete the regional representativity of the group.;Why south-south co-operation is a myth when it comes to BRICS and Africa;Anwar, M.;2015;Anwar, M.;"Digital Economies; Information Geography and Inequality"
"The crisis in Ukraine, as Russian troops apparently occupy Crimea and threaten its borders with the rest of the country, has sent strong tremors through the international diplomatic community. Both Barack Obama and Angela Merkelhave made lengthy phone calls to Vladimir Putin to urge restraint, and to make him aware of “consequences” if Ukraine’s sovereignty isn’t respected. As to what consequences Russia’s military incursion into Crimea may have, we cannot yet be sure. There is talk of boycotting the G8 meeting in Sochi later this year; there is also talk of expelling Russia from the G8 altogether in favour of a G7. Meanwhile, the EU’s high representative for foreign affairs, Catherine Ashton, is heading to Ukraine this week. EU sanctions are being discussed, and the IMF will visit Kiev to discuss a financial aid package to the interim government. But what of the United Nations? What action can the UN take to help defuse the situation? To get a sense of whether or not the UN can achieve anything, we only have to look to Syria – where, after three years, a nightmarish conflict still rages at a cost of more than 140,000 people killed, millions exiled, and millions more internally displaced. There are numerous other examples of conflicts shaped by the military interventions of the US or Russia: Iraq, Afghanistan, Nicaragua, Chile, Iran, Vietnam and Kosovo, to name a few. The UN has been spectacularly unable to deal with those as well. At the heart of its failing are the organisation’s power structure, and the problem of enforcing member states’ accountability to international law – and the institutional failings of the UN Security Council (UNSC).";UN Security Council’s failure stretches from Syria to Crimea;Anwar, M.;2014;Anwar, M.;"Digital Economies; Information Geography and Inequality"
This paper addresses cross-boundary coordination in a multiparty collaboration. So far, collaboration among among multiple dispersed parties has received scant attention in research on cross-boundary coordination. Building on this gap, this study analyzes an extreme case of inter-organizational collaboration between four geographically dispersed groups of engineers from subsidiaries of a Japanese multinational and an American engineering contractor. We explain how coordination is achieved among multiple parties. In our study, diverse boundaries posed challenges to the execution of work tasks being performed. In response, collaborating parties developed four organizing processes for coordinating their task-related activities, comprising information sharing, task negotiation, task execution and task integration. We suggest that together, these processes constitute a dynamic coordinating structure that is developed and enacted in parties' everyday collaborating and coordinating activities, which may enable but can also impede the successful execution of joint work tasks.;10,000 miles across the room?: emergent coordination in multiparty collaboration;Corporaal, G.., Ferguson, J. & de Gilder, D. et al.;2014;Corporaal, G.;Digital Economies
A wide range of platforms now enable firms to source work directly from freelancers on an on-demand basis, but little is known about how and why rms are making use of such platforms. We conducted nine case studies to examine how Fortune 500 firms and multinational enterprises are adopting platform sourcing as part of their business models. Suggested citation: Corporaal, G.F., & Lehdonvirta, V. (2017). Platform Sourcing: How Fortune 500 Firms are Adopting Online Freelancing Platforms.;Platform Sourcing: How Fortune 500 Firms Are Adopting Online Freelancing Platforms;Corporaal, G. & Lehdonvirta, V.;2017;Corporaal, G.;"Digital Economies; Information Geography and Inequality"
Wikipedia is one of the predominant ways in which internet users obtain knowledge about the world. It is also one of the most important mirrors, or augmentations, of the world: it contains representations of all manner of places. However, Wikipedia’s knowledge of the world is characterised by a linguistic inequality. Although it is written in a growing number of languages, some languages are overrepresented and contribute significantly more to Wikipedia’s body of knowledge than others. This deeply affects how the world is represented on Wikipedia, and by whom: it has been shown that for many countries in the Global South, there are more articles written in English than in their respective native languages. As a result, a significant number of people are being excluded from the collective process of knowledge production, solely on the basis of their native language. Who writes these representations of local places, and for which audiences? We present early findings from the first study of Wikipedia’s geolinguistic contours. We investigate to what extent local languages are involved in the process of creating local representations. In a large-scale quantitative analysis across the almost 300 language versions of Wikipedia, we identify regions of the world where local languages such as Armenian, Catalan or Malay are dominant sources of representation for local places, and we contrast these findings with instances where representations are significantly shaped by foreign languages. Where do, and do not, we see significant amounts of local content available in local languages? Where are the most detailed local representations largely written in foreign languages, intended for foreign audiences? And what factors can explain this?;Mapping Wikipedia’s Geolinguistic Contours;Dittus, M. & Graham, M.;2019;Dittus, M.;"Digital Economies; Information Geography and Inequality"
Technology innovation hubs (TIHs), an emerging phenomenon in developing countries, offer a combination of business incubation services and open convening spaces, events, and innovation competitions. To understand TIHs’ roles and identify pathways for future research, I analyzed data from interview and focus groups with 220 stakeholders of seven TIHs in five countries, using an informed grounded theory approach. Theory from economic geography (on regional advantage, regional innovation systems, and clusters) provided the most useful perspectives, which I related to themes emerging from the data. I found that TIH stakeholders saw themselves and the TIH as part of an “innovation ecosystem,” with ecosystems growing and maturing in distinct patterns over time. Ecosystem quality was understood as the completeness of complementary actor groups. TIHs functioned predominantly as linkage builders between ecosystem actors and actor groups. TIHs promoted innovation ecosystems in particular by creating visibility for small enterprise and individual innovators, by leveraging and pooling larger organizations’ resources, and by feeding “buzz” in the ecosystem. TIHs had multi-stakeholder governance and engagement models, leading to both advantages and frictions. For future research, I propose a theoretical framework of innovation networking, integrating literature from economic geography and network science. I conclude that future TIH analysis ought to examine TIHs’ shaping of innovation networks (as the relational structure of innovation ecosystems) for the largest potential theoretical and empirical contributions.;More Art than Science? Exploring the Roles of Technology Innovation Hubs for Urban Regions in Developing Countries;Friederici, N.;2014;Friederici, N.;"Digital Economies; Information Geography and Inequality"
Knowledge Portals (KPs) are highly integrative Knowledge Management Systems (KMS) that promise to synthesize widely dispersed knowledge and to interconnect individuals by functioning as a ‘one-stop knowledge shop’. Yet, in practice, KPs face major challenges, which are for the most part due to the intricacies of knowledge exchange being subjected to multi- faceted individual and social factors. At the same time, growing anecdotal evidence from case studies indicates KPs’ enormous potential. This paper makes an effort to more distinctly conceptualize KPs and emphasize a KP’s role to unify networking and repository KMS features. The paper develops three major challenges to successful KP deployment, namely(1) knowledge integration, (2) sufficient participation, and (3) favorable organizational culture and validates these as applicable to KP through a review of 42 empirical papers. The paper concludes with suggestions towards a set of design principles for KP. ;Integrated Customer-Focused Knowledge Portals: Design Challenges and Empirical Approaches ;Loebbecke, C., Crowston, K. & Friederici, N.;2011;Friederici, N.;"Digital Economies; Information Geography and Inequality"
Innovation hub organizations have become a fixture in African cities. Proponents of hubs commonly envision them to function as network infrastructures. Specifically, hubs are conceived of as network intermediaries within entrepreneurial ecosystems, allowing for seamless collaboration between diverse actors. This paper grounds such visions and conceptualizations, presenting case studies of two hubs in Rwanda: kLab and The Office. It draws on interviews with 47 participants with a stake in technology entrepreneurship in Kigali, about half of them founders or CEOs of small technology startups. Ultimately, the case studies show that implementation realities of hubs are far removed from aspirational visions. Notably, the paper finds that facets of community (such as boundaries and cultures) need to be continually negotiated between hub leaders and entrepreneurs, resulting in tensions and tradeoffs.;GROUNDING THE DREAM OF AFRICAN INNOVATION HUBS: TWO CASES IN KIGALI;Friederici, N.;2018;Friederici, N.;"Digital Economies; Information Geography and Inequality"
Digital enterprises from the US, Europe, and East Asia have been recognized for their potential to achieve global market reach, and for forming a globalized digital infrastructure. However, digital enterprises from economically peripheral countries have usually remained local. This paper seeks to understand the enterprise-level reasons for these global differences. Drawing on in-depth interviews with founders, we empirically examine the value creation and geographical market scope of 73 digital enterprises in Lagos, Nairobi, Accra, and Kigali. We develop theory that explains why enterprises in global economic peripheries are able to exploit some but not all opportunities of digital technologies. In contrast to the claim in current scholarship that digital enterprises can operate in relatively unbounded ways, we find that African enterprises cannot compete in global digital markets and are ultimately compelled to offer localized digital products. Based on these findings, we theorize that digital products with the greatest global scaling potential are the least likely to be owned and controlled by digital enterprises located in economic peripheries. We thus encourage scholars of digital enterprise to more carefully take geographical variation into account, and acknowledge technological drivers of increasing unevenness in the global digital economy.;The Bounded Opportunities of Digital Enterprises in Global Economic Peripheries;Friederici, N. & Graham, M.;2018;Friederici, N.;"Digital Economies; Information Geography and Inequality"
In recent years, Internet connectivity has greatly improved across the African continent. This article examines the consequences that this shift has had for East African firms that are part of global value chains (GVCs). Prior work yielded contradictory expectations: firms might benefit from connectivity through increased efficiencies and improved access to markets, although they might also be further marginalized through increasing control of lead firms. Drawing on extensive qualitative research in Kenya and Rwanda, including 264 interviews, we examine 3 sectors (tea, tourism, and business process outsourcing) exploring overarching, cross-cutting themes. The findings support more pessimistic expectations: small African producers are only thinly digitally integrated in GVCs. Moreover, shifting modes of value chain governance, supported by lead firms and facilitated by digital information platforms and data standards are leading to new challenges for firms looking to digitally integrate. Nevertheless, we also find examples in these sectors of opportunities where small firms are able to cater to emerging niche customers, and local or regional markets. Overall, the study shows that improving connectivity does not inherently benefit African firms in GVCs without support for complementary capacity and competitive advantages.;Digital Control in Value Chains: Challenges of Connectivity for East African Firms;Foster, C., Graham, M. & Mann, L. et al.;2017;Friederici, N.;"Digital Economies; Information Geography and Inequality"
Corporations, development organisations and governments have launched ambitious programmes to ‘connect the unconnected’, reasoning that this creates economic growth and inclusive development. This paper contrasts these actors’ discourses with evidence from academic research. The evidence suggests a highly uneven economic impact of Internet connectivity across geographies and social strata. The analysed sources of discourse (African ICT policies and reports by international organisations) instead propose Grand Visions of connectivity, attributing a self‐evident positive, widespread, and transformational impact to the Internet. We discuss technological determinism, acontextual modernism, and optimistic simplism as underlying this contrast, calling for more reflexivity towards the opportunities of ‘digital development’.;The Impact of Connectivity in Africa: Grand Visions and the Mirage of Inclusive Digital Development;Friederici, N., Ojanperä, S. & Graham, M.;2017;Friederici, N.;"Digital Economies; Information Geography and Inequality"
The arrival of East Africa’s first fibre optic cables in 2009 promised a new beginning for Kenya’s economic relationship with the rest of the world. Many hoped the faster and more reliable internet connectivity would boost the country’s nascent Business Processing Out-sourcing (BPO) sector and create 20,000 direct jobs and 10 Billion Kenyan shillings by 2030 (Graham and Mann 2013). In addition to early invest-ment into one of the four cables, the Kenyan gov-ernment called for the creation of a 7,500 seat BPO Park at the Athi River EPZ, an aggressive marketing campaign, targeted training and the establishment of a new Kenya ICT Board to help guide the development of the sector.;Growing the Kenyan Business Process Outsourcing Sector;Graham, M., Mann, L. & Friederici, N. et al.;2016;Friederici, N.;"Digital Economies; Information Geography and Inequality"
"Platforms in the urban environment are fundamentally unaccountable. They present themselves as too big to control, too new to regulate, and too innovative to stifle, and remain un-democratic, and usually distant, organizations with no interest in promoting local voices or investing in local priorities. This paper argues that platforms control urban interactions whilst remaining unaccountable through a strategic deployment of ‘conjunctural geographies’ – a way of being simultaneously embedded and disembedded from the space-times they mediate. These conjunctural geographies, however, render platforms vulnerable. The ephemeral nature of platforms means we can avoid them, circumvent them and replicate them; their material nature suggests points of regulation and resistance. The paper closes by pointing to three broad strategies —regulate, replicate, and resist - which can be deployed to build alternate platform futures. Each of which is built on understanding the simultaneously embedded and disembedded ways in which platforms occupy their conjunctural geographies.";Regulate, replicate, and resist – the conjunctural geographies of platform urbanism;Graham, M.;2020;Graham, M.;"Digital Economies; Information Geography and Inequality"
This paper introduces the Fairwork Foundation, a research initiative that is also developing an intervention around the quality of work on digital labour platforms. Lacking the ability to collectively bargain, many platform workers have little ability to negotiate wages or working conditions with their employers. As a result of this new, digitally-managed market for work, many workers have jobs characterised by long and irregular hours, low income, and high stress. Fairwork's field research across India and South Africa finds challenges for workers across a range of issues which form the basis for a set of decent work principles on: pay, conditions, contracts, management, and representation. The results of the field research are being used to rank and compare platforms against these principles as a means to encourage decent, and discourage ‘un-decent’ platform work.;The Fairwork Foundation: Strategies for improving platform work in a global context;Graham, M., Woodcock, J. & Heeks, R. et al.;2020;Graham, M.;"Digital Economies; Information Geography and Inequality"
In the digital economy, innovation processes increasingly rely on highly specialised know-how and open-source software shared on digital platforms on collaborative programming. The information that feeds into the content on these platforms is provided voluntarily by a vast crowd of knowledgeable users from all over the world. In contributing to the platforms, users invest their time and share knowledge with strangers to add to the rising body of digital knowledge. This requires an open mindset and trust. In this study, we argue that such a mindset is not just an individual asset but determined by the local communities the users are embedded in. We, therefore, hypothesise that places with higher levels of trust should contribute more to Stack Overflow, the world's largest question-and-answer platform for programming questions. In relating the city-level contributions of 266 OECD metropolitan areas to infrastructure, economic, and trust measures, we find support for this hypothesis. In contrast, click rates to the platform are solely driven by infrastructure and economic variables, but not by trust. These findings highlight the importance of societal values in the twenty-first-century knowledge economy: if policymakers want to develop a lively local digital economy, it is not enough to provide fast Internet access and business opportunities. Instead, it is equally important to establish a trust-building environment that fosters sharing of innovative ideas, collaborations, and knowledge spillovers.;Coding together – coding alone: the role of trust in collaborative programming;Stephany, F., Braesemann, F. & Graham, M.;2020;Graham, M.;"Digital Economies; Information Geography and Inequality"
To understand the dynamics of the digital knowledge economy, it is crucial to reveal the geography of global flows of knowledge on digital platforms. This article visualizes a key form of knowledge production in the digital economy: mapping the joint collaborations of users from different cities on Stack Overflow, the world’s most popular question-and-answer website for programming questions. The network map reveals that users from only a limited number of places are actively taking part in the exchange of programming knowledge. While Stack Overflow access and participation are theoretically unrestricted, contributions are clustered in metropolitan regions in North America, Western Europe, and South Asia.;Global networks in collaborative programming;Braesemann, F., Stoehr, N. & Graham, M.;2019;Graham, M.;"Digital Economies; Information Geography and Inequality"
We propose the construction of a Digital Knowledge Economy Index, quantified by way of measuring content creation and participation through digital platforms, namely the code sharing platform GitHub, the crowdsourced encyclopaedia Wikipedia, and Internet domain registrations and estimating a fifth sub-index for the World Bank Knowledge Economy Index for year 2012. This approach complements conventional data sources such as national statistics and expert surveys and helps reflect the underlying digital content creation, capacities, and skills of the population. An index that combines traditional and novel data sources can provide a more revealing view of the status of the world’s digital knowledge economy and highlight where the (un)availability of digital resources may actually reinforce inequalities in the age of data.;The Digital Knowledge Economy Index: Mapping Content Production;Ojanperä, S., Graham, M. & Zook, M.;2019;Graham, M.;"Digital Economies; Information Geography and Inequality"
This article investigates the (dis)embeddedness of digital labour within the remote gig economy. We use interview and survey data to highlight how platform workers in Southeast Asia and Sub-Saharan Africa are normatively disembedded from social protections through a process of commodification. Normative disembeddedness leaves workers exposed to the vagaries of the external labour market due to an absence of labour regulations and rights. It also endangers social reproduction by limiting access to healthcare and requiring workers to engage in significant unpaid ‘work-for-labour’. However, we show that these workers are also simultaneously embedded within interpersonal networks of trust, which enable the work to be completed despite the low-trust nature of the gig economy. In bringing together the concepts of normative and network embeddedness, we reconnect the two sides of Polanyi’s thinking and demonstrate the value of an integrated understanding of Polanyi’s approach to embeddedness for understanding contemporary economic transformations.;Networked but Commodified: The (Dis)Embeddedness of Digital Labour in the Gig Economy;Wood, A., Graham, M. & Lehdonvirta, V. et al.;2019;Graham, M.;"Digital Economies; Information Geography and Inequality"
The rise of the “gig economy” has enabled Internet users to find new work previously unavailable to them. For this purpose, “online labor platforms” have been set up, which constitute a global remote gig economy and enable clients to access world-wide labor power. This chapter discusses how these platforms work and to this end 250 remote gig economy workers across ten countries and four continents have been interviewed along with platform CEOs and government and trade union officials. Moreover, a survey encompassing 679 Asian and African workers has been conducted in addition to an analysis of transaction data and observation studies. The authors conclude by cautioning against having online gig work function as an unregulated labor market and propose some suggestions to improve relationships and conditions between the employing class, the governing class, and the working class.;Minimum Wages for Online Labor Platforms?: Regulating the Global Gig Economy;Wood, A., Graham, M. & Anwar, M.;2019;Graham, M.;"Digital Economies; Information Geography and Inequality"
More than one billion people over the last five years have become new Internet users. Digital connectivity was once confined to economically prosperous parts of the world, but now Internet users make up a majority of the world’s population. In this book, contributors from a range of disciplines and locations investigate the impact of increased digital connectivity on the people and places at the world’s economic margins. Does a digitalized economy mean that those in economic peripheries can transcend spatial, organizational, social, and political constraints — or do digital tools and techniques tend to reinforce existing inequalities? Reporting on digitalization in countries ranging from Chile to Kenya to the Philippines, the contributors present a diverse set of case studies and develop a broad range of theoretical positions. They consider, among other things, data-driven disintermediation, women’s economic empowerment and gendered power relations, digital humanitarianism and philanthropic capitalism, the spread of innovation hubs, hackathons, the gig economy, and a rethinking of how a more progressive politics of connectivity could look.;Digital Economies at Global Margins;Graham, M. (ed.);2019;Graham, M.;"Digital Economies; Information Geography and Inequality"
Rights are always bounded, granted, bestowed, enjoyed, received, performed, and violated in real, physical places. The chapter argues that just because actions or interactions happen ‘online’ – or mediated by digital networks – does not mean that they happen in any sort of alternate sphere or space beyond the laws, norms and principles that apply and are practiced elsewhere. It posits that we have often taken a wrong turn in discussions about digital politics and rights by relying on inaccurate and unhelpful spatial metaphors. In particular, the chapter focuses on the usage of the ‘cyberspace’ metaphor and outlines why the reliance by con- temporary policy-makers on this inherently geographic metaphor matters. The metaphor constrains, enables and structures very distinct ways of imagining the interactions between people, information, code and machines through digital networks. These distinct imaginations, in turn, have real effects on how we enact politics and bring places into being. The chapter traces the history of ‘cyberspace’, explores the scope of its current usage, and highlights the discursive power of its distinct way of shaping our spatial imagination of the Internet. It then concludes by arguing that we should take the lead in employing alternate, nuanced and spatially grounded ways of envisioning the myriad ways in which the Internet mediates social, economic and political experiences.;There are No Rights ‘in’ Cyberspace;Graham, M.;2019;Graham, M.;"Digital Economies; Information Geography and Inequality"
This article evaluates the job quality of work in the remote gig economy. Such work consists of the remote provision of a wide variety of digital services mediated by online labour platforms. Focusing on workers in Southeast Asia and Sub-Saharan Africa, the article draws on semi-structured interviews in six countries (N = 107) and a cross-regional survey (N = 679) to detail the manner in which remote gig work is shaped by platform-based algorithmic control. Despite varying country contexts and types of work, we show that algorithmic control is central to the operation of online labour platforms. Algorithmic management techniques tend to offer workers high levels of flexibility, autonomy, task variety and complexity. However, these mechanisms of control can also result in low pay, social isolation, working unsocial and irregular hours, overwork, sleep deprivation and exhaustion.;Good Gig, Bad Gig: Autonomy and Algorithmic Control in the Global Gig Economy;Wood, A., Graham, M. & Lehdonvirta, V. et al.;2018;Graham, M.;"Digital Economies; Information Geography and Inequality"
This article presents findings regarding collective organisation among online freelancers in middle‐income countries. Drawing on research in Southeast Asia and Sub‐Saharan Africa, we find that the specific nature of the online freelancing labour process gives rise to a distinctive form of organisation, in which social media groups play a central role in structuring communication and unions are absent. Previous research is limited to either conventional freelancers or ‘microworkers’ who do relatively low‐skilled tasks via online labour platforms. This study uses 107 interviews and a survey of 658 freelancers who obtain work via a variety of online platforms to highlight that Internet‐based communities play a vital role in their work experiences. Internet‐based communities enable workers to support each other and share information. This, in turn, increases their security and protection. However, these communities are fragmented by nationality, occupation and platform.;Workers of the Internet unite? Online freelancer organisation among remote gig economy workers in six Asian and African countries;Wood, A., Lehdonvirta, V. & Graham, M.;2018;Graham, M.;"Digital Economies; Information Geography and Inequality"
Global online platforms match firms with service providers around the world, in services ranging from software development to copywriting and graphic design. Unlike in traditional offshore outsourcing, service providers are predominantly one-person microproviders located in emerging-economy countries not necessarily associated with offshoring and often disadvantaged by negative country images. How do these microproviders survive and thrive? We theorize global platforms through transaction cost economics (TCE), arguing that they are a new technology-enabled offshoring institution that emerges in response to cross-border information asymmetries that hitherto prevented microproviders from participating in offshoring markets. To explain how platforms achieve this, we adapt signaling theory to a TCE-based model and test our hypotheses by analyzing 6 months of transaction records from a leading platform. To help interpret the results and generalize them beyond a single platform, we introduce supplementary data from 107 face-to-face interviews with microproviders in Southeast Asia and Sub-Saharan Africa. Individuals choose microprovidership when it provides a better return on their skills and labor than employment at a local (offshoring) firm. The platform acts as a signaling environment that allows microproviders to inform foreign clients of their quality, with platform-generated signals being the most informative signaling type. Platform signaling disproportionately benefits emerging-economy providers, allowing them to partly overcome the effects of negative country images and thus diminishing the importance of home country institutions. Global platforms in other factor and product markets likely promote cross-border microbusiness through similar mechanisms.;The Global Platform Economy: A New Offshoring Institution Enabling Emerging-Economy Microproviders;Lehdonvirta, V., Kässi, O. & Hjorth, I. et al.;2018;Graham, M.;"Digital Economies; Information Geography and Inequality; Digital Knowledge and Culture"
"Increasingly greater numbers of people are now using apps, platforms, and websites to find and perform jobs. There are at least seven million platform workers that live all over the world, doing work valued at US$5 billion per year outsourced via platforms or apps (Kuek et al. 2015; Heeks 2017). Eleven percent of the labour force in the United Kingdom have already earned income from digital labour platforms (Huws and Joyce 2016), and it is predicted that by 2025, one third of all labour transactions will be mediated by digital platforms (Standing 2016). The common feature of all digital labour platforms is that they offer tools to bring together the supply of, and demand for, labour. The functions fulfilled by digital labour platforms vary greatly. In some cases, they simply become new intermediaries for existing services. In others, they facilitate new jobs and skills (Drahokoupil and Fabo 2016). And, in yet others, they fragment labour processes temporally and spatially in order to allow for fundamentally reconfigured economic geographies of work (Graham and Anwar 2018).";Towards a Fairer Platform Economy: Introducing the Fairwork Foundation;Graham, M. & Woodcock, J.;2018;Graham, M.;"Digital Economies; Information Geography and Inequality"
Information‐technologies are essential for global capitalism to function at speed across scale, space and complexity. The importance of software and algorithms in the governance of these systems is reflected in the attention of scholars to the ways digital code and materiality (re)combine to create hybrid digital/material spaces of economic activity, movement and everyday life. This paper extends this work in two key ways: first by emphasising the relational aspect of these code/spaces, and second by showing how the digital algorithms of code/spaces are hackable rather than hegemonic. Using the case study of frequent flyer programmes we demonstrate how networked knowledge‐sharing reshapes code/spaces to provide unintended opportunities such as low‐cost travel and access to spaces normally only frequented by global elites. Although this case highlights vulnerabilities in the code of global capitalism, it is primarily only those with significant lifestyle privilege who are able to fully participate in these subversions. Moreover, while much of the value captured by “airline hackers” comes from airlines’ profit margins, the relationality of code/spaces can impact citizens and consumers not directly connected to or interested in airline travel. Ultimately, this paper demonstrates that in contrast to characterisations of omnipresence and hegemony, the information networks and algorithms of global capitalism contain moments of uneven porousness and selective hackability.;Hacking code/space: Confounding the code of global capitalism;Zook, M. & Graham, M.;2018;Graham, M.;"Digital Economies; Information Geography and Inequality"
"Cities are comprised of bricks and mortar, concrete and glass, roads, rails, pipes and cables, people, plants and animals. The layers of cities also include the many histories, memories, legends, and stories that people ascribe to place (Crang 1996; Graham 2009). Yet cities have been going through two important transitions that have brought into being new dimensions that profoundly matter for the ways that we interact with our urban environments. Cities are no longer just confined to their material presences: they have become both digital and digitised. Within the Global City literature, much has been written about the ways that both social/business and material/infrastructural networks crucially matter to the development of cities (e.g. Acuto 2011). But this chapter focuses on another component of our urban environments: the many, often invisible and ephemeral, digital layers of cities. The virtual elements of cities are immensely significant. Cities ooze data; they are structured by code and software; they cast innumerable digital shadows. The goal of this chapter is to interrogate these virtual layers of the city. By asking what they are, where they are, and why they matter, we can then explore their significance for global cities. Specifically, with a case study of user-generated content about the city of Jerusalem, we can ask whether contemporary ICTs are reinforcing dual urbanisms and splintering cities or if their potentially open nature allows for a bypassing of concentrations of power that have always been found between and within global cities (Sassen 2005). The rest of this section now reviews some of the key ways in which the virtual dimensions of cities influence the social, economic, and political characteristics, experiences, and interpretations of cities. Four concepts are particularly useful in describing the intersections between the digital and the material: palimpsests, augmented reality, code/space, and digiplace.";The Virtual Dimension;Graham, M.;2013;Graham, M.;"Digital Economies; Information Geography and Inequality"
This short chapter is a reflection on future directions that research on the geoweb and big data could take. It is derived from a reflection that the editors of this volume asked me to provide to a session on the geoweb and big data at the 2014 meeting of the Association of American Geographers. Panelists were asked to summarize some of the day’s themes and to debate how they speak to future directions in the discipline. My reflections are organized into seven themes.;Rethinking the Geoweb and Big Data: Future Research Directions;Graham, M.;2018;Graham, M.;"Digital Economies; Information Geography and Inequality"
Every day, billions of Internet users rely on search engines to find information about places to make decisions about tourism, shopping, and countless other economic activities. In an opaque process, search engines assemble digital content produced in a variety of locations around the world and make it available to large cohorts of consumers. Although these representations of place are increasingly important and consequential, little is known about their characteristics and possible biases. Analyzing a corpus of Google search results generated for 188 capital cities, this article investigates the geographic dimension of search results, focusing on searches such as “Lagos” and “Rome” on different localized versions of the engine. This study answers these questions: To what degree is this city-related information locally produced and diverse? Which countries are producing their own representations and which are represented by others? Through a new indicator of localness of search results, we identify the factors that contribute to shape this uneven digital geography, combining several development indicators. The development of the publishing industry and scientific production appears as a fairly strong predictor of localness of results. This empirical knowledge will support efforts to curb the digital divide, promoting a more inclusive, democratic information society.;Digital Hegemonies: The Localness of Search Engine Results;Ballatore, A., Graham, M. & Sen, S.;2017;Graham, M.;"Digital Economies; Information Geography and Inequality"
Combining data from a sample survey, the 2013 Oxford Internet Survey, with the 2011 UK Census, we employ small area estimation to estimate Internet use in small geographies in Britain. This is the first attempt to estimate Internet use at any small-scale level. Doing so allows us to understand the local geographies of British Internet use, showing that the area with least use is in the North East, followed by central Wales. The highest Internet use is in London and southeastern England. The most interesting finding is that after controlling for demographic variables, geographic differences become nonsignificant. The apparent geographic differences appear to be due to differences in demographic characteristics. We conclude by considering the policy implications of this fact.;Local Geographies of Digital Inequality;Blank, G., Graham, M. & Calvino, C.;2017;Graham, M.;"Digital Economies; Information Geography and Inequality"
Henri Lefebvre talked of the “right to the city” alongside a right to information. As the urban environment becomes increasingly layered by abstract digital representation, Lefebvre's broader theory warrants application to the digital age. Through considering what is entailed by the urbanization of information, this paper examines the problems and implications of any “informational right to the city”. In directing Tony Benn's five questions of power towards Google, arguably the world's most powerful mediator of information, this paper exposes processes that occur when geographic information is mediated by powerful digital monopolies. We argue that Google currently occupies a dominant share of any informational right to the city. In the spirit of Benn's final question—“How do we get rid of you?”—the paper seeks to apply post‐political theory in exploring a path to the possibility of more just information geographies.;An Informational Right to the City? Code, Content, Control, and the Urbanization of Information;Shaw, J. & Graham, M.;2017;Graham, M.;"Digital Economies; Information Geography and Inequality"
"The term ‘spatial media’ has traditionally been used to describe the intersections between information and geography. It signifies information that describes, or is about, a particular place. A street map of Chicago, geographic data files about Copenhagen, a postcard with a picture of Oxford on it, a travel guide to Sweden, are all examples of spatial media; in other words, information about geography. It was only relatively recently that geographic information became so easily infused into spatial media. For most of human history, geographic information was tethered onto particular parts of the world. It passed from person to person, often changing because it was so difficult to attach it to stable containers. But then, a succession of technological advancements (like papyrus and the printing press) allowed for the invention of books, newspapers, maps, and other media. What these mediums had in common was that they that fixed geographic information to its containers: they made it immutable. A paper map for instance, could be moved from place to place without the map itself changing. This chapter, however, is about something different that has recently happened to spatial media. What has occurred is not just a move from mutable geographic information to immutable geographic information, but also the increasing proliferation of what could be called ‘augmented spatial media.’ Instead of just being fixed to containers, information can now augment and be tethered to places; it can form parts of the layers or palimpsests of place (Graham et. al., 2013). A building or a street can now be more than stone, bricks, and glass; it is also constructed of information that hovers over that place: invisible to the naked eye, but accessible with appropriate technological affordances. In other words, while it has long been argued that “the map is not the territory” (Korzybski, 1948; Harley, 1989; Crampton, 2001), this chapter argues is that the map is indeed becoming part of the territory.";Digitally Augmented Geographies;Graham, M.;2017;Graham, M.;"Digital Economies; Information Geography and Inequality"
Global production networks (GPNs) have become a key framework in conceptualizing linkages, power and structure in globalized production. However, this framework has been less successful in integrating the influence of digital information and ICTs in production, and this problematic in a world where relationships and power are increasingly mediated by digital information flows and resources. We thus look to adapt the GPN framework to allow more substantive analysis of ‘the digital’. Primarily, this is done through a theoretical analysis of the three core categories of the GPN framework – embeddedness, value and networks – to highlight how these categories can better integrate a more dynamic and contested conceptualization of the digital. Illustrations from research on the digitalization of tea sector GPNs in East Africa highlight how these theoretical advances provide new insights on the digital and its expanding role in economic production.;Reconsidering the role of the digital in global production networks;Foster, C. & Graham, M.;2016;Graham, M.;"Digital Economies; Information Geography and Inequality"
The ability of search engines to shape our understandings of the world by controlling what people discover when looking for information is well known. We argue that the power of search engines has become further entrenched in the wake of the current move to restructure the Web according to the logics of ‘linked data’ and the ‘semantic Web’. With the goal of sharing information according to structured formats that computers (rather than humans) can easily process and analyse, linked data engineers are abstracting information from fact sharing websites like Wikipedia into short, uniform statements that can be more efficiently shared, compared and analysed. In response to this enhanced power by search engines and the corresponding loss of agency by ordinary users, some Wikipedians have challenged the ways in which data from the encyclopedia has been used (often without credit) by search engines like Google. Using the capabilities approach first developed by Amartya Sen, we interrogate exactly what some Wikipedians believe they are losing when they complain about how Google represents facts about the world obtained from Wikipedia and other sites.;Provenance, power and place: Linked data and opaque digital geographies;Ford, H. & Graham, M.;2016;Graham, M.;"Digital Economies; Information Geography and Inequality"
This paper examines the discourses around emerging Internet connectivity solutions for rural and resource-constrained populations in the developing world. It draws primarily on interviews undertaken with 26 experts within the Information and Communications Technologies for Development (ICTD) field, as well as on institutional explanatory and publicity materials put forward by several industry actors. We identify a sustained disconnect between different conceptions of how technology alters or bridges space—spatial imaginaries. Institutions use narratives that assume technologies eradicate or collapse distance, and thus drive transformative socioeconomic change. By contrast, expert accounts underscore the socially embedded nature of technologically mediated relations and non-infrastructural barriers to connectivity. The paper draws attention to the ways that these spatial ideas are used to justify the development of new infrastructures to extend Internet access in the developing world. The paper identifies a need for continued attention to spatial imaginaries in ICTD, not only as a guiding frame for critical research, but also as a means to improve collaboration between research and industrial practice. ;“Connecting the world from the sky”: Spatial discourses around Internet access in the developing world ;Smart, C., Donner, J. & Graham, M.;2016;Graham, M.;"Digital Economies; Information Geography and Inequality"
For many people, internet access is an essential part of everyday economic, social, and political activities (c.f. Graham and Dutton 2014). Yet access to the internet is, and has always been, geographically concentrated (Graham, Hale, and Stephens 2012, 2011). As such, it is important to focus on the people and places that are largely left out of digital connectivity. This visualisation looks at these in terms of internet penetration (i.e. the share of their population that have “used the Internet (from any location) in the last 12 months” (source: UN 2015)). The map highlights all territories that either have internet penetration below 10%, or for which no data from the World Bank exists. A lack of data can exist for several reasons, for exam- ple: some of these territories are statistically grouped together with bigger entities, no data have been collected or inferred, or the territories lack widespread recognised statehood. The map ultimately highlights an archipelago of land whose population is mostly cut off from the internet. This Archipelago of Disconnection has its centre of gravity in Sub-Saharan Africa where 28 countries have internet penetration rates beneath the 10% threshold we applied. As the internet becomes ever more embedded into global economic flows (Malecki and Morisset 2011), to the inhabitation of urban spaces (Graham 2013), and to other facets of everyday life, those living in the Archipelago of Disconnection are largely barred from participating in the cultural, educational, political, and economic activities that it affords. ;Who isn’t online? Mapping the ‘Archipelago of Disconnection’ ;Straumann, R. & Graham, M.;2016;Graham, M.;"Digital Economies; Information Geography and Inequality"
After observing the growth of the Indian and Filipino Business Process Outsourcing (BPO) sectors, Kenyan policy-makers and managers made substantial investments in international internet infrastructure and BPO marketing campaigns. While observers continue to discuss the sector in terms of its international work opportunities, in recent years the sector has increasingly focused on contracts sourced from Kenyan and other East African clients. The government has also refocused efforts on attracting international BPO companies. This domestic turn signals both the difficulties of gaining access to overseas work due to the power of incumbents and the increasing use of the internet and ICT-enabled automation within Kenyan organisations. In effect, better connectivity has enabled a two-way globalisation of services: Kenyan BPO companies can access international work opportunities but connectivity has also contributed to the inflow of international business practices into Kenya. The conclusion examines what these shifts might entail for the sector and its workers in future.;The Domestic Turn: Business Process Outsourcing and the Growing Automation of Kenyan Organisations;Mann, L. & Graham, M.;2016;Graham, M.;"Digital Economies; Information Geography and Inequality"
"Sub-Saharan Africa has traditionally been characterised by stark barriers to telecommunication and flows of information. Rates for long distance phone calls throughout Sub-Saharan Africa (SSA) used to be some of the highest in the world, and Internet costs and speeds similarly were out of the reach of all but the most privileged citizens. However, in the last few years, there have been radical changes to SSA’s international connectivity. Fibre-optic cables have been laid throughout the continent and there are now over one hundred and fifty million Internet users and over seven hundred million mobile users in the region. This rapid transformation in the region’s connectivity has encouraged politicians, journalists, academics, and citizens to speak of an ICT-fuelled revolution happening on the continent. Individuals and firms would increasingly be linked into global networks - interacting, selling and using knowledge through this connectivity (Graham & Mann 2013). This has also been reflected in new ambitions and policy in SSA. For example, in Rwanda (a strong advocate of upgrading connectivity to drive development) the stated policy goal has been to: “transform her subsistence agriculture dominated economy into a service-sector driven high value- added information and knowledge economy that can compete on the global market” (GoR 2001 p.7). Changing connectivity thus is articulated as a core driver of wider eco- nomic change in SSA (Graham et. al. 2015; Graham 2015). It is seen as providing a path for the region to move away from reliance on agriculture and extractive industries and towards a focus on the quaternary and quinary sectors (in other words, the knowledge-based parts of the economy). However, while much research has been conducted into the impacts of ICTs on older economic processes and practices, there remains surprisingly little research into the emergence of the new informationalized economy in Africa. As such, it is precisely now that we urgently need research to understand what impacts are observable, who benefits, who doesn’t, and how these changes match up to our expectations for change. We need to ask if we are seeing a new era of development on the continent fuelled by ICTs, or whether Sub-Saharan Africa’s engagement with the global knowledge economy continues to be on terms that reinforce dependence, inequality, underdevelopment, and economic extraversion. We begin to address this issue by synthesising the outputs of two multi-year research projects that we have carried out which provide in-depth analysis of SSA connectivity use. The first one addresses the effects of changing connectivities on global geographies of voice, representation, and participation, particularly through exploring the dynamics on- line platforms, tools and databases. The second project is grounded in in-depth qualitative research, examining the effects of changing connectivity on firms in core sectors of the economy (tea, tourism, and Business Process Outsourc- ing) in Kenya and Rwanda (see Foster and Graham 2015a, 2015b, Mann et. al. 2015 for full details).";GEOGRAPHIES OF INFORMATION INEQUALITY IN SUB-SAHARAN AFRICA ;Graham, M. & Foster, C.;2016;Graham, M.;"Digital Economies; Information Geography and Inequality"
In order to understand how the city’s contested political contexts are embedded into its digital layers, we traced how the city is represented on online platforms that house facts about much of the world. We did this by analyzing representations of Jerusalem across the Arabic, Hebrew and English versions of Wikipedia (working with a translator on the Arabic and Hebrew versions), as well as on the platforms of Wikidata, Freebase and Google. As our cities become increasingly digital, and as the digital becomes increasingly governed by the logics of the semantic web, there are important questions to ask about how these new alignments of code and content shape how cities are presented, experienced, and brought into being. What we found is a paradoxical situation whereby, through connecting datasets, semantic web initiatives detach localized information from the contexts of its creation. By divorcing content from its contexts, this process establishes new contexts in which necessarily political decisions are being made with far reaching consequences.;Semantic Cities: Coded Geopolitics and the Rise of the Semantic Web;Ford, H. & Graham, M.;2015;Graham, M.;"Digital Economies; Information Geography and Inequality"
This chapter outlines how one might utilize the massive amounts of web-based, geographically-referenced digital social data for geographical research. Because much of these data are user-generated and produced through social media platforms, we also focus on the pitfalls associated with such sources and the benefits of a mixed methods approach to these data. Not only can digital social data be mapped for visual analysis, it is also useful to use a range of quantitative methods to understand relationships between different subsets of the data. In addition, closer, systematic readings via qualitative methods of social data provides insights of particular people’s perceptions and experiences of the world around them. Thus, while making maps is often the starting point for geographers working with this kind of research, it is rarely the end point.;Using Geotagged Digital Social Data in Geographic Research;Poorthuis, A., Zook, M. & Shelton, T. et al.;2014;Graham, M.;"Digital Economies; Information Geography and Inequality"
Human geographies are infused with information. Information is created, processed, and used in places. It is stored in places, moves across places, and ultimately annotates and augments places.;Information Geographies and Geographies of Information;Graham, M.;2015;Graham, M.;"Digital Economies; Information Geography and Inequality"
"Information has always had geography. It is from somewhere; about somewhere; it evolves and is transformed somewhere; it is mediated by networks, infrastructures, and technologies: all of which exist in physical, material places. These geographies of information about places matter because they shape how we are able to find and understand different parts of the world. Places invisible or discounted in representations are invisible in practice to many people. In other words, geographic augmentations are much more than just representations of places: they are part of the place itself; they shape it rather than simply reflect it. This fusing of the spatial and informational augmentations that are immutable means that annotations of place emerge as sites of political contestation: with different groups of people trying to impose different narratives on informational augmentations. This paper therefore explores how information geographies have their own geographic distributions: geographies of access, of participation, and of representation. The paper offers a deliberately broad survey of a range of key platforms that mediate, host, and deliver different types of geographic information. It does so using a combination of existing statistics and bespoke data not previously mapped or analysed. Through this effort, the paper demonstrates that in addition to the geographies of uneven access to contemporary modes of communication, uneven geographies of participation and representation are also evident and in some cases are being amplified rather than alleviated. In other words, the paper comprehensively shows one important facet of contemporary information geographies: that geographic information itself is characterised by a host of uneven geographies. The paper concludes that there are few signs that global informational peripheries are achieving comparable levels of participation or representation with traditional information cores, despite the hopes that the fast‐paced spread of the internet to three billion people might change this pattern.";Towards a study of information geographies: (im)mutable augmentations and a mapping of the geographies of information;Graham, M., de Sabbata, S. & Zook, M.;2015;Graham, M.;"Digital Economies; Information Geography and Inequality"
East Africa has traditionally been characterized by stark barriers to nonproximate communication and flows of information. It was the world's last major region without fibre-optic broadband Internet access, and until the summer of 2009 had been forced to rely on slow and costly satellite connectivity. This all changed when the first of four fibre-optic cables was connected in Kenya: bringing with it the promise of fast and affordable Internet access for the masses, and the ability of the country to move towards a knowledge-based economy. Within the context of this moment of change, this paper explores the ways that managers of outsourcing firms envisage ‘connectivity.’ Over the course of forty-one interviews, contradictory spatial imaginaries were discovered. When describing their perceptions of the country's new technomediated positionalities, many interviewees repeated visions that allowed geographic frictions to evaporate. But when managers were asked about their actual mediated positionalities, they presented a very different world: one of barriers, frictions, and the very real role that distance continues to play in the world's economic peripheries. The goal of this paper is to interrogate why we see such stark disconnects between perceptions and practices of connectivity. The contradictions could be seen as an exposition of a scalar schism between internationally operating regimes of truth (ie, powerful discourses that have their origin nonlocally) and local experiences and practices in Kenya. Alternatively, we can think about the contradictory accounts of connectivity as emergent from strategic spatial essentialisms that are practised to achieve particular goals. By focusing on the contradictions embedded into the ways in which people speak about connectivity in the Kenyan outsourcing sector, we can learn much about how arguments about the entanglement of connectivity, growth, and development are operationalized. ‘Connectivity’ is offered as a necessary, and sometimes even sufficient, condition from which growth and economic development can be brought into being: a set of spatial imaginaries that conveniently support a national development strategy of remaking Kenya in the contemporary knowledge economy.;Contradictory Connectivity: Spatial Imaginaries and Technomediated Positionalities in Kenya's Outsourcing Sector;Graham, M.;2015;Graham, M.;"Digital Economies; Information Geography and Inequality"
This paper analyses and compares two transformative moments of technologically mediated change in East Africa, the construction of the Uganda railway between Mombasa and Lake Victoria (1896–1903) and the introduction of fibre‐optic cables that landed into the ports of Dar Es Salaam and Mombasa in 2009. The paper uses discourse analysis to examine how technologically mediated connectivity has been represented by political and economic actors during these transformative moments. In both cases we explore the origins of the expectations of connectivity and the hope and fear associated with them. Building on Massey's notion of power‐geometry and Sheppard's concept of positionality, the paper focuses on power relationships in discussions of connectivity and asks how people understand the abilities of transformative technologies to modify positionalities and alter relational distance and proximity. Ultimately, by examining historical and contemporary expectations of connectivity in East Africa, this paper works towards more grounded and historicised understandings of the coming‐together of technology and connectivity.;Geographical imagination and technological connectivity in East Africa;Graham, M., Andersen, C. & Mann, L.;2014;Graham, M.;"Digital Economies; Information Geography and Inequality"
Gazetteers are dictionaries of geographic place-names that have important implications far beyond the worlds of geographers and cartographers. By containing ‘definitive’ lists of places, gazetteers have the ontological power to define what will and will not be geocoded and represented in databases, maps, search engines, and ultimately our spatial understandings of place. This paper focuses attention on GeoNames, which is the world's largest freely available and widely used gazetteer. We illustrate how content in GeoNames is characterised by highly uneven spatial distributions. There are dense clusters of place-names in some parts of the world and a relative absence of geographic content in others. These patterns are related not just to the wealth and population size of a country, but also to its policies on Internet access and open data. The paper then traces some of the specific implications of this information inequality: showing how biases in gazetteers are propagated in a variety of geographic meaning-making.;Mapping information wealth and poverty: the geography of gazetteers;Graham, M. & de Sabbata, S.;2015;Graham, M.;"Digital Economies; Information Geography and Inequality"
Localness is an oft-cited benefit of volunteered geographic information (VGI). This study examines whether localness is a constant, universally shared benefit of VGI, or one that varies depending on the context in which it is produced. Focusing on articles about geographic entities (e.g. cities, points of interest) in 79 language editions of Wikipedia, we examine the localness of both the editors working on articles and the sources of the information they cite. We find extensive geographic inequalities in localness, with the degree of localness varying with the socioeconomic status of the local population and the health of the local media. We also point out the key role of language, showing that information in languages not native to a place tends to be produced and sourced by non-locals. We discuss the implications of this work for our understanding of the nature of VGI and highlight a generalizable technical contribution: an algorithm that determines the home country of the original publisher of online content.;Barriers to the Localness of Volunteered Geographic Information;Sen, S., Ford, H. & Musicant, D. et al.;2015;Graham, M.;"Digital Economies; Information Geography and Inequality"
"So far Arabic content on Wikipedia Arabic is very weak; this paper questions the barriers that prevent contributors to Wikipedia Arabic from being more active in content production on Wikipedia Arabic. The research problematic is twofold: first, it suggests a digital cartography of Wikipedia in the MENA region that informs about those who represent Wikipedia Arabic and thus, contribute to the Arab knowledge distribution. Second, while adopting a qualitative approach, the research looks at the content of Arabic Wikipedia. It questions its production methods, contribution barriers, dynamics of negotiation between Wikipedians, and strategies of consensus between editors and admins, as well as the envisioned solutions to improve and promote Wikipedia Arabic content production.";Wikipedia Arabe et la Construction Collective du Savoir (Wikipedia Arabic and the Collective Construction of Knowledge);Allagui, I., Graham, M. & Hogan, B.;2013;Graham, M.;"Digital Economies; Information Geography and Inequality; Digital Knowledge and Culture; Education, Digital Life and Wellbeing"
"A key and distinguishing feature of society today is that its increasingly documented by crowd-sourced social media discourse about public experiences. Much of this social media content is geo-referenced and exists in layers of information draped over the physical world, invisible to the naked eye but accessible to range of digital (and often) mobile devices. When we access these information layers, they mediate the mundane practices of everyday life, (e.g., What or who is nearby? How do I move from point A to B) through the creation of augmented realities, i.e., unstable, context dependent representations of places brought temporary into being by combining the space of material and virtual experience. These augmented realities, as particular representations of locations, places and events, are vigorously promoted or contested and thus become important spots in which power is exercised, much in the same way that maps have long had power to reinforce or challenge the status quo. However, because many of the processes and practices behind the creation of augmented realities are unseen, its power is often overlooked in the process of representation or place-making. This paper highlights the points at which power acts and demonstrate that all representations of place – including augmented realities derived from social media – are products of and productive of, social relationships and associated power relations. Building upon a case study of Abbottabad, Pakistan after the raid on Osama bin Laden's compound we construct a four-part typology of the power relations emerging from social practices that enact augmented realities. These include: Distributed power, the complex and socially/spatially distributed authorship of user-generated geospatial content; Communication power, the ways in which particular representations gain prominence; language is a particularly key variable; Code power, the autonomy of software code to regulate actions, or mediate content, or ordering representations in particular ways; and Timeless power, the ways in which digital representations of place reconfigure temporal relationships, particularly sequence and duration, between people and events.";Crowd-Sourced Augmented Realities: Social Media and the Power of Digital Representation;Zook, M., Graham, M. & Boulton, A.;2013;Graham, M.;"Digital Economies; Information Geography and Inequality"
The piece asks whether increasing Internet access around the world is resulting in a levelling of digital participation, or if we are seeing older patterns of visibility, representation, and power be reproduced. The piece is open-accessand I’d welcome thoughts or comments on it.;Inequitable Distributions in Internet Geographies: The Global South Is Gaining Access, but Lags in Local Content;Graham, M.;2014;Graham, M.;"Digital Economies; Information Geography and Inequality"
"Today, practices of food production, consumption, and distribution have the potential to go through immensely transformative shifts as information and communication technologies (ICTs) become increasingly embedded in every domain of contemporary life. This paper addresses key challenges and opportunities at the intersection of food and ICTs. The paper argues for the need for interdisciplinary research to examine the key roles that network technologies play in re-shaping social and economic networks of food, focusing on three main research areas: food data, transparency and changing practices; new forms of sociality around food supported by technologies, and; re-routings of mobility and distribution networks made possible by ICTs.";Urban food futures: ICTs and opportunities;Choi, J. & Graham, M.;2014;Graham, M.;"Digital Economies; Information Geography and Inequality"
Digital social data are now practically ubiquitous, with increasingly large and interconnected databases leading researchers, politicians, and the private sector to focus on how such ‘big data’ can allow potentially unprecedented insights into our world. This paper investigates Twitter activity in the wake of Hurricane Sandy in order to demonstrate the complex relationship between the material world and its digital representations. Through documenting the various spatial patterns of Sandy-related tweeting both within the New York metropolitan region and across the United States, we make a series of broader conceptual and methodological interventions into the nascent geographic literature on big data. Rather than focus on how these massive databases are causing necessary and irreversible shifts in the ways that knowledge is produced, we instead find it more productive to ask how small subsets of big data, especially georeferenced social media information scraped from the internet, can reveal the geographies of a range of social processes and practices. Utilizing both qualitative and quantitative methods, we can uncover broad spatial patterns within this data, as well as understand how this data reflects the lived experiences of the people creating it. We also seek to fill a conceptual lacuna in studies of user-generated geographic information, which have often avoided any explicit theorizing of sociospatial relations, by employing Jessop et al’s TPSN framework. Through these interventions, we demonstrate that any analysis of user-generated geographic information must take into account the existence of more complex spatialities than the relatively simple spatial ontology implied by latitude and longitude coordinates.;Mapping the Data Shadows of Hurricane Sandy: Uncovering the Sociospatial Dimensions of ‘Big Data’;Shelton, T., Poorthuis, A. & Graham, M. et al.;2014;Graham, M.;"Digital Economies; Information Geography and Inequality"
"As you introduce yourself to this book, you might find it useful to consider many of the significant ways in which (not) having access to the Internet can alter how you interact with the world around you, such as: How you create, get, and distribute information: The Internet might enable you to create content and get access to information more easily and quickly, compared to working in the library, but also make a difference to the extent of your knowledge. Internet-mediated access to information, media, and other content might also shape your movement. Being able to access information electronically means that you can get where you want to be, or meet with people with whom it is important to interact face to face. How you communicate with people you know, and how you might meet and interact with people you don’t yet know in your social and worklife: The Internet, social media, and video communication introduce you to new people, as well as helping you keep in touch with old friends and associates. It will shape whom you know as well as how you communicate. How you obtain services, from banking and shopping to entertainment, games, and public services: If you decide to shop on the Internet, for example, you might shop from different companies, or purchase services you might not otherwise have known about. What technologies link you to the Internet, from wired and wireless infrastructures to devices you carry with you or wear: This will not only shape what technologies you require, but also what know-how you require to live and work in a world of digital media, information and communication technologies (ICTs). Just as importantly, think of how people use the Internet to get information about you, to communicate with you, to provide you with services, and perhaps even to observe your Internet-mediated behaviour. The Internet is shaping access to you, just as you employ the Internet to shape access to the world (Dutton 1999). It is also important to put some of the significant ways that the Internet mediates everyday life into historical perspective. It was only slightly more than two decades ago that the Web was invented; it was impossible to use Google or Wikipedia in order to look up information fifteen years ago; we couldn’t use Facebook to connect with friends a decade ago; and even five years ago it was only a small minority of people who had access to the Internet on mobile devices. If the next two decades of Internet time are as transformative as the previous two, it is likely that many of us will be living in a very different technologically, informationally, and algorithmically mediated world. As such, there will be an increased need for sustained inquiry into crucial, critical, and timely questions about the interaction of the Internet and society.";Introduction to: Society and the Internet;Dutton, W. & Graham, M.;2014;Graham, M.;"Digital Economies; Information Geography and Inequality"
"Across the globe, daily economic, social, and political activities increasingly revolve around the use of content on the Internet. This content influences our understandings of, and interactions with, our social environment. Yet it is remarkable how little we know about the broader contexts in which much of that content in created. As such, this chapter sets out to comprehensively uncover: (1) where Internet content is being created; (2) whether the amount of content created in different places is changing over time; and (3) the ways in which landscapes of content are structured and formed. Mapping the geographic diversity and concentration of a broad variety of sources of online content is ultimately crucial to develop more informed strategies to combat digital divides and ultimately benefit those who are left out of flows of information.";Internet Geographies: Data Shadows and Digital Divisions of Labour;Graham, M.;2014;Graham, M.;"Digital Economies; Information Geography and Inequality"
Information and communication technologies shape economic spaces, transactions, and relationships, and are often promoted as an essential development strategy in both rich and poor countries. But development strategies often rest on very particular understandings of the ways in which ICTs transform and shape economic relationships. Drawing on Asian and African case studies, this chapter first analyses some of the most common expectations associated with networked connections in low-income countries. It then places those expectations within the contexts of observable economic effects of ICTs in disadvantaged parts of the world. By drawing on a rich set of interviews and examining the tensions between expected and observable effects of ICTs, this chapter shows how discourses about the transformative potentials of ICTs actively shape the ways that development is enacted in low-income countries. Unrealistic and deterministic expectations interestingly have the dual effect of encouraging impractical investments of resources while simultaneously driving many useful projects and practices that would otherwise not occur. This chapter concludes by arguing that by deconstructing and working to refine assumptions that underpin the expectations of transformative effects of ICTs in low-income countries, we will be able to more accurately envision the potentials for new types of ICT-based development.;A Critical Perspective on the Potential of the Internet at the Margins of the Global Economy;Graham, M.;2014;Graham, M.;"Digital Economies; Information Geography and Inequality"
Information is the raw material for much of the work that goes on in the contemporary global economy, and there are few people and places that remain entirely disconnected from international and global economic processes (Castells 1996). Information, and ultimately knowledge, is the carrier for the myriad signals needed for such markets to constantly be enacted, performed and understood. As such, it is important to understand who produces and reproduces, who has access, and who and where are represented by information in our contemporary knowledge economy. This chapter discusses inequalities in traditional knowledge and information geographies, before moving to examine the Internet-era potentials for new and more inclusionary patterns. It concludes that rather than democratizing platforms of knowledge sharing, the Internet seems to be enabling a digital division of labour in which the visibility, voice and power of the North is reinforced rather than diminished.;The Knowledge Based Economy and Digital Divisions of Labour;Graham, M.;2014;Graham, M.;"Digital Economies; Information Geography and Inequality"
"We present, visualize and analyse the similarities and differences between the controversial topics related to “edit wars” identified in 10 different language versions of Wikipedia. After a brief review of the related work we describe the methods developed to locate, measure, and categorize the controversial topics in the different languages. Visualizations of the degree of overlap between the top 100 list of most controversial articles in different languages and the content related geographical locations will be presented. We discuss what the presented analysis and visualizations can tell us about the multicultural aspects of Wikipedia, and, in general, about cultures of peer-production with focus on universal and specifically, local features. We demonstrate that Wikipedia is more than just an encyclopaedia; it is also a window into divergent social-spatial priorities, interests and preferences.";The Most Controversial Topics in Wikipedia: A Multilingual and Geographical Analysis;Yasseri, T., Spoerri, A. & Graham, M. et al.;2013;Graham, M.;"Digital Economies; Information Geography and Inequality"
As digital social data have become increasingly ubiquitous, many have turned their attention to harnessing these massive data sets in order to produce purportedly more accurate and complete understandings of social processes. This intervention addresses the relationships between geography and big data and their intertwined futures. We focus on the impacts of an age of big data on the discipline of geography and geographic thought and methodology, as well as how geography might provide a useful lens through which to understand big data as a social phenomenon in its own right. Ultimately, we see significant potential in big data, but remain skeptical of the prevalent discourses around it, as they tend to obscure, more than reveal, the complexity of social and spatial processes.;Geography and the future of big data, big data and the future of geography;Graham, M. & Shelton, T.;2013;Graham, M.;"Digital Economies; Information Geography and Inequality"
A central problem in contemporary processes of economic globalization is that information about commodities has not been globalized at the same rate as the commodities themselves. Contemporary capitalism conceals the histories and geographies of most commodities from consumers. These consumers rarely have opportunities to gaze backward through the chains of production in order to gain knowledge about the sites of production, transformation, and distribution of products. The complexity of commodity chains leaves us with highly opaque production processes. Transnational companies often strive to maintain this opacity through a separation between the “airbrushed world” communicated through advertising on one hand, and the actual world of production on the other. With this problem in mind, this chapter discusses the potential for emergent practices of collaboration and communication through the Internet to facilitate flows of information about commodity chains. The hope is that, by transcending barriers of time and space, new practices tied to communication and information sharing through the Internet will open up the politics of consumer activism and influence the way goods are produced, particularly those that that originate in the Global South.;Transparency and Development: Ethical Consumption through Web 2.0 and the Internet of Things;Graham, M. & Haarstad, H.;2013;Graham, M.;"Digital Economies; Information Geography and Inequality"
Academia and the networks of knowledge and information that it is embedded in are changing. This response highlights three areas of concern within the coming-togethers of social media and geography. First, although blogs can create the fissures in media/social constellations, they more often than not form an integral part of those very mediascapes. Second, while social media may have allowed for some changing gatekeepers, it remains that the creation and dissemination of information are highly socially and spatially uneven. Finally, we are able to critically reflect on how much change we have actually seen in knowledge flows out of academia and whether we are truly seeing new forms and enactions of public geographies. The paper ultimately argues that while new channels and digital mediations of information might allow us to reach different publics, it remains that our digitally mediated work is far from being public.;Social media and the academy: New publics or public geographies?;Graham, M.;2013;Graham, M.;"Digital Economies; Information Geography and Inequality"
This paper analyzes the digital dimensions of places as represented by online, geocoded references to the economic, social, and political experiences of the city. These digital layers are invisible to the naked eye, but form a central component of the augmentations and mediations of place enabled by hundreds of millions of mobile computing devices and other digital technologies. The analysis highlights how these augmentations of place differ across space and language and highlights both the differences and some of the causal factors behind them. This is performed through a global study of all online content indexed within Google Maps, and more specific analyses of the linguistically and topically segregated layers of information over four selected places. The uneven linguistic geographies that this study reveals undoubtedly influence the many ways in which place is enacted and brought into being. The larger aim of this project is to use these initial mappings of the linguistic contours of the geoweb to push forward a broader debate about how augmented inclusions and exclusions, visibilities and invisibilities will shape the way that places become defined, imagined, and experienced.;Augmented Realities and Uneven Geographies: Exploring the Geolinguistic Contours of the Web;Graham, M. & Zook, M.;2013;Graham, M.;"Digital Economies; Information Geography and Inequality"
Enormous amounts of online and networked data are becoming part of the layers, experiences, and landscapes of place. Geographers and other social scientists have only relatively recently begun to understand this rapid expansion of user-centered, locational media. Movements in the academy in response to these phenomena have offered a series of organising labels, with different levels of speciﬁ city and layers of connotation: the geoweb, spatial/social media, user-generated content, ‘big data’, as well as volunteered geographic information (VGI) and neogeography. Within geography a number of events mark these developments, including a VGI specialist meeting in Santa Barbara, USA in 2007 and an accompanying special issue of GeoJournal (Elwood, 2008), a World University Network seminar on neogeography in 2008, an interview with Michael Goodchild conducted by Nadine Schuurman and published by Environment and Planning D: Society and Space in 2009, Progress in Human Geography reports authored by Jeremy Crampton (2009) and Sarah Elwood (2010), a specialist meeting on space–time geographies of social networks in Santa Barbara in 2010, a preconference gathering on VGI in Seattle in 2011 reported in an edited collection (Sui et al, 2013), and numerous special sessions at the AAG meetings annually since 2008. Meanwhile, popular technology conferences like Where 2.0 have met for eight years and draw designers and engineers representing software giants and startups, as well as representatives from municipal, state, and federal governments, with only a handful of academic geographers. Since Andrew Turner’s Introduction to Neogeography (2006), neogeographers are increasingly deﬁning themselves in arenas outside of the academy.;Situating Neogeography;Wilson, M. & Graham, M.;2013;Graham, M.;"Digital Economies; Information Geography and Inequality"
Many of the ways in which we discuss, imagine and envision the internet rely on inaccurate and unhelpful spatial metaphors. This paper focuses on the usage of the ‘cyberspace’ metaphor and outlines why the reliance by contemporary policymakers on this inherently geographic metaphor matters. The metaphor constrains, enables and structures very distinct ways of imagining the interactions between people, information, code and machines through digital networks. These distinct imaginations, in turn, have real effects on how we enact politics and bring places into being. The paper broadly traces the history of ‘cyberspace’, explores the scope of its current usage and highlights the discursive power of its distinct way of shaping our spatial imagination of the internet. It then concludes by arguing that geographers should take the lead in employing alternate, nuanced and spatially grounded ways of envisioning the myriad ways in which the internet mediates social, economic and political experiences.;Geography/internet: ethereal alternate dimensions of cyberspace or grounded augmented realities?;Graham, M.;2013;Graham, M.;"Digital Economies; Information Geography and Inequality"
"Zombies exist, though perhaps not in an entirely literal sense. But the existence, even the outright prevalence, of zombies in the collective social imaginary gives them a ‘realness,’ even though a zombie apocalypse has yet to happen. The zombie trope exists as a means through which society can playfully, if somewhat grimly and gruesomely, discover the intricacies of humanity’s relationship with nature and the socially constructed world that emerges from it. In this chapter, we present an analysis of the prevalence of zombies and zombie-related terminology within the geographically grounded parts of cyberspace, known as the geoweb (see also Haklay et al. 2008 and Graham 2010). Just as zombies provide a means to explore, imagine and reconstruct the world around us, so too do the socio-technical practices of the geoweb provide a means for better understanding human society (Shelton et al. forthcoming; Graham and Zook 2011; Zook et al. 2010; Zook and Graham 2007). In short, looking for and mapping geo-coded references to zombies on the web provides insight on the memes, mechanisms and the macabre of the modern world. Using a series of maps that visualize the virtual geographies of zombies, this chapter seeks to comprehend the ways in which both zombies and the geoweb are simultaneously reflective of and employed in producing new understandings of our world.";Mapping Zombies: A Guide for Digital Pre-Apocalyptic Analysis and Post-Apocalyptic Survival;Graham, M., Shelton, T. & Zook, M.;2013;Graham, M.;"Digital Economies; Information Geography and Inequality"
"This is a paper about expectations surrounding a potentially highly transformative moment in East Africa's history: the arrival of underwater fibre‐optic broadband communications cables into the Indian Ocean port of Mombasa. It combines a media content analysis with findings from interviews with business owners in Kenya's nascent business process outsourcing (BPO) and software development sectors in order to explore how such moments of technological ‘connectivity’ are imagined, marketed and enacted within economic development. It argues that connectivity is not just a matter of boosting physical/material capacity but also about redressing conceptual connectivity; bringing places ‘closer together’ involves rehabilitating the images of places in peoples’ minds and removing imagined senses of distance. As such, technologies of connectivity are marketed not just as tools of altered communications affordances, but more importantly, as momentary opportunities for revisiting the image of places from afar. Additionally, the cables reveal the importance of fostering internal linkages in order to better build international recognition and connections. ‘Moments of expectation’ that surround new ICT technologies reveal how discourse and representation play a strong role in enabling markets to form and change. The very idea of ‘connectivity’ itself is driving plans and policies throughout the region.";Imagining a Silicon Savannah? Technological and Conceptual Connectivity in Kenya's BPO and Software Development Sectors;Graham, M. & Mann, L.;2017;Graham, M.;"Digital Economies; Information Geography and Inequality"
"This article presents an overview and initial results of a geoweb analysis designed to provide the foundation for a continued discussion of the potential impacts of ‘big data’ for the practice of critical human geography. While Haklay's (2012) observation that social media content is generated by a small number of ‘outliers’ is correct, we explore alternative methods and conceptual frameworks that might allow for one to overcome the limitations of previous analyses of user-generated geographic information. Though more illustrative than explanatory, the results of our analysis suggest a cautious approach toward the use of the geoweb and big data that are as mindful of their shortcomings as their potential. More specifically, we propose five extensions to the typical practice of mapping georeferenced data that we call going ‘beyond the geotag’: (1) going beyond social media that is explicitly geographic; (2) going beyond spatialities of the ‘here and now’; (3) going beyond the proximate; (4) going beyond the human to data produced by bots and automated systems, and (5) going beyond the geoweb itself, by leveraging these sources against ancillary data, such as news reports and census data. We see these extensions of existing methodologies as providing the potential for overcoming existing limitations on the analysis of the geoweb. The principal case study focuses on the widely reported riots following the University of Kentucky men's basketball team's victory in the 2012 NCAA championship and its manifestation within the geoweb. Drawing upon a database of archived Twitter activity – including all geotagged tweets since December 2011–we analyze the geography of tweets that used a specific hashtag (#LexingtonPoliceScanner) in order to demonstrate the potential application of our methodological and conceptual program. By tracking the social, spatial, and temporal diffusion of this hashtag, we show how large databases of such spatially referenced internet content can be used in a more systematic way for critical social and spatial analysis.";Beyond the geotag: situating ‘big data’ and leveraging the potential of the geoweb;Crampton, J., Graham, M. & Poorthuis, A. et al.;2013;Graham, M.;"Digital Economies; Information Geography and Inequality"
The production of silk occupies a unique place in Thai cultural and economic practices. However, the practice is rarely passed on to the younger generation and is widely considered to be a dying craft. In response, influential organizations have proposed use of the internet as a way to reinvigorate the industry and attract new customers. This paper looks at the discourses used to sell silk and the ways in which sellers are either framing Thai silk as a traditional craft in need of saving or as an enterprise that efficiently engages with the commercial needs of the global economy. The paper reviews the range of, often problematic, emotions, images, and associations used to sell a dying craft. Ultimately, it argues that, in contrast to many of the theorized effects of the internet, it seems to be neither encouraging mass homogenization nor pushing sellers to effectively integrate themselves into global markets.;Thai Silk Dot Com: Authenticity, Altruism, Modernity and Markets in the Thai Silk Industry;Graham, M.;2013;Graham, M.;"Digital Economies; Information Geography and Inequality"
"With the increasing prevalence of both geographically referenced information and the code through which it is regulated, digital augmentations of place will become increasingly important in everyday, lived geographies. Through two detailed explorations of ‘augmented realities’, this paper provides a broad overview of not only the ways that those augmented realities matter, but also the complex and often duplicitous manner that code and content can congeal in our experiences of augmented places. Because the re‐makings of our spatial experiences and interactions are increasingly influenced through the ways in which content and code are fixed, ordered, stabilised and contested, this paper places a focus on how power, as mediated through technological artefacts, code and content, helps to produce place. Specifically, it demonstrates there are four key ways in which power is manifested in augmented realities: two performed largely by social actors, distributed power and communication power; and two enacted primarily via software, code power and timeless power. The paper concludes by calling for redoubled attention to both the layerings of content and the duplicity and ephemerality of code in shaping the uneven and power‐laden practices of representations and the experiences of place augmentations in urban places.";Augmented reality in urban places: contested content and the duplicity of code;Graham, M., Zook, M. & Boulton, A.;2012;Graham, M.;"Digital Economies; Information Geography and Inequality"
This article combines geographical studies of both the Internet and religion in an analysis of where and how a variety of religious practices are represented in geotagged Web content. This method provides needed insight into the geography of virtual expressions of religion and highlights the mutually constitutive, and at times contradictory, relationship between the virtual and material dimensions of religious expression. By using the spatialities of religious practice and contestation as an example, this article argues that mappings of virtual rep- resentations of material practices are important tools for understanding how online activities simultaneously represent and reproduce the material world. ;The Technology of Religion: Mapping Religious Cyberscapes ;Shelton, T., Zook, M. & Graham, M.;2012;Graham, M.;"Digital Economies; Information Geography and Inequality"
Production is the creation of utilities, goods, and services that are useful to someone, by means of human and material resources. Human societies have, throughout history, built increasingly complex systems of production and exchange, and these systems have had profound effects on everyday life. These systems of production and the effects that they have are not evenly dispersed over the surface of the earth. Rather, systems of production create complex geographical patterns. Today it is impossible to properly understand production and how systems of production shape our surroundings without adopting a global perspective. ;Global Production Patterns;Haarstad, H. & Graham, M.;2012;Graham, M.;"Digital Economies; Information Geography and Inequality"
The practice of handmade silk weaving has disappeared from much of the world, but continues to be practiced by thousands of people in Northeastern Thailand. However, as the Thai economy becomes increasingly embedded into global flows and networks of commodities, capital and culture, there are worries that silk weaving as a practice will either cease to be reproduced or will have to radically change in order to service the global market. This paper, based on in- depth interviews and surveys with sellers of silk, examines this dilemma faced by the industry. It finds that the means through which economic information is codified and transmitted over space and the tastes of non-local markets are ultimately resulting in changes to production practices throughout the country. Despite the fact that the internet is enabling trade and thereby allowing production practices to continue, fears are being realized about traditional practices being replaced as producers become ever more integrated into global networks. ;“Perish or Globalize:” Network Integration and the Reproduction and Replacement of Weaving Traditions in the Thai Silk Industry ;Graham, M.;2011;Graham, M.;"Digital Economies; Information Geography and Inequality"
Commentators are now pointing to the potential for a globalization of knowl- edge and transparency that will harness the power of the Internet to allow consumers to learn more about the commodities they buy. This article dis- cusses the potential for emergent Web 2.0 technologies to transcend barriers of time and space, both to facilitate oows of information about the chains of commodities, and to open up potential politics of consumer activism, particu- larly to inouence the way goods that originate in the Global South are pro- duced. We argue that these prospects are ultimately tempered by a number of persistent barriers to the creation and transmission of information about com- modities (infrastructure and access, actors’ capacities, the continued role of infomediaries, and intelligent capture and use by consumers). ;Transparency and Development: Ethical Consumption Through Web 2.0 and the Internet of Things ;Graham, M. & Haarstad, H.;2011;Graham, M.;"Digital Economies; Information Geography and Inequality"
This paper makes the argument that there are three core reasons global component of the palimpsests of place. That is, Wikipedia is characterised by uneven geographies, uneven directionality and uneven politics. This paper will thus detail the abilities of Wikipedia of all geo-tagged articles in the encyclopaedia is examined in order to visualise the distinct geographies of Wikipedia. Some parts of the world are characterised by highly dense virtual representations, while others have essentially become virtual terra incognita. Second, the geographies of some of the language editions are looked at in order the encyclopaedia. Finally, through a few case studies, some of the politics and power relationships of representation within Wikipedia are highlighted. The paper concludes on the note that Wikipedia, at least the English language version, is often presented as having exhausted potential topics. Much work has gone into examining bias in the content that already exists, but maybe more focus is needed on the information that simply isn’t there.;Wiki Space–Palimpsests and the Politics of Exclusion ;Graham, M.;2010;Graham, M.;"Digital Economies; Information Geography and Inequality"
It is frequently argued that the 'digital divide' is one of the most significant development issues facing impoverished regions of the world. Yet, even though the term is inherently spatial, there have been no sustained efforts to examine the geographic assumptions underlying discourses of the ‘digital divide.’ This article traces the history of the term, reviewing some of its tangible effects, and placing a focus on the temporal and spatial assumptions underpinning ‘digital divide’ discourses. Alternative formulations of the ‘digital divide’ are offered which take into account the hybrid, scattered, ordered, and individualised nature of cyberspaces.;Time Machines and Virtual Portals: The Spatialities of the Digital Divide;Graham, M.;2011;Graham, M.;"Digital Economies; Information Geography and Inequality"
This article focuses on the representation of physical places on the Internet or what we term cyberscape. While there is a wide range of online place-related information available, this project uses the metric of the number of user-generated Google Maps placemarks containing specific keywords in locations worldwide. After setting out the methods behind this research, this article provides a cartographic analysis of these cyberscapes and examines how they inform us about the material world. Visibility and invisibility in material space are increasingly being defined by prominence, ranking, and presence on the Internet, and Google has positioned itself as a highly authoritative source of online spatial information. As such, any distinct spatial patterns within uploaded information have the potential to become real and reinforced as Google is relied upon as a mirror of the offline world.;Visualizing Global Cyberscapes: Mapping User-Generated Placemarks;Graham, M. & Zook, M.;2011;Graham, M.;"Digital Economies; Information Geography and Inequality"
The Thai silk industry is in a worrying position. For centuries the industry has provided economic support to hundreds of thousands of people in the northeast of Thailand and become a part of the region's cultural heritage. However, the industry is now dying largely because of uncompetitive nature of the silk being produced. This paper therefore examines one of the most widely touted development strategies: the use of the internet to both expand markets and disintermediate commodity chains. Using surveys and interviews, this study examines the geographic and topological effects that the internet has had in the value chains of Thai silk.;Disintermediation, Altered Chains and Altered Geographies: The Internet in the Thai Silk Industry;Graham, M.;2011;Graham, M.;"Digital Economies; Information Geography and Inequality"
The internet has made possible a pooling of labor from around the world on a scale never before possible in human history. Millions of people now contribute work to cyberprojects like Facebook, Wikipedia, and Google Earth. Unfortunately, distinct demographic biases characterize both the creators and the content of these new projects. Rather than bringing everyone into a global village, the internet instead enables hybrid physical/virtual spaces to be created that can never eliminate the global economic inequalities that characterize the physical world. Many of the free contributions of labor on the internet are based on a hope that a shared, open, transparent, and democratic digital commons is being created. However, new cyberspaces are frequently subject to many of the same power relations that characterize the offline-world, with large profits being made by private companies from freely contributed labor. Hopes for a digital commons built by global workforce of volunteers should not be lightly discarded, but as this chapter demonstrates, there remain myriad forms of bias, control and exploitation that characterize many of the projects being constructed on the internet.;Cloud Collaboration: Peer-Production and the Engineering of Cyberspace;Graham, M.;2011;Graham, M.;"Digital Economies; Information Geography and Inequality"
This article examines some of the discourses being put forward as justifications for Internet use and altered commodity chains in the Thai silk industry. Those discourses are then compared to data on the relationship between the Internet and prices and wages. The article speciªcally looks at claims about who is benefiting from value chain reconfigurations, and then it compares those claims to insights about the Thai silk industry collected using a series of surveys and in-depth interviews. The article demonstrates that claims are put forth that altered commodity chain topologies will necessarily result in an ac-crual of economic and cultural benefits for producers and/or consumers. How-ever, there is little empirical proof that the integration of the Internet into the Thai silk industry is having any noticeable effect on prices or wages.;Justifying Virtual Presence in the Thai Silk Industry GRAHAM Research Article Justifying Virtual Presence in the Thai Silk Industry: Links Between;Graham, M.;2010;Graham, M.;"Digital Economies; Information Geography and Inequality"
This paper outlines the ways in which information technologies (ITs) were used in the Haiti relief effort, especially with respect to web-based mapping services. Although there were numerous ways in which this took place, this paper focuses on four in particular: CrisisCamp Haiti, OpenStreetMap, Ushahidi, and GeoCommons. This analysis demonstrates that ITs were a key means through which individuals could make a tangible difference in the work of relief and aid agencies without actually being physically present in Haiti. While not without problems, this effort nevertheless represents a remarkable example of the power and crowdsourced online mapping and the potential for new avenues of interaction between physically distant places that vary tremendously. ;Volunteered Geographic Information and Crowdsourcing Disaster Relief: A Case Study of the Haitian Earthquake;Zook, M., Graham, M. & Shelton, T. et al.;2010;Graham, M.;"Digital Economies; Information Geography and Inequality"
"Places have always been palimpsests. The contemporary is constantly being constructed upon the foundations of the old. Yet only recently has place begun to take on an entirely new dimension. Millions of places are being represented in cyberspace by a labour force of hundreds of thousands of writers, cartographers and artists. This paper traces the history and geography of virtual places. The virtual Earth is not a simple mirror of its physical counterpart, but is instead characterised by both black holes of information and hubs of rich description and detail. The tens of millions of places represented virtually are part of a worldwide engineering project that is unprecedented in scale or scope and made possible by contemporary Web 2.0 technologies. The virtual Earth that has been constructed is more than just a collection of digital maps, images and articles that have been uploaded into Web 2.0 cyberspaces; it is instead a fluid and malleable alternate dimension that both influences and is influenced by the physical world.";NEOGEOGRAPHY AND THE PALIMPSESTS OF PLACE: WEB 2.0 AND THE CONSTRUCTION OF A VIRTUAL EARTH;Graham, M.;2010;Graham, M.;"Digital Economies; Information Geography and Inequality"
In an introductory human geography course, students research chains of production for consumer products and put what they’ve learned on the internet for others to see and use.;Web 2.0 and Critical Globalization Studies;Graham, M.;2010;Graham, M.;"Digital Economies; Information Geography and Inequality"
This paper asks whether the economic success of low cost carriers (LCCs) has been achieved through optimizations of network structures to liberalized market conditions. By examining the network structures of six successful LCCs, the paper moves away from the idea of a “low cost carrier model” and instead highlights the range of spatial strategies employed by budget airlines in liberalized marketplaces. Findings show that significantly different spatial models are employed by the six carriers examined in this study. Unlike previous studies on the networks structures of LCCs, this paper moves beyond any one specific region and looks at the networks of budget airlines in Asia, Europe, and North America. Fundamental differences in the network structures of LCCs on these three continents indicate that liberalization has allowed for the utilization of a variety of spatial operating models rather than a singular optimal model.;Different models in different spaces or liberalized optimizations? Competitive strategies among low-cost carriers;Graham, M.;2009;Graham, M.;"Digital Economies; Information Geography and Inequality"
The Internet is frequently touted as the engine of a new revolution that can eliminate poverty and bring prosperity to producers of crafts and commodities in economically impoverished areas of the world. ‘E‐commerce’, ‘commodity chains’, the ‘digital divide’, and ‘disintermediation’ are all inherently geographical ideas, as well as being integral components to many theories of economic development. However, despite a movement by geographers to recognize the nuanced relationships between the Internet and geography, such ideas have remained largely absent from much development discourse. By reviewing writing on geographical concepts such as ‘commodity chains’, the ‘digital divide’, ‘disintermediation’, and ‘e‐commerce’ within the contexts of contemporary debates about development, this article highlights some of the geographic assumptions wrapped up in a range of theories of development and shows how these spatial assumptions matter. The article concludes by reflecting on alternate geographic metaphors that could be employed within development discourse to better express the complicated and spatially contingent relationships between information and communication technologies, geography, and economic development.;Warped Geographies of Development: The Internet and Theories of Economic Development;Graham, M.;2008;Graham, M.;"Digital Economies; Information Geography and Inequality"
The Internet has often been portrayed as the ultimate leveler of information where existing hierarchies of power and privilege are undermined by meritocracy. Some websites and functions are, however, more equal than others. In particular, search engines such as Google have been a key means to construct meaning out of disorder. This ordering (or enclosing of the Internet commons), however, comes at a cost as a location within the top 10 Google search results, marks the boundary (albeit a fluid one) between the core and the periphery of the Internet. The recent incorporation of spatial elements into the Google indexing raises fresh and geographically relevant concerns. This article focuses on the construction, access and use of Google derived rankings to deploy geo-referenced information in the physical environment and the way this melding of code and place affects how people interact with place. Using the theoretical concept of DigiPlace this article analyzes how Google Maps and Google Earth are structured and shape what appears (and what does not) in cyberspace and DigiPlace. Of particular concern are the implications of a private corporation controlling this new space.;The Creative Reconstruction of the Internet: Google and the Privatization of Cyberspace and Digiplace;Zook, M. & Graham, M.;2007;Graham, M.;"Digital Economies; Information Geography and Inequality"
"The information age is increasingly mobile with ever finer webs of potential connectivity overlaying the physical spaces we inhabit. While commentators have long argued that cyberspace can only be understood in reference to material places (Castells, 1996; Graham, 1998; Zook, 2000), this chapter analyzes a particularly striking example of the connections between electronic data, physicality and mobility, i.e., Google’s localization products. In fact, we argue that services such as GoogleLocal engender a type of hybrid space (Couclelis, 1996), which we term DigiPlace, in which digital data and physical places are continually re-combined into lived, subjective space as one negotiates through time, space and information. Particularly novel factors behind the construction and experience of DigiPlace are: the ability to access it in real time and on the move, and the impact of this electronic visibility on perceptions of physical accessibility. This chapter briefly outlines a theoretical lens through which this phenomenon can be viewed and presents a case study based on mobile access to GoogleLocal to explore changes to the way people experience and move through data, space and place.";From cyberspace to DigiPlace: Visibility in an age of information and mobility;Zook, M. & Graham, M.;2007;Graham, M.;"Digital Economies; Information Geography and Inequality"
Now that Wal-Mart has conquered the US, can it conquer the world? As Wal-Mart World shows, the corporation is certainly trying. For a number of years, Wal-Mart has been the largest company in the United States. Now, though, it is the largest company in the world. Its global labor practices and outsourcing strategies represent for many what contemporary economic globalization is all about. But Wal-Mart is not standing still, and is opening up stores everywhere. From Germany to Beijing to Mexico City to Tokyo, more than a billion shoppers can now hunt for bargains at a Wal-Mart superstore. Wal-Mart World is the first book to look at this incredibly important phenomenon in global perspective, with chapters that range from its growth in the US and impact on labor relations here to its fortunes overseas. How Wal-Mart manages this transition in the near future will play a significant role in the determining the character of the global economy. Wal-Mart World's impressively broad scope makes it necessary reading for anyone interested in the global impact of this economic colossus.;Wal-Mart Nation: Mapping the Reach of a Retail Colossus;Zook, M. & Graham, M.;2006;Graham, M.;"Digital Economies; Information Geography and Inequality"
Information and communication technologies have long been predicted to spread economic opportunities to rural areas. However, the actual trend in the 21st century has been the opposite. Knowledge spillovers have fuelled urbanisation and pulled job-seekers into large cities, increasing the gap with rural areas. We argue that new assemblages of technologies and social practices, so-called ‘online labour platforms’, have recently started to counter this trend. By providing effective formal and informal mechanisms of enforcing cooperation, these platforms for project-based remote knowledge work enable users to hire and find work across distance. In analysing data from a leading online labour platform in more than 3000 urban and rural counties in the United States, we find that rural workers made disproportionate use of the online labour market. Rural counties also supplied, on average, higher-skilled online work than urban areas did. However, many of the most remote regions of the country did not participate in the online labour market at all. Our findings highlight the potentials and limitations of such platforms for regional economic development.;ICTs and the urban-rural divide: can online labour platforms bridge the gap?;Braesemann, F., Lehdonvirta, V. & Kässi, O.;2020;Kässi, O.;"Digital Economies; Information Geography and Inequality"
Labour markets are thought to be in the midst of a dramatic transformation, where standard employment is increasingly supplemented or substituted by temporary work mediated by online platforms. Yet the scale and scope of these changes is hard to assess, because conventional labour market statistics and economic indicators are ill-suited to measuring this “online gig work”. We present the Online Labour Index (OLI), an experimental economic indicator that approximates the conventional labour market statistic of new open vacancies. It measures the utilization of online labour across countries and occupations by tracking the number of projects and tasks posted on major online gig platforms in near-real time. The purpose of this article is to introduce the OLI and describe the methodology behind it. We also demonstrate how it can be used to address previously unanswered questions about the online gig economy. To benefit policymakers, labour market researchers and the general public, our results are published in an interactive online visualisation which is updated daily.;Online labour index: Measuring the online gig economy for policy and research;Kässi, O. & Lehdonvirta, V.;2018;Kässi, O.;"Digital Economies; Information Geography and Inequality"
I decompose the earnings variance of Finnish male and female workers into its permanent and transitory components using the approach of Baker (J Labor Econ,15:338–375, 1997) and Haider (J Labor Econ, 19:799–836, 2001) in the spirit of scientific replication. I find that the increasing earnings inequality of men and women is driven by both the transitory and permanent components of earnings. In addition, I find considerable differences in the earnings dynamics of men and women, that have been largely neglected in previous studies of earnings dynamics. The inequality among men is dominated by the permanent component. Conversely, permanent and transitory components are of comparable magnitudes to women. As a corollary, men experience more stable income paths but display larger permanent earnings differences. Women, on the other hand, face more unstable earnings profiles but show smaller permanent differences in earnings.;Earnings dynamics of men and women in Finland: permanent inequality versus earnings instability;Kässi, O.;2014;Kässi, O.;Digital Economies
This study explores the short-run spillover effects of popular research papers. We consider the publicity of ‘Male Organ and Economic Growth: Does Size Matter?’ as an exogenous shock to economics discussion paper demand, a natural experiment of a sort. In particular, we analyze how the very substantial visibility influenced the downloads of Helsinki Center of Economic Research discussion papers. Difference in differences and regression discontinuity analysis are conducted to elicit the spillover patterns. This study finds that the spillover effect to average economics paper demand is positive and statistically significant. It seems that hit papers increase the exposure of previously less downloaded papers. We find that part of the spillover effect could be attributable to Internet search engines’ influence on browsing behavior. Conforming to expected patterns, papers residing on the same web page as the hit paper evidence very significant increases in downloads which also supports the spillover thesis.;Demand spillovers of smash-hit papers: evidence from the ‘Male Organ Incident’;Kässi, O. & Westling, T.;2013;Kässi, O.;Digital Economies
Digital labor markets are structured around tasks and not around fixed or long term employment contracts. We study the consequences of the granularization of work for digital micro workers. To address this question, we combine interview data from active online microworkers and online data on open projects scraped from Amazon's Mechanical Turk platform to study how the digital microworkers choose which tasks they work on. We find evidence for preferential attachment: workers prefer to attach themselves to experienced employers who are known to offer high quality projects. In addition, workers also clearly prefer long series of repeatable tasks over one off tasks, even when one off tasks pay considerably more. We thus see a reemergence of certain types of organizational structure.;Workers’ task choice heuristics as a source of emergent structure in digital microwork;Kässi, O., Lehdonvirta, V. & Dalle, J-M.;2019;Kässi, O.;"Digital Economies; Information Geography and Inequality"
Recent years have witnessed a surge in the development of digital real estate technologies. Often referred to as PropTech (property technology), these innovations might variously promise more efficient portfolio management (e.g. VTS), new ways to rent accommodation (e.g. Airbnb), or hassle-free maintenance (e.g. FixFlo). Whilst commentators have debated their novelty as either highly disruptive or a temporary fad, few researchers have sought to fully theorize the digital real estate platform. And those that have provided overviews of the so-called PropTech landscape have failed to do so in a sufficiently critical manner, instead opting for a raft of essentialist and categorical terms. Borrowing the lenses of Science and Technology Studies (STS) and platform studies, this paper develops a theory of digital real estate platforms to address this conceptual gap. And through a qualitative analysis of some 400 businesses, it provides a series of key observations of Platform Real Estate as an improved theoretical neologism to inform future research. These observations are important to better understand the nature of digital real estate platforms and the manner in which they may reconstruct future urban real estate markets – a subject of great concern to researchers and market participants alike.;Platform Real Estate: theory and practice of new urban real estate markets;Shaw, J.;2018;Shaw, J.;Digital Economies
"Research by psychologists and others has consistently found that employees experience better psychological wellbeing than those who are unemployed. This finding has proven remarkably robust across time and across countries, and seems to affect all groups regardless of their age, sex or social class. Finding a theoretical framework to understand the negative psychological consequences has, on the other hand, generated a lot of controversy despite many decades of serious research on the subject. There is consensus that unemployment cannot be understood in simply economic terms, but requires psychological insight. Some theorists have focused on the good things about being in paid work, others on the distinctly negative things about unemployment. This chapter will describe some of the most influential theories, and how well they are supported by empirical evidence, before considering their applicability in a wider variety of settings. The theories were generated in a time when employment in industrialised countries was more homogeneous; people went to the factory or office, worked and then went home. Now many employees' lives have moved beyond this. The shift away from manufacturing to service industries combined with the internet and mobile technologies such as laptops and phones have softened the boundaries around workplaces so that employees can increasingly work from anywhere. And the rise of zero-hour contracts and other flexible forms of work scheduling have detracted from the security and predictability of paid work that is central to many psychological theories of wellbeing. A growing awareness of the very different labour markets that exist in developing countries, where the boundaries between employment, self-employment and work within the family have also challenged the applicability of our understanding of employment and unemployment. This chapter will provide a solid coverage of the conventional material in this area as well as a critical analysis of its global applicability in the 21st century.";Unemployment and Well-Being;Wood, A.;2018;Wood, A.;Digital Economies
"Numerous accounts of work have emphasised the ways in which the labour process and employment relations can be obscured. Burawoy (1979) goes as far as to argue that the obscuring of the labour process is a central feature of capitalist employment. Burawoy (1979); Durand and Stewart (1998); Heyes (1997); Pollert (1981) and Peng (2011) all demonstrate the ways in which a variety of ‘work games’ act to mystify the nature of the workplace and experiences of employment.";Temporal flexibility: scheduling gifts and the obscuring of employment relations;Wood, A.;2016;Wood, A.;Digital Economies
"The purpose of this paper is to review “institutional experimentation” for protecting workers in response to the contraction of the standard employment relationship and the corresponding rise of “non-standard” forms of paid work. The paper draws on the existing research and knowledge base of the authors as well as a thorough review of the extant literature relating to: non-standard employment contracts; sources of labour supply engaging in non-standard work; exogenous pressures on the employment relationship; intermediaries that separate the management from the control of labour; and entities that subvert the employment relationship. Post-war industrial relations scholars characterised the traditional regulatory model of collective bargaining and the standard employment contract as a “web of rules”. As work relations have become more market mediated, new institutional arrangements have developed to govern these relations and regulate the terms of engagement. The paper argues that these are indicative of an emergent “patchwork of rules” which are instructive for scholars, policymakers, workers’ representatives and employers seeking solutions to the contraction of the traditional regulatory model. While the review of the institutional experimentation is potentially instructive for developing solutions to gaps in labour regulation, a drawback of this approach is that there are limits to the realisation of policy transfer. Some of the initiatives discussed in the paper may be more effective than others for protecting workers on non-standard contracts, but further research is necessary to test their effectiveness including in different contexts. The findings indicate that a task ahead for the representatives of government, labour and business is to determine how to adapt the emergent patchwork of rules to protect workers from the new vulnerabilities created by, for example, employer extraction and exploitation of their individual bio data, social media data and, not far off, their personal genome sequence. The paper addresses calls to examine the “institutional intersections” that have informed the changing ways that work is conducted and regulated. These intersections transcend international, national, sectoral and local units of analysis, as well as supply chains, fissured organisational dynamics, intermediaries and online platforms. The analysis also encompasses the broad range of stakeholders including businesses, labour and community groups, nongovernmental organisations and online communities that have influenced changing institutional approaches to employment protection.";Towards a new web of rules: An international review of institutional experimentation to strengthen employment protections;Wright, C., Wood, A. & Trevor, J. et al.;2019;Wood, A.;Digital Economies
This article uses two ethnographic retail case studies to investigate contemporary workplace control. The findings highlight how flexible scheduling has serious consequences for workers and causes insecurity. This provides managers with a powerful and unaccountable mechanism for securing control. The benefits for managers of using flexible scheduling to secure control are shown to be its ambiguity and flexibility. Moreover, flexible scheduling creates an environment where workers must continually strive to maintain managers’ favour. Little evidence is found to suggest that this control is aided by work games obscuring workplace relations. Flexible scheduling does, however, enable misrecognition of workplace relations due to the schedule gifts which it entails. Schedule gifts act to bind workers to managers’ interests through feelings of gratitude and moral obligation.;Powerful Times: Flexible Discipline and Schedule Gifts at Work;Wood, A.;2017;Wood, A.;Digital Economies
This article examines the operation of flexible scheduling in practice through a case study of a large retail firm in the United Kingdom. It includes analysis of 39 semi-structured interviews, participant observation of shop floor work and non-participant observation of union organizing as well as analysis of key documents. The findings highlight the high level of generalized temporal flexibility across employment statuses. This temporal flexibility enables firm flexibility without necessitating a reliance upon contingent workers. Temporal flexibility is found to entail manager-control of flexible scheduling and is shown to be damaging to perceptions of job quality as it acts as a barrier to work-life balance. Union presence and collective bargaining at the firm are found to be ineffective at influencing flexible scheduling so as to improve job quality. This ineffectiveness can be explained by the union operating in an employer-dominated industrial relations environment in which its associational power is unable to compensate for a lack of institutional and structural economic power.;Flexible scheduling, degradation of job quality and barriers to collective voice;Wood, A.;2016;Wood, A.;Digital Economies
For many critics, the growth of insecure work is evidence that United Kingdom (UK) government’s economic policies are failing. It is argued that the declining unemployment of the past 18 months, which the government vaunts as evidence that the UK is back on track to a full economic recovery, is largely due to people accepting the low-quality jobs that have proliferated since the onset of the ‘Great Recession’. For many in the UK, the precarious nature of much of this work is epitomized by so-called ‘zero hours employment’. There is no legal definition of zero hours employment, but the government’s Department of Business Innovation and Skill understands it as employment “in which the employer does not guarantee the individual any work, and the individual is not obliged to accept any work offered.” This description suggests that under such arrangements, both employers and employees have equal and reciprocal rights. In practice though, low-end workers with little labor-market bargaining power and few alternatives have little choice but to accept the diktats of their employer and do any work offered to them. Despite working whenever their employer demands, they have no guarantee of future work, and thus their income is unpredictable and insecure.;Zero Hours Employment: A New Temporality of Capitalism?;Wood, A. & Burchell, B.;2015;Wood, A.;Digital Economies
This article investigates the use of Internet networks during the recent mobilisation of Californian Walmart workers. The findings of this case study suggest that Internet‐based mass self‐communication networks (Facebook, YouTube, etc.) can complement traditional organising techniques. Mass self‐communication networks ameliorate many of the weaknesses identified by previous studies of Internet networks. In particular, these types of networks can help overcome negative dispositions towards unions, increase the density of communication and the level of participation among members, create a collective identity congruent with trade unionism, facilitate organisation and spread ‘swarming actions’ which are effective at leveraging symbolic power. Moreover, unions may be well suited to providing crucial strategic oversight and coordination to wider worker networks.;Networks of injustice and worker mobilisation at Walmart;Wood, A.;2015;Wood, A.;Digital Economies
Income earned through gig platforms, letting platforms, and other digital intermediaries presents new challenges for taxation. This article evaluates the efforts of three European Union Member States – Denmark, Estonia, and France – to obtain data on platform users’ earnings directly from platform companies, including Uber, Airbnb, and domestic platforms. The authors furthermore assess the viability of scaling up the national initiatives into an EU-level “Digital Single Window” that would facilitate the automated reporting of income data by platforms, and the forwarding of that data to national tax and social security agencies for taxation and collection according to national rules.;Taxing Earnings from the Platform Economy: An EU Digital Single Window for Income Data?;Ogembo, D. & Lehdonvirta, V.;2020;Lehdonvirta, V.;"Digital Economies; Information Geography and Inequality"
"How the basic concepts of economics—including markets, institutions, and money—can be used to create and analyze economies based on virtual goods. In the twenty-first-century digital world, virtual goods are sold for real money. Digital game players happily pay for avatars, power-ups, and other game items. But behind every virtual sale, there is a virtual economy, simple or complex. In this book, Vili Lehdonvirta and Edward Castronova introduce the basic concepts of economics into the game developer's and game designer's toolkits. Lehdonvirta and Castronova explain how the fundamentals of economics—markets, institutions, and money—can be used to create or analyze economies based on artificially scarce virtual goods. They focus on virtual economies in digital games, but also touch on serious digital currencies such as Bitcoin as well as virtual economies that emerge in social media around points, likes, and followers. The theoretical emphasis is on elementary microeconomic theory, with some discussion of behavioral economics, macroeconomics, sociology of consumption, and other social science theories relevant to economic behavior. Topics include the rational choice model of economic decision making; information goods versus virtual goods; supply, demand, and market equilibrium; monopoly power; setting prices; and externalities. The book will enable developers and designers to create and maintain successful virtual economies, introduce social scientists and policy makers to the power of virtual economies, and provide a useful guide to economic fundamentals for students in other disciplines.";Virtual Economies: Design and Analysis;Lehdonvirta, V. & Castronova, E.;2014;Lehdonvirta, V.;"Digital Economies; Information Geography and Inequality"
"The use of online surveys has grown rapidly in social science and policy research, surpassing more established methods. We argue that a better understanding is needed, especially of the strengths and weaknesses of non‐probability online surveys, which can be conducted relatively quickly and cheaply. We describe two common approaches to non‐probability online surveys—river and panel sampling—and theorize their inherent selection biases: namely, topical self‐selection and economic self‐selection. We conduct an empirical comparison of two river samples (Facebook and web‐based sample) and one panel sample (from a major survey research company) with benchmark data grounded in a comprehensive population registry. The river samples diverge from the benchmark on demographic variables and yield much higher frequencies on non‐demographic variables, even after demographic adjustments; we attribute this to topical self‐selection. The panel sample is closer to the benchmark. When examining the characteristics of a non‐demographic subpopulation, we detect no differences between the river and panel samples. We conclude that non‐probability online surveys do not replace probability surveys, but augment the researcher's toolkit with new digital practices, such as exploratory studies of small and emerging non‐demographic subpopulations.";Social Media, Web, and Panel Surveys: Using Non‐Probability Samples in Social and Policy Research;Lehdonvirta, V., Oksanen, A. & Räsänen, P. et al.;2020;Lehdonvirta, V.;"Digital Economies; Information Geography and Inequality"
Subscribing to a techno-utopian discourse replacing institutions and experts with “trust in code,” digital alternative currency Bitcoin is pitched as a “math-based money” governed by incorruptible code rather than human regulators. In three cases, which occurred between 2013 and 2015, we examine this system at moments of breakdown. In contrast to the discourse, we find that power is concentrated to critical sites and individuals who manage the system through ad hoc negotiations, and who users must therefore implicitly trust—a contrast we call Bitcoin’s “promissory gap.” But even in the face of such contradictions between premise and reality, the discourse is maintained. We identify four authorizing strategies used in this work: conflating people with devices, assuming actors conform to notions of economic rationality, appealing to technical expertise, and explaining contradictions as temporary bugs. We contend that these strategies are mobilized widely to legitimize a variety of applications of algorithmic regulation and peer production projects.;Mine the gap: Bitcoin and the maintenance of trustlessness;Vidan, G. & Lehdonvirta, V.;2018;Lehdonvirta, V.;"Digital Economies; Information Geography and Inequality"
Gig economy platforms seem to provide extreme temporal flexibility to workers, giving them full control over how to spend each hour and minute of the day. What constraints do workers face when attempting to exercise this flexibility? We use 30 worker interviews and other data to compare three online piecework platforms with different histories and worker demographics: Mechanical Turk, MobileWorks, and CloudFactory. We find that structural constraints (availability of work and degree of worker dependence on the work) as well as cultural‐cognitive constraints (procrastination and presenteeism) limit worker control over scheduling in practice. The severity of these constraints varies significantly between platforms, the formally freest platform presenting the greatest structural and cultural‐cognitive constraints. We also find that workers have developed informal practices, tools, and communities to address these constraints. We conclude that focusing on outcomes rather than on worker control is a more fruitful way to assess flexible working arrangements.;Flexibility in the gig economy: managing time on three online piecework platforms;Lehdonvirta, V.;2018;Lehdonvirta, V.;"Digital Economies; Information Geography and Inequality"
Information and communication technologies are blurring the boundaries between work and play. We present the first empirical investigation of gender gaps in virtual game economies. Analyzing big data sets from two major game economies, we find that player gender and character gender influence virtual wealth in different ways in different games. We conclude that this can be explained by different returns on female- and male-dominated play activities, that is, virtual pink- and blue-collar occupations. As the line between work and play increasingly blurs, researchers should track which occupations get to keep their conventional economic rewards, and which end up being remunerated in play money.;Pink and Blue Pixel$: Gender and Economic Disparity in Two Massive Online Games;Lehdonvirta, V., Ratan, R. & Kennedy, T. et al.;2014;Lehdonvirta, V.;"Digital Economies; Information Geography and Inequality"
What kind of research does Policy & Internet publish? This is a question that has been on my mind frequently since I joined the journal as an editor last September. In this editorial, I will approach the question from two angles. First, I will examine the question empirically, through a brief thematic analysis of all the articles that the journal has published so far, over the 5 years since its launch in 2009. Rather than simply take the journal's stated aims at face value, I will look at what themes and approaches have come out strongest in practice, and consider how the result compares with the original vision. Second, I will consider what kind of research the journal is likely to publish in the future, both in terms of what kind of trends can be seen emerging in policy and Internet research, as well as in terms of what challenges outlined in the journal's original vision that continue to be pertinent today are yet to be fully addressed. In outlining the future agenda, I will also highlight my own focus areas as an editor.;Past and Emerging Themes in Policy and Internet Studies;Lehdonvirta, V.;2014;Lehdonvirta, V.;"Digital Economies; Information Geography and Inequality"
It’s called the Helsinki Spring – a sudden flourishing of Silicon Valley -style entrepreneurial aspiration on Finland’s frigid soil. Students and young people all over Finland are setting up entrepreneurship societies, forming startups, de- veloping business plans and pitching for funding from angel investors. Instead of partying and drinking, they write code and prepare presentations. Instead of organizing protests and demonstrations, they talk at seminars and dream of changing the world through innovations. The media reports about their activities enthusiastically. Political and business leaders are ecstatic. The above is of course an image constructed in the media. The true extent of the startup craze that began around 2010 is still hard to gauge. But since January 2010, at least ten stu- dent entrepreneurship societies have sprung up1, 688 compa- nies have been added to an index of Finnish startups2, and at least two dozen startups have obtained significant funding from international investors. Moreover, startups have entered the public agenda in a big way. In 2011, the Party Secre- tary of the Social Democratic Party Mikael Jungner called the Helsinki Spring a “revolution”. The purpose of this short essay is to examine what kind of a revolution it is. In terms of subject matter and approach, the essay plunges into a chasm between entrepreneurship studies and cultural critique. ;The Helsinki Spring: an essay on entrepreneurship and cultural change ;Lehdonvirta, V.;2013;Lehdonvirta, V.;"Digital Economies; Information Geography and Inequality"
In this article, we describe four case studies of ubiquitous persuasive technologies that support behavior change through personalized feedback reflecting a user’s current behavior or attitude. The first case study is Persuasive Art, reflecting the current status of a user’s physical exercise in artistic images. The second system, Virtual Aquarium, reflects a user’s toothbrushing behavior in a Virtual Aquarium. The third system, Mona Lisa Bookshelf, reflects the situation of a shared bookshelf on a Mona Lisa painting. The last case study is EcoIsland, reflecting cooperative efforts toward reducing CO2 emissions as a set of virtual islands shared by a neighborhood. Drawing from the experience of designing and evaluating these systems, we present guidelines for the design of persuasive ambient mirrors: systems that use visual feedback to effect changes in users’ everyday living patterns. In particular, we feature findings in choosing incentive systems, designing emotionally engaging feedback, timing feedback, and persuasive interaction design. Implications for current design efforts as well as for future research directions are discussed.;Designing motivation using persuasive ambient mirrors;Nakajima, T. & Lehdonvirta, V.;2013;Lehdonvirta, V.;"Digital Economies; Information Geography and Inequality"
Recent years have witnessed the impact of crowdsourcing model, social media, and pervasive computing. We believe that the more significant impact is latent in the convergence of these ideas on the mobile platform. In this paper, we introduce a mobile crowdsourcing platform that is built on top of social media. A mobile crowdsourcing application called UbiAsk is presented as one study case. UbiAsk is designed for assisting foreign visitors by involving the local crowd to answer their image-based questions at hand in a timely fashion. Existing social media platforms are used to rapidly allocate microtasks to a wide network of local residents. The resulting data are visualized using a mapping tool as well as augmented reality (AR) technology, result in a visual information pool for public use. We ran a controlled field experiment in Japan for 6 weeks with 55 participants. The results demonstrated a reliable performance on response speed and response quantity: half of the requests were answered within 10 min, 75% of requests were answered within 30 min, and on average every request had 4.2 answers. Especially in the afternoon, evening and night, nearly 88% requests were answered in average approximately 10 min, with more than 4 answers per request. In terms of participation motivation, we found the top active crowdworkers were more driven by intrinsic motivations rather than any of the extrinsic incentives (game-based incentives and social incentives) we designed.;Drawing on mobile crowds via social media;Liu, Y., Lehdonvirta, V. & Alexandrova, T. et al.;2012;Lehdonvirta, V.;"Digital Economies; Information Geography and Inequality"
Men are more reluctant to seek help for their problems than women. This difference is attributed to social expectations regarding the male gender role. Today, help-seeking is moving online: instead of traditional peer groups and counselors, people depend on online communities and e-counselors. But online users can appear in guises that differ from their physical sex. An empirical study was conducted in an online game to examine whether users’ avatars’ gender influences how they seek and receive help. Analysis is based on user-to-user communications and back-end data. Results indicate that male avatars are less likely to receive sought-for help than female avatars and more likely to be the recipients of indirectly sought help. The authors conclude that avatar gender influences help seeking independent of physical sex: Men overcome their inhibition for help seeking when using female avatars. Practitioners should ensure that means for indirect help seeking are available in order not to exclude male-pattern help seekers.;The Stoic Male: How Avatar Gender Affects Help-Seeking Behavior in an Online Game;Lehdonvirta, M., Nagashima, Y. & Lehdonvirta, V. et al.;2012;Lehdonvirta, V.;"Digital Economies; Information Geography and Inequality"
Research on prosocial behaviour shows that help‐giving differs between the sexes. Gender role theory posits that males specialise in material aid, while females specialize in emotional support. Today, people increasingly help and support each other via online environments. The purpose of this study is to examine whether the genders of avatars used in online interactions influence help‐giving behaviour in similar ways as physical sex does in face‐to‐face situations. An empirical study was conducted using a unique observational data set from a Japanese massively‐multiplayer online game. Instances of help‐giving were identified from conversation logs, coded, and analysed statistically to discover differences between male and female avatars and different help types. Avatar gender is found to influence help‐giving in ways that deviate from expected gender roles: female avatars are more likely than males to provide assistance in the form of material support and labour, and no more likely than males to provide emotional support. Female avatars are more likely to give help to male avatars than other females. The results are explained using behavioural confirmation and self‐perception theory. Physical sex was not controlled for, but most players were male. The study should be repeated in other environments. The results suggest that designers can prime users towards prosocial behaviours by tuning the available line‐up of avatars. The research question and use of observational data are novel. The study is valuable to online educators, companies seeking to reduce customer support costs through peer help, and policymakers dealing with ICTs and societal change.;Prosocial behaviour in avatar‐mediated interaction: the influence of character gender on material versus emotional help‐giving;Lehdonvirta, M., Lehdonvirta, V. & Baba, A.;2011;Lehdonvirta, V.;"Digital Economies; Information Geography and Inequality"
"Transcripts of conversations are a valuable research resource in social sciences and can be used to make inferences about subjects’ behavior and intentions. Large-scale communications records can be coded and analyzed statistically for generalizable results. Virtual environments are a good place to gather communications records, because they exhibit a wide variety of subject behaviors However, compared to traditional channels such as forums and chat rooms, virtual environments can be more challenging to obtain data from. In this article, we describe three approaches to collecting user-to-user communications data from virtual environments: requesting back-end records from the operator of the environment, recruiting “data donors” among the users, and setting up researchers’ own “listening posts”. The data collection approaches are evaluated empirically in Uncharted Waters Online, a Japanese massively-multiplayer game. Avatar gender ratio is used as a diagnostic variable to compare the representativeness of the resulting data sets. Both data donors and listening posts yielded data with a gender ratio that corresponds to the back-end records, but the back-end gender ratios differed significantly between two different servers. We conclude that all three approaches can be statistically viable: the choice of method depends more on desired sampling scope and on practical factors such as resources and timetable; but when defining a sampling frame, it cannot be assumed that one server is necessarily representative of the whole platform. ";Collecting conversations: three approaches to obtaining user-to-user communications data from virtual environments ;Lehdonvirta, M., Lehdonvirta, V. & Baba, A.;2011;Lehdonvirta, V.;"Digital Economies; Information Geography and Inequality"
Computers, mobile phones and other information and communication technologies (ICTs) have become a major part of the everyday life in affluent societies, yet significant socio-demographic disparities remain in their use. Young adults in particular continue to be much more active users of ICTs than the older generations. In this article we explore an approach to understand the institutional implications of ICT usage disparity: the socio-psychological significance of a technology to its users. We argue that identification mediated by technology is for many purposes at least as important of a measure as the actual quantity and quality of their use for many peer groups. Analyses of a nationally representative survey sample collected in 2009 (N = 1202) indicate that young Finns identify with online communities significantly more strongly than their elders do. Overall, however Finns identify much more with traditional offline formations.;Identification with online and offline communities: Understanding ICT disparities in Finland;Näsi, M., Räsänen, P. & Lehdonvirta, V.;2011;Lehdonvirta, V.;"Digital Economies; Information Geography and Inequality"
Peer groups such as neighbourhoods and hobby circles are important sources of social identity for young people, but their viability is challenged by processes of urbanisation and labour mobility. In recent years, traditional peer groups have been joined by easily accessible computer-mediated groups, which have become an everyday part of life in many countries. In this article, we examine how young people identify with various online and offline peer groups. We compare online and offline identification experiences from the perspective of how socio-demographic position and individual sociability characteristics influence them, and examine how these identification processes differ between national contexts. Empirical analyses are conducted based on a survey of online community users from the UK, Spain and Japan (N=4299). It is found that participants identify as strongly with their online communities as they do with their own families, and stronger than with offline hobby groups. In the mature online societies of the UK and Japan, the online group provides a more socio-demographically inclusive source of identification than traditional leisure-time formations. As friends and family move online, affinity towards online groups is more likely to be a reflection of high sociability than a lack of it. Games, social networking sites and other online environments should be seen as crucial contexts for today's youth's socialisation and identification experiences.;How do young people identify with online and offline peer groups? A comparison between UK, Spain and Japan;Lehdonvirta, V. & Räsänen, P.;2010;Lehdonvirta, V.;"Digital Economies; Information Geography and Inequality"
"In early 2009, in the middle of the so-called financial crisis, Nobel Prize-winning economist George Akerlof and Yale economist Robert Shiller published a book in which they argue for the following claim: To understand how economies work and how we can manage them and prosper, we must pay attention to the thought patterns that animate people’s ideas and feelings, their animal spirits. We will never really understand important economic events unless we confront the fact that their causes are largely mental in nature. (Akerlof and Shiller, 2009: 1) In the history of the study of economic policies and behaviours, there is a well-known pattern where a crisis in the real economy results in a change of the dominant approach used to understand it. The Great Depression of the 1930s resulted in neoclassical dogma being replaced with Keynesian economics. The oil crisis of the 1970s resulted in the Keynesian system being scrapped in favour of monetary economics. The latest global economic downturn, suggest Akerlof and Shiller, highlights a need for psychologically and sociologically informed understandings of economic behaviour. In sociology and anthropology, there is a long tradition of scholarship dealing with behaviour that is today considered part of the economic sphere. This tradition is based on the observation that one of the most important ways in which we relate to each other and ourselves is through material objects (Douglas and Isherwood, 1978; McCracken, 1990; Mauss, 1990). Gifts express love and gratitude. Possessions establish social standing. Dress and accessories organize people and time into occupations and leisure activities.";Online spaces have material culture: goodbye to digital post-materialism and hello to virtual consumption;Lehdonvirta, V.;2010;Lehdonvirta, V.;"Digital Economies; Information Geography and Inequality"
Selling virtual goods for real money is an increasingly popular revenue model for massively- multiplayer online games (MMOs), social networking sites (SNSs) and other online hangouts. In this paper, we argue that the marketing of virtual goods currently falls short of what it could be. Game developers have long created compelling game designs, but having to market virtual goods to players is a relatively new situation to them. Professional marketers, on the other hand, tend to overlook the internal design of games and hangouts and focus on marketing the services as a whole. To begin bridging the gap, we propose that the design patterns and game mechanics commonly used in games and online hangouts should be viewed as a set of marketing techniques designed to sell virtual goods. Based on a review of a number of MMOs, we describe some of the most common patterns and game mechanics and show how their effects can be explained in terms of analogous techniques from marketing science. The results provide a new perspective to game design with interesting implications to developers. Moreover, they also suggest a radically new perspective to marketers of ordinary goods and services: viewing marketing as a form of game design. ;Game design as marketing: How game mechanics create demand for virtual goods ;Hamari, J. & Lehdonvirta, V.;2010;Lehdonvirta, V.;"Digital Economies; Information Geography and Inequality"
Millions of people around the world are spending billions of euros per year on virtual items, characters and currencies in online games, social networking sites, and other digital hangouts. In this paper, we examine this shift in consumer behavior and business models from a public policy perspective. We present three case studies to examine the key policy issues that virtual goods are giving rise to, and analyze some of the regulatory responses that have been effected so far: judicial protection of the possession of virtual goods in Finland and the Netherlands, statutory regulation of virtual goods trade in Korea, and application of consumer protection law to virtual goods sales in Finland. As with the debate over copyright, the first big content policy debate of the digital era, this new digital policy debate tends to pit individual consumers and entrepreneurs against the interests of publishers and established public policy. However, the roles are curiously reversed: it is not the publishers but the consumers who demand that pieces of digital content be respected as property, and turn to courts to enforce their view. While copyright and virtual goods both aim to impose artificial scarcity on non‐rivalrous matter, copyright is designed to provide economic incentives to producers, while in virtual goods scarcity provides benefits to consumers directly.;A New Frontier in Digital Content Policy: Case Studies in the Regulation of Virtual Goods and Artificial Scarcity;Lehdonvirta, V. & Virtanen, P.;2012;Lehdonvirta, V.;"Digital Economies; Information Geography and Inequality"
I argue that much influential scholarship on massively-multiplayer online games and virtual environments (MMO) is based on a dichotomous “real world vs. virtual world” model. The roots of this dichotomy can be traced to the magic circle concept in game studies and the cyberspace separatism of early Internet thought. The model manifests on a number of dimensions, including space, identity, social relationships, economy and law. I show a number of problems in the use of this model in research, and propose an alternative perspective based on Anselm Strauss’s concept of overlapping social worlds. The world of players does not respect the boundaries of an MMO server, as it frequently flows over to other sites and forums. At the same time, other social worlds, such as families and workplaces, penetrate the site of the MMO and are permanently tangled with the players' world. Research programs that approach MMOs as independent mini-societies are therefore flawed, but there are many other kinds of research that are quite feasible.;Virtual Worlds Don't Exist: Questioning the Dichotomous Approach in MMO Studies;Lehdonvirta, V.;2010;Lehdonvirta, V.;"Digital Economies; Information Geography and Inequality"
The global market for virtual items, characters and currencies was estimated to exceed 2.1 Billion USD in 2007. Selling virtual goods for real money is an increasingly common revenue model not only for online games and virtual worlds, but for social networking sites and other mainstream online services as well. What drives consumer spending on virtual items is an increasingly relevant question, but little research has been devoted to the topic so far. Previous literature suggests that demand for virtual items is based on the items’ ability to confer gameplay advantages on one hand, and on the items’ decorative value on the other hand. In this paper, I adopt a perspective from the sociology of consumption and analyse examples from 14 virtual asset platforms to suggest a more detailed set of item attributes that drive virtual item purchase decisions, consisting of functional, hedonic and social attributes.;Virtual item sales as a revenue model: identifying attributes that drive purchase decisions;Lehdonvirta, V.;2009;Lehdonvirta, V.;"Digital Economies; Information Geography and Inequality"
Selling virtual items for real money is increasingly being used as a revenue model in games and other online services. To some parents and authorities, this has been a shock: previously innocuous ‘consumption games’ suddenly seem to be enticing players into giving away their money for nothing. In this article, we examine the phenomenon from a sociological perspective, aiming to understand how some media representations come to be perceived as ‘virtual commodities’, what motivations individuals have for spending money on these commodities, and how the resulting ‘virtual consumerism’ relates to consumer culture at large. The discussion is based on a study of everyday practices and culture in Habbo Hotel, a popular massively-multiuser online environment permeated with virtual items. Our results suggest that virtual commodities can act in essentially the same social roles as material goods, leading us to ask whether ecologically sustainable virtual consumption could be a substitute to material consumerism in the future.;VIRTUAL CONSUMERISM;Lehdonvirta, V., Wilska, T-A. & Johnson, M.;2009;Lehdonvirta, V.;"Digital Economies; Information Geography and Inequality"
The European Union Data Protection Directive requires member states to place restrictions on transfers of personal data to countries that cannot guarantee an adequate level of data protection. Countries that do guarantee adequate protection enjoy a smooth business environment and an enhanced ability to participate in trade. In this paper I examine the adequacy of Singapore's data protection regime, and in particular the Model Data Protection Code. I suggest various amendments to the regime to enable Singapore to meet the Directive requirements. To carry out the assessment, I use a framework developed by the Article 29 Working Party, the body that in practice carries out the official adequacy assessments forthe EU. ;EUROPEAN UNION DATA PROTECTION DIRECTIVE: ADEQUACY OF DATA PROTECTION IN SINGAPORE ;Lehdonvirta, V.;2004;Lehdonvirta, V.;"Digital Economies; Information Geography and Inequality"
Marx posited that labour is “disciplined, united, organized by the very mechanism of the process of capitalist production” (1906: 836-37). The regimented nature of factory work and life in an industrial community provided the material basis for collective action and for the shared identity required to support it. But is this still true of the mechanisms of 21st-century informational capitalism? Castells notes that in informational capitalism, “[t]he work process is globally integrated, but labor tends to be locally fragmented” (Castells 2000: 18). The exploitation of global wage, skill, and regulatory differentials means that workers are often physically, temporally, and administratively detached and desynchronized from each other (Ashford et al. 2007). In the extreme case, coordination of workers’ efforts is achieved algorithmically, that is, by automated data and rule based decision making (O’Reilly 2013), leaving no opportunity for human-to-human communication. Under such dispersal and disconnection, it would seem difficult for a common identity, let alone effective organization, to arise among workers. Yet algorithms can also unite. Information and communication technologies (ICTs) have long been used to construct ‘sites of resistance’ that bring together people prevented from organizing via conventional means (Ho and Zaheer 2002). Sites or communities formed online can offer potent identification experiences that rival the degree of identification with conventional workplaces (Lehdonvirta and Räsänen 2011). ICTs are used as part of almost any campaign of political mobilization today, at least in the industrialized countries (Karpf 2010, Wells 2014). To what extent, then, can dispersed informational labourers make use of ICTs to re-establish links, develop shared identities, and mobilize for collective action? In this chapter, we will examine both the dynamics of dispersal as well as the dynamics of unification in informational labour, and the technological, organizational, and identity processes that underlie them. These topics are examined via an empirical study of ‘microwork’, an extreme example of commodified and delocalized knowledge work. We study three different ‘microwork platforms’, or companies that provide microwork opportunities, and their workers. Microwork refers to work consisting of the remote completion of small information processing tasks, such as transcribing a snippet of hand-written text, classifying an image, or categorizing the sentiment expressed in a comment (Lehdonvirta and Ernkvist 2011, Kittur et al. 2013). The oldest and most well known microwork platform is Amazon Mechanical Turk (‘MTurk’), operated by Seattle- based e-commerce giant Amazon. A worker enters the site using their own or borrowed computer or mobile device, selects a task, completes it, is credited with the proceeds, and selects the next task. MTurk started as a way for Amazon to source workers for its own information processing needs, but evolved into an open marketplace where any U.S.-based employer can post digital tasks for the site’s users to complete. At the time of writing, over 300,000 such tasks are listed. Each completed task earns the worker-user a small remuneration, typically ranging from a few cents to a dollar or two. Two main theoretical claims are developed throughout the chapter. One is that we must distinguish between delocalized work that is subsequently relocalized elsewhere, and delocalized work that remains dispersed. Relocalization is exemplified by the offshoring of work to business process service centres, where the work is performed at least partially in the context of local institutions and social networks. Dispersal is exemplified by the outsourcing of work to a platform such as MTurk, where the work is detached from local institutions and workers are dispersed. Of note is that workers may remain geographically proximate to each other and to the employer, while a state of dispersal in the sense of social and institutional disconnectedness is achieved by organizational and technological means. Distinguishing between relocalized and dispersed work is important, because these two modes of or outcomes from delocalization can have very different implications to the nature of the work and the institutions that regulate it. The second theoretical point developed in this chapter is that the effectiveness of online self-organizing as a platform for collective action in the labour market depends on the topology of such ‘virtual places’, and how it matches with the contours of the market it is intended to influence. The algorithms and social processes that shape the memberships of online communities may leave such communities ill organized to exert collective influence on a particular employer or market segment. Boundaries of shared identities are particularly important in this regard. The rest of the chapter is organized as follows. First, we will draw on previous literature to discuss the organizational and technological processes through which microwork platforms produce placelessness and dispersal. We argue that we must also understand what impacts these processes have on identity formation in order to understand their full implications to collective action. We will then introduce the empirical study, consisting mainly of participant observation and interviews of 30 microworkers. We first use the empirical study to examine microwork in everyday life and its consequences to organizational identity. We will then use the study to examine attempts to counter dispersal and reunite microworkers, both on the material level of bringing dispersed workers together in ‘virtual places’, as well as on the ideological level of developing occupational and class identities. In the final section, we will discuss the main findings in the context of some earlier work on service value chain restructuring and freelance knowledge workers, highlighting theoretical and policy implications. ;Algorithms That Divide and Unite: Delocalization, Identity, and Collective Action in ‘Microwork’ ;Lehdonvirta, V.;2016;Lehdonvirta, V.;"Digital Economies; Information Geography and Inequality"
The purpose of this chapter is to examine how information society interacts with consumer society. I trace a brief history of the digitalization of consumption, starting from the online retail sites of 1990s and ending with digital virtual consumption in online games and communities. On the way, I ask what changes, if any, digitalization has brought about in the sites, processes, subjects and objects of consumption. Consumers have not necessarily become any less materialistic despite goods turning digital. But virtualized consumerism presents a new hope for environmentalists.;The rest of the chapter is organized as follows. First, we will draw on previous literature to discuss the organizational and technological processes through which microwork platforms produce placelessness and dispersal. We argue that we must also understand what impacts these processes have on identity formation in order to understand their full implications to collective action. We will then introduce the empirical study, consisting mainly of participant observation and interviews of 30 microworkers. We first use the empirical study to examine microwork in everyday life and its consequences to organizational identity. We will then use the study to examine attempts to counter dispersal and reunite microworkers, both on the material level of bringing dispersed workers together in ‘virtual places’, as well as on the ideological level of developing occupational and class identities. In the final section, we will discuss the main findings in the context of some earlier work on service value chain restructuring and freelance knowledge workers, highlighting theoretical and policy implications. ;Lehdonvirta, V.;2012;Lehdonvirta, V.;"Digital Economies; Information Geography and Inequality"
Research is increasingly highlighting the potential for situated crowdsourcing to overcome some crucial limitations of online crowdsourcing. However, it remains unclear whether a situated crowdsourcing market can be sustained, and whether worker supply responds to price-setting in such a market. Our work is the first to systematically investigate workers' behaviour and response to economic incentives in a situated crowdsourcing market. We show that the market-based model is a sustainable approach to recruiting workers and obtaining situated crowdsourcing contributions. We also show that the price mechanism is a very effective tool for adjusting the supply of labour in a situated crowdsourcing market. Our work advances the body of work investigating situated crowdsourcing.;Situated crowdsourcing using a market model;Hosio, S., Goncalves, J. & Lehdonvirta, V. et al.;2014;Lehdonvirta, V.;"Digital Economies; Information Geography and Inequality"
In this paper we present a on-field study for evaluating a crowd sourcing mobile social search application. With the help of the local crowd via social medias, this application assists foreign visitors in Japan by answering their image-based questions at hand in a timely fashion. We ran a controlled field experiment for 6 weeks with 55 participants. We found that the mobile crowd sourcing model demonstrated a reliable performance on response speed and response quantity: half of the requests were answered within 10 minutes, 75% of requests were answered within 30 minutes, and on average every request had 4.2 answers. Especially in the afternoon, evening and night, nearly 88% requests were answered in average approximately 10 minutes, with more than 4 answers per request. In terms of participation motivation, we found the top active crowd workers were more driven by intrinsic motivations rather than any of the extrinsic incentives (gamification incentives and social incentives) we designed.;Mobile Image Search via Local Crowd: A User Study;Liu, Y., Alexandrova, T. & Nakajima, T. et al.;2011;Lehdonvirta, V.;"Digital Economies; Information Geography and Inequality"
In this paper, we further develop the idea of combining pervasive computing techniques with electronic payment systems to create activity-based micro-incentives. Economic incentives are an effective way to influence consumer behavior, and are used in e.g. marketing and resource coordination. Our approach allows marketers and regulators to induce consumers to perform particular actions in new application domains by attaching micro-prices to a wider range of behaviors. A key challenge is designing incentive mechanisms that result in desired behavior changes. We examine two basic incentive models. Based on the results of preliminary experiments, we discuss how economic incentives can affect consumer attitudes and lead to sustainable behavior changes.;Activity-Based Micro-pricing: Realizing Sustainable Behavior Changes through Economic Incentives;Yamabe, T., Lehdonvirta, V. & Ito, H. et al.;2010;Lehdonvirta, V.;"Digital Economies; Information Geography and Inequality"
Implementing an electronic payment system involves striking a balance between usability and security. Systems that allow payments to be completed with little effort on the part of the consumer, such as smart cards, carry a higher probability of incorrect payments. Systems that eliminate the possibility of incorrect payments by requiring explicit approval from the consumer, such as credit cards, make the system too cumbersome for small payments. In this paper, we model the usability and security tradeoff as a problem of minimizing the transaction cost imposed by the payment system on the consumer. We propose a mobile payment scheme called UbiPay that attempts to push this transaction cost towards zero by offering a range of user interaction modes and choosing the minimum sufficient one based on context data. The aim is to make paying like breathing: something we are only peripherally aware of unless we exert our resources beyond the usual. Results from a user study on a prototype system suggest that the concept is feasible. The idea has powerful implications for the economic organization of everyday life.;UbiPay: minimizing transaction costs with smart mobile payments;Lehdonvirta, V., Soma, H. & Ito, H. et al.;2009;Lehdonvirta, V.;"Digital Economies; Information Geography and Inequality"
A significant portion of the carbon dioxide emissions that have been shown to cause global warming are due to household energy consumption and traffic. EcoIsland is a computer system aimed at persuading and assisting individual families in changing their lifestyle patterns in a way that reduces CO 2 emissions. The system builds on our earlier work on persuasive ubiquitous computing applications and applies ideas from behaviorism, social psychology and emissions trading to attempt to motivate changes in users' behavior. In this paper, we briefly describe the concept and the theories behind it, and provide preliminary results from a user study measuring its effectiveness.;ECOISLAND: A System for Persuading Users to Reduce CO2 Emissions;Takayama, C., Lehdonvirta, V. & Shiraisi, M. et al.;2009;Lehdonvirta, V.;"Digital Economies; Information Geography and Inequality"
This paper aims to discuss the roles for the two types of tracking user behavior. Considering these two types of tracking, sensor based recognition has a great advantage when sensing human activity, but it is not always adequate when tracking in the real world. In this paper, we compare the benefits and drawbacks of sensor-based tracking versus self-reported data in persuasive applications called EcoIsland.;Tracking behavior in persuasive apps: is sensor-based detection always better than user self-reporting?;Shiraisi, M., Washio, Y. & Takayama, C. et al.;2009;Lehdonvirta, V.;"Digital Economies; Information Geography and Inequality"
This paper presents EcoIsland, which is a system persuading individuals and families to change their lifestyle patterns to reduce CO2 emissions. EcoIsland visualizes the user's current eco-friendly behavior as an island shared by his/her family members. Several persuasive techniques developed in behaviorism, social psychology, and economy are used to offer incentives to him/her to encourage eco-friendly behavior. We examine and compare the implementation and effectiveness of different types of persuasive techniques in several user studies.;Using individual, social and economic persuasion techniques to reduce CO2 emissions in a family setting;Shiraisi, M., Washio, Y. & Takayama, C. et al.;2009;Lehdonvirta, V.;"Digital Economies; Information Geography and Inequality"
"Ambient lifestyle feedback systems are embedded computer systems designed to motivate changes in a person's lifestyle by reflecting an interpretation of targeted behavior back to the person. Other interactive systems including ""serious games"" have been applied for the same purpose in areas such as nutrition, health and energy conservation, but they suffer from drawbacks such as inaccurate self-reporting, burdens placed on the user, and lack of effective feedback. Ambient lifestyle feedback systems overcome these challenges by relying on passive observation, calm presentation style and emotionally engaging feedback content. In this paper, we present an ambient lifestyle feedback system concept and provide insights from the design and implementation of two prototype systems, Virtual Aquarium and Mona Lisa Bookshelf. In particular, we discuss the theory and practice of effective feedback design by drawing on elementary behavioral psychology and small-scale user studies. The work is aimed at aiding in the design of ambient persuasive technologies and ambient interaction in general.";Reflecting human behavior to motivate desirable lifestyle;Nakajima, T., Lehdonvirta, V. & Tokunaga, E. et al.;2008;Lehdonvirta, V.;"Digital Economies; Information Geography and Inequality"
As services embedded into public spaces become increasingly transparent, one peripheral aspect of use continues to demand explicit user attention: payment. UbiPay is a system that carries out small everyday payments in a way that minimises user involvement by choosing an interaction method based on context information. The aim is to make paying like breathing: something we are only peripherally aware of unless we exert our resources beyond the usual. This idea has powerful implications for business and design.;Ubipay: conducting everyday payments with minimum user involvement;Lehdonvirta, V., Soma, H. & Ito, H. et al.;2008;Lehdonvirta, V.;"Digital Economies; Information Geography and Inequality"
In recent years, the deteriorations of living habits like immobilization or unhealthy diet are becoming serious social problems in many developed countries. Even if we know the importance, it is difficult to change our undesirable habits and to maintain a desirable lifestyle. This study demonstrates a concept called ambient lifestyle feedback systems to be used to motivate people to change their undesirable habits to improve their lifestyle. In the concept, aesthetic and empathetic expressions reflect the feedback of the user’s current behavior to the user. When keeping desirable habits, the user is offered with a feedback expression designed to boost his positive emotion. When turning to undesirable habits, the feedback expression is changed to increase the user’s negative emotions. In this paper, we present brief overviews of four case studies of ambient lifestyle feedback systems, and discuss several findings that we while designing and evaluating the case studies. Future directions will also be discussed.;Using Aesthetic and Empathetic Expressions to Motivate Desirable Lifestyle;Nakajima, T., Kimura, H. & Yamabe, T. et al.;2008;Lehdonvirta, V.;"Digital Economies; Information Geography and Inequality"
In this paper, we propose a novel computer gaming style Lifestyle Ubiquitous Gaming, where our experience is enriched in our daily lives by using gaming concepts. In lifestyle ubiquitous gaming, human daily activities are implicitly tracked and interpreted. Then, the feedback of the activities are returned to a user as a game system' presentation. The user can enjoy the game without too much conscious. This characteristic has a very big difference from traditional gaming. We describe the motivation and conceptual framework of lifestyle ubiquitous gaming, and show two case studies that we have conducted to demonstrate our proposal's feasibility and effectiveness.;Lifestyle Ubiquitous Gaming: Making Daily Lives More Plesurable;Nakajima, T., Lehdonvirta, V. & Tokunaga, E. et al.;2007;Lehdonvirta, V.;"Digital Economies; Information Geography and Inequality"
"Online research methods have come of age as the permeation of everyday life by infor- mation and communication technologies has grown ever more ubiquitous. Although sub- stantial digital divides remain by country, and within countries by age, gender and socioeconomic status, the number of Internet users worldwide quadrupled between 2000 and 2014, and the current proportion of the world’s population using the Internet is now said to be in excess of 40 per cent (International Telecommunications Union, 2015). Information and communication tech- nologies have had socially transformative effects. They increasingly affect how people make and maintain social relationships, the structure of their social networks, how they go about their work, meet their partners, edu- cate their children, how they shop, take their leisure, present themselves to the world and store their memories. Such things are, of course, of interest to social scientists in and of themselves. However, to study them also requires methods of communication, ways of harvesting and capturing information, obser- vational strategies and tools for collabora- tion, not to mention analytic techniques adapted to what are often novel forms and volumes of data, all of which themselves have the capacity to be transformed by new technologies. Introducing the first edition of The SAGE Handbook of Online Research Methods, we emphasised the newness of online meth- ods, and the need for a cautious and critical appraisal of their use and potential. Less than a decade onwards, the terrain occupied by online research methods has changed rap- idly, social researchers across a wide range of social science disciplines have become much more familiar with such methods, more adept at their use, and more attuned to the issues and challenges that they pose. As before, our primary purpose in this Handbook is to explore this terrain by highlighting across a wide range of areas the key facets of online research methods and their implications for practice. Given our focus, as was true of the first edition, we pay relatively little attention to theoretical discourses on the wider social or cultural significance of online environ- ments. While we recognise the historically contingent and socially constructed nature of the changes wrought by development of Internet-based technologies, we leave inves- tigation of such issues to others. So too, we take largely as a given the infrastructural ‘substrate’ (Star, 1999) that underpins online practices; the standards, protocols, mecha- nisms, tools and resources without which activity online would be impossible. Neither do we address in any systematic way the driv- ers of methodological innovation in the social sciences and the social processes that have allowed new online methodologies to be adopted, diffused and used. The Handbook, in other words, retains a pragmatic focus on the current state of the art and on the further potential of online research methods in the social sciences. ";Online Research Methods in the Social Sciences: An Editorial Introduction ;Lee, R., Fielding, N. & Blank, G.;2008;Blank, G.;Information Geography and Inequality
"We examine the dimensions of Internet use based on a representative sample of the population of the UK, making three important contributions. First, we clarify theoretical dimensions of Internet use that have been conflated in prior work. We argue that the property space of Internet use has three main dimensions: amount of use, variety of different uses, and types of use. Second, the Oxford Internet Survey 2011 data set contains a comprehensive set of 48 activities ranging from email to online banking to gambling. Using the principal components analysis, we identify 10 distinctive types of Internet activities. This is the first typology of Internet uses to be based on such a comprehensive set of activities. We use regression analyses to validate the three dimensions and to identify the characteristics of the users of each type. Each type has a distinctive and different kind of user. The Internet is an extremely diverse medium. We cannot discuss ‘Internet use’ as a general phenomenon; instead, researchers must specify what kind of use they examine.";Dimensions of Internet use: amount, variety, and types;Blank, G. & Groselj, D.;2014;Blank, G.;Information Geography and Inequality
This book presents state of the art theoretical and empirical research on the ubiquitous internet: its everyday users and its economic stakeholders. The book offers a 360-degree media analysis of the contemporary terrain of the internet by examining both user and industry perspectives and their relation to one another. Contributors consider user practices in terms of internet at your fingertips—the abundance, free flow, and interconnectivity of data. They then consider industry’s use of user data and standards in commodification and value-creation.;Next Generation Users: Changing Access to the Internet;Blank, G. & Dutton, W.;2014;Blank, G.;Information Geography and Inequality
A major worry over the Internet is its potential to undermine the business model and value of traditional newspapers, the home of quality, critical, investigative journalism. Using the UK as a case study, this chapter draws from survey research of individuals, and features evidence on patterns of news readership among Internet users and non-users, and qualitative case studies of developments in online news organizations based on interviews and log files of journalistic sites. There has been a step-jump in the use of online news since 2003, but generally as a complement to print sources. However, access seems to have been leveling off since 2009, partly reflecting the move to social networks where people are increasing referred to news. Instead of any simple substitution of online for offline news, this chapter argues that the Internet and social media are contributing to a more complex ecology of news production and consumption.;Social Media and the News: Implications for the Press and Society;Newman, N., Dutton, W. & Blank, G.;2014;Blank, G.;Information Geography and Inequality
In this talk, Dr Blank and Professor Dutton explain how a new pattern of Internet access is developing through the use of a growing variety of devices than enable increasing mobility. Survey research on Internet use in Britain has highlighted two dramatic and interrelated shifts in how users are accessing the Internet. From our early study of Internet use in 2003, the primary pattern of Internet access was based on the use of a personal computer in one’s household, and at times complemented by similar access at the workplace, linked to the Internet through a modem or broadband connection. The major change in access since 2003 was around the speed of connections, with the major trend being the uptake of broadband Internet until 2009, by when nearly all Internet users used a broadband connection. This dominant pattern of Internet access characterizes the ‘first generation user’ in Britain. In contrast to this first generation of Internet users, there is a new pattern of Internet access developing through the use of a growing variety of devices than enable increasing mobility. Laptops, smart phones, tablet computers, and readers are providing a multitude of entry points that most often complement but occasionally replace the centrality of the household personal computer. We call those who link to the Internet in this increasingly mobile style as the ‘next generation users’ (NGUs). Who are the next generation users, the more tethered users, and non-users? Additionally, socioeconomic divides and the choices of many individuals not to use the Internet are socially distributed in ways that reignite issues over digital divides in society.;Next Generation Internet Users: Digital Divides, Choices, and Inequalities ;Blank, G. & Dutton, W.;2012;Blank, G.;Information Geography and Inequality
"The Internet is central to the new media, but the Internet is itself a dynamic technology that is constantly evolving as users adopt and reject new features, devices and applications and use them in ways that are often unanticipated. This article is anchored in longitudinal survey data on how Britons use the Internet, which illuminates the emergence of new patterns of accessing the Internet over multiple devices—some of which are portable—in everyday life and work. We call those who adopt this new approach ‘next generation users’. In contrast, first generation users remain anchored to one or more personal computers in the household or workplace for accessing the Internet. The analysis shows how this emerging pattern of access is reshaping the use and impact of the Internet, such as in supporting the production of user generated content. The analysis also shows how next generation access is socially distributed; creating a new digital divide that reinforces socioeconomic inequalities. Future research needs to move beyond the study of access to the Internet to track the diffusion of next generation access and its implications across a wider array of nations.";The Emergence of Next‐Generation Internet Users;Blank, G. & Dutton, W.;2013;Blank, G.;Information Geography and Inequality
Sociological studies on the Internet have often examined digital inequalities. These studies show how Internet access, skills, uses and outcomes vary between different population segments. However, we know more about social inequalities in general Internet use than in social media use. Especially, we lack differentiated statistical evidence of the social profiles of distinct social media platforms. To address this issue, we use a large survey data set in the United Kingdom and investigate the social structuration of six major social media platforms. We find that age and socio-economic status are driving forces of several -- but not all -- of these platforms. Aggregating platform adoption into a general measure of social media use blurs some of the subtleties of more fine-grained indicators, namely platform uses and specific activities, such as status updating and commenting.;The Social Structuration of Six Major Social Media Platforms in the United Kingdom: Facebook, LinkedIn, Twitter, Instagram, Google+ and Pinterest;Blank, G. & Lutz, C.;2016;Blank, G.;Information Geography and Inequality
"There is a widespread impression that younger people are less concerned with privacy than older people. For example, Facebook founder Mark Zuckerberg justified changing default privacy settings to allow everyone to see and search for names, gender, city and other information by saying “Privacy is no longer a social norm”. We address this question and test it using a representative sample from Britain based on the Oxford Internet Survey (OxIS). Contrary to conventional wisdom, OxIS shows a negative relationship between age and privacy; young people are actually more likely to have taken action to protect their privacy than older people. Privacy online is a strong social norm. We develop a sociological theory of privacy that accounts for the fact of youth concern. The new privacy paradox is that these sites have become so embedded in the social lives of users that they must disclose information on them despite the fact that these sites do not provide adequate privacy controls.";A New Privacy Paradox: Young People and Privacy on Social Network Sites;Blank, G., Bolsover, G. & Dubois, E.;2014;Blank, G.;Information Geography and Inequality
This paper looks at how the production and consumption of news is changing in the UK. It draws from survey research of individuals in Britain from 2003-2011, which includes evidence on patterns of news readership among Internet users and non-users, as well as more qualitative case studies of developments in online news organizations, based on interviews and log files of journalistic sites. Survey evidence has shown a step-jump in the use of online news since 2003, as a complement to print news reading, but a leveling off since 2009. However, this relative stability in news consumption masks a change in the growing role of social networks, both as a substitute for search in many cases, but also in their relationship with online newspapers, as the interaction of mainstream news and networked individuals has begun to reshape the ecology of production and consumption. Institutionally the paper argues that these patterns underscore recent changes in news media, such as their continued reliance on the Internet, but also added competition from social media, which are becoming a major portal to the Internet. Individually we see the empowerment of networked individuals of a Fifth Estate who have achieved a growing independence from the Fourth Estate as more information moves online and individuals become routinely linked to the Internet. However, a growing synergy between the Fourth and Fifth Estate might be one of the more important aspects of the new news ecology.;Social Media in the Changing Ecology of News Production and Consumption: The Case in Britain;Newman, N., Dutton, W. & Blank, G.;2011;Blank, G.;Information Geography and Inequality
The platform economy is rapidly transforming the dynamics of the labor market. Optimists argue platform work functions as a social equalizer, opening opportunities for additional earnings for those who need it most. Pessimists suggest that the platform economy widens earning disparities by providing additional income to people who already have good jobs. We contribute to this debate by examining who participates in the platform economy and their motivation for participation, using a US nationally representative sample. Our findings offer support for both perspectives. Those who participated in labor-exchange platforms were more likely to come from disadvantaged backgrounds. By contrast, those who participated in online selling platforms were more likely to come from more affluent backgrounds. When we further examined different types of platform work, we found that different types of platform work were performed by different demographic and social groups. In addition, participation in some platform work, such as rideshare driving and house/laundry cleaning, is motivated out of necessity, while other platform work, such as selling used goods and performing online tasks, is generally used to supplement incomes. Distinct occupations tend to benefit different social groups in different ways and, taken together, disadvantaged groups are less likely to perform types of platform work that would improve their economic position and reduce income disparities. This tends to offer more support for the pessimist’s perspective. We conclude that the platform economy is strongly segregated by occupation and it should be examined as a set of distinct occupations rather than a homogenous industry.;The winners and the losers of the platform economy: who participates?;Hoang, L., Blank, G. & Quan-Haase, A.;2020;Blank, G.;Information Geography and Inequality
Framed by domestication theory, affordances and use genres, this study explores early adopters’ uses of smart speaker assistants (SSAs), like Amazon Echo (Alexa) and Google Home. Based on semi-structured, in-depth interviews, we develop a typology of use genres, and describe spatially distributed uses. The interviews revealed six use genres that go beyond the well-known convenience and entertainment. Specifically, the use genres of companionship, self-control and productivity, sleep aid, health care, peace of mind and increased accessibility emerged from participants’ accounts. In addition, we found spatially distributed uses based on the users’ perception of the spatial affordances of SSAs. These spatially distributed uses lead us to propose the process of externalization as a necessary extension of domestication theory for the appropriation of networked devices.;Externalized domestication: smart speaker assistants, networks and domestication theory;Brause, S. & Blank, G.;2020;Blank, G.;Information Geography and Inequality
Based on the 2019 Oxford Internet Surveys (OxIS), this working paper provides detail on the rise of mobile devices and the mobile Internet and how they are used, identifying several modes of use defined by the combination of mobile devices, computers and the Internet. The analysis then looks in detail at who uses and does not use mobile Internet alone or in conjunction with other devices, arguing that this could make a substantial difference the role mobile technology will play in society. In the course of this discussion, the paper provides evidence to address several key questions: Are mobile Internet services substituting or complementing other forms of Internet access? Is mobile changing what people do online? Has the household remained the major place from which people access the Internet?;OxIS 2019: The Rise of Mobile Internet Use in Britain;Blank, G., Dutton, W. & Lefkowitz, J.;2020;Blank, G.;Information Geography and Inequality
The existence of a ‘digital divide’ has been one of the key social issues of the Internet since its early diffusion at the turn of the twenty-first century. Over time, as access to the Internet has become increasingly central to everyday life, those without access to broadband infrastructures, digital devices, and Internet skills have been socially, politically and economically disadvantaged. Therefore, critical questions remain about levels of access and skills that shape who uses and does not use the Internet, why, and what difference this makes. This report summarizes recent broad changes in the digital divide in Britain. The digital divide has narrowed but about 15 percent of the British population remains offline. At the same time, those who are online have markedly intensified their use. They use more devices and do more online. The nuances of these trends are fleshed out using 23 graphics and accompanying commentary. In addition to fundamental demographic factors like age, education, income and literacy, we present data on the relationships between divides and variables such as gender, employment marital status, ethnicity, disability, urban/rural residence, social grade and children in the household.;OxIS 2019: Digital divides in Britain are narrowing but deepening;Blank, G.;2020;Blank, G.;Information Geography and Inequality
In a high-choice media environment, there are fears that individuals will select media and content that reinforce their existing beliefs and lead to segregation based on interest and/or partisanship. This could lead to partisan echo chambers among those who are politically interested and could contribute to a growing gap in knowledge between those who are politically interested and those who are not. However, the high-choice environment also allows individuals, including those who are politically interested, to consume a wide variety of media, which could lead them to more diverse content and perspectives. This study examines the relationship between political interest as well as media diversity and being caught in an echo chamber (measured by five different variables). Using a nationally representative survey of adult internet users in the United Kingdom (N = 2000), we find that those who are interested in politics and those with diverse media diets tend to avoid echo chambers. This work challenges the impact of echo chambers and tempers fears of partisan segregation since only a small segment of the population are likely to find themselves in an echo chamber. We argue that single media studies and studies which use narrow definitions and measurements of being in an echo chamber are flawed because they do not test the theory in the realistic context of a multiple media environment.;The echo chamber is overstated: the moderating effect of political interest and diverse media;Dubois, E. & Blank, G.;2018;Blank, G.;Information Geography and Inequality
Recent studies have enhanced our understanding of digital divides by investigating outcomes of Internet use. We extend this research to analyse positive and negative outcomes of Internet use in the United Kingdom. We apply structural equation modelling to data from a large Internet survey to compare the social structuration of Internet benefits with harms. We find that highly educated users benefit most from using the web. Elderly individuals benefit more than younger ones. Next to demographic characteristics, technology attitudes are the strongest predictors of online benefits. The harms from using the Internet are structured differently, with educated users and those with high levels of privacy concerns being most susceptible to harm. This runs counter to intuitions based on prior digital divide research, where those at the margins should be most at risk. While previous research on digital inequality has only looked at benefits, the inclusion of harms draws a more differentiated picture.;Benefits and harms from Internet use: A differentiated analysis of Great Britain;Blank, G. & Lutz, C.;2016;Blank, G.;Information Geography and Inequality
Hundreds of papers have been published using Twitter data, but few previous papers report the digital divide among Twitter users. British Twitter users are younger, wealthier, and better educated than other Internet users, who in turn are younger, wealthier, and better educated than the off-line British population. American Twitter users are also younger and wealthier than the rest of the population, but they are not better educated. Twitter users are disproportionately members of elites in both countries. Twitter users also differ from other groups in their online activities and their attitudes. These biases and differences have important implications for research based on Twitter data. The unrepresentative characteristics of Twitter users suggest that Twitter data are not suitable for research where representativeness is important, such as forecasting elections or gaining insight into attitudes, sentiments, or activities of large populations. In general, Twitter data seem to be more suitable for corporate use than for social science research.;The Digital Divide Among Twitter Users and Its Implications for Social Research;Blank, G.;2016;Blank, G.;Information Geography and Inequality
Sociological studies show that Internet access, skills, uses, and outcomes vary between different population segments. However, we lack differentiated statistical evidence of the social characteristics of users of distinct social media platforms. We address this issue using a representative survey of Great Britain and investigate the social characteristics of six major social media platforms. We find that age and socioeconomic status are driving forces of several—but not all—of these platforms. The findings suggest that no social media platform is representative of the general population. The unrepresentativeness has major implications for research that uses social media as a data source. Social media data cannot be used to generalize to any population other than themselves.;Representativeness of Social Media in Great Britain: Investigating Facebook, LinkedIn, Twitter, Pinterest, Google+, and Instagram;Blank, G. & Lutz, C.;2017;Blank, G.;Information Geography and Inequality
Everybody eats to live but food is more than nutrition. Food is a powerful carrier of cultural meaning. Food touches many core issues in sociology and social life, like social stratification, poverty, and prestige, class, gender roles, consumption, nationalism, and globalization. These cross‐pressures generate rich collections of meaning. Local meanings as they are modified by culture, institutional context, class, politics, and history make food a rewarding research subject.;Consumption, Food and Cultural;Blank, G.;2016;Blank, G.;Information Geography and Inequality
These cultures of the internet are significant because they suggest that stratification online is strongly influenced by cultural values and meaning because they influence social mobility, skill development, and digital choice.;Cultural Stratification on the Internet: Five Clusters of Values and Beliefs among Users in Britain;Dutton, W. & Blank, G.;2015;Blank, G.;Information Geography and Inequality
William Dutton and Grant Blank identify groups of internet users with different ‘cultures’, and challenge conventional thinking about internet demographics.;Cultures on the Internet;Dutton, W. & Blank, G.;2015;Blank, G.;Information Geography and Inequality
Research on Internet use typically has been concerned with issues of access or activities people do online. This research has been fruitful, but it has not been fully linked to larger theories of stratification. Although Max Weber says little about technology, his general approach to studying society suggests concepts other than access and demographics will be important. From his perspective, the primary sources of social stratification are class, status, and power. As the Internet has become more important, it has moved to a steadily more central position in the stratification system. Thus, it is important to look at Internet use through a Weberian lens, asking how class, status, and power help explain who participates in what online activities.;Examining Internet Use Through a Weberian Lens;Blank, G. & Groselj, D.;2015;Blank, G.;Information Geography and Inequality
"This paper examines patterns of beliefs and attitudes among Internet users in Britain. Using data from the 2013 Oxford Internet Survey (OxIS), we employ principal components analysis to identify four sets of beliefs and attitudes that capture most variance across fourteen items. This four dimensions were called enjoyable escape, instrumental efficiency, social facilitator, and problem generator. Hierarchical cluster analysis was then used to identify groups of respondents who cluster together across these four dimensions. Each cluster represents a distinctive cultural perspective on the Internet. The five clusters, which we call cultures of the Internet, and the estimated portion of the British population are: e-Mersives (12%), who are fully at home in and positive about the digital environment, using it for entertainment as well as work and social life; Techno-pragmatists (17%), who use the Internet for instrumental and work-related purposes; the Cyber-savvy (19%), who understand and use all aspects of the Internet, but who are also aware of the risks online; and the Cyber-moderates (37%), who are blaise, not responding strongly to either the positive or negative aspects of the Internet; and the Adigitals (14%), who are online but not happy about it, as they harbour overwhelmingly negative beliefs and attitudes about the Internet. These cultures are not surrogates for demographic patterns, but they each have distinctive demographic characteristics although they challenge conventional conceptions, such as of the born digital generation. They also are related strongly to patterns of use and attitudes toward government regulation of the Internet, suggesting the need for more focused and comparative research on cultures of the Internet.";Cultures of the Internet: Five Clusters of Attitudes and Beliefs Among Users in Britain;Dutton, W. & Blank, G.;2014;Blank, G.;Information Geography and Inequality
During the 1980s and 1990s many early users saw the Internet through a strongly positive, even utopian lens. In 2019 we are in a very different place. Critics see the empowerment of the public to be an illusion. Instead of cooperation and community promised in the early years, reporting on the Internet emphasises dangers to children, potential financial losses and privacy breaches. Instead of democracy we hear about Russian interference in elections. One summary of 2018 says “Malware authors continue to innovate, find new infection vectors and better obfuscate their wares. Heading into 2019, you can bet that cybercriminals will do everything in their power to become even more effective and virulent.” (O’Donnell 2019). Reporting often emphasises the rise in problems (Malwarebytes Labs 2019). In 2019 there is a widespread narrative arguing that the Internet is a dangerous place. This perspective emphasises the risks and the potential for significant harm. Britain is at the forefront of this narrative. In February the British government published a report called Disinformation and “Fake News”’ (House of Commons 2019) and in April an ‘Online Harms’ White Paper (Ofcom 2019). These reports emphasise a need to address online harms. For example, the ‘Online Harms’ White Paper briefly acknowledges the value of the Internet, and then goes on to say “there is growing evidence of the scale of harmful content and activity that people experience online. Online services can be used to spread terrorist propaganda and child abuse content, they can be a tool for abuse and bullying and they can be used to undermine civil discourse.” (Ofcom 2019:12) In this context, we believe it is important to look at the Internet through the views and behaviour of Internet users in Britain. We bring OxIS data to bear on the state of the Internet in Britain in 2019, with a special emphasis on the effects of harmful content and activity. The perspective in this paper is that of Internet users. With systematic comparisons to the last wave of OxIS in 2013 we answer questions like: What evidence suggests widespread threats? What are the major trends in online threats when viewed from the perspective of Internet users in Britain? The paper is divided into three parts. First, we describe the basic social aspects of the Internet in Britain in 2019 using demographic information. Then we look at the experience of users with respect to harms and threats. To what extent have users experienced problems online, and have their experiences influenced their behavior? Finally, we compare users and non-users. Does the narrative about harms seem to influence non-users as well as users?;Perceived Threats to Privacy Online: The Internet in Britain;Blank, G., Dutton, W. & Lefkowitz, J.;2019;Blank, G.;Information Geography and Inequality
"My paper (Blank 2013) makes two fundamental points about content production and inequality. First, content production can be meaningfully grouped into several discrete types. Using British data from the 2011 Oxford Internet Survey (OxIS), I find three types, which I called ‘skilled content’, ‘social and entertainment content’, and ‘political content’. Second, these types are produced by distinctive social categories. Social stratification variables are irrelevant to skilled content; social and entertainment content is more likely to be produced by pcople of Jower social status; and political content is produced by highly educated elites. This implies that the effects of social stratification vary depending on who produces it and on the type of content.";COMMENT SOCIAL STRATIFICATION AND CONTENT PRODUCTION: A Response;Blank, G.;2013;Blank, G.;Information Geography and Inequality
Under the joint sponsorship of NatCen Social Research, Sage Publications and the Oxford Internet Institute, the New Social Media, New Social Science (NSMNSS) network has been exploring the challenges of new social media (NSM). These challenges are described here as a series of strengths and weaknesses. Most challenges are particularly important for quantitative research. NSM provides rich settings for qualitative researchers. In NSM the qualitative focus on meaning and developing understanding through participation and progressive collection of data remains consistent with traditional approaches. Understanding meaning is not simplified by digital data. In general, NSM has had less impact on qualitative research.;Blurring the Boundaries: New Social Media, New Social Science (NSMNSS);Blank, G.;2013;Blank, G.;Information Geography and Inequality
"Until the Internet arrived, content creation and distribution was always an expensive, difficult process. With the Internet it is dramatically easier, faster, and cheaper. Some argue that this will move creation out of the hands of elites and lead to wider participation in the public sphere and to enhanced democracy. This paper makes three contributions to this debate. First, it uses a national random sample of the British population. This is much broader than most prior work. Second, it creates the first evidence-based typology of Internet content creation, identifying three types named ‘skilled content’, ‘social and entertainment content’, and ‘political content’. The implicit assumption of many researchers that only one type of content exists is not accurate. Third, using multivariate logistic regression it shows the characteristics of different populations that produce each type of content. Elites have no impact on creation of skilled content. Social and entertainment content is more likely to be created by non-elites. Only creation of political content is significantly and positively associated with elite status. These results clarify inconsistencies in prior studies. Each type of content is produced by a different kind of creator. Thus, type is more than just content; it also describes differences in who creates the content. The varying relationships between elite status and content creation suggest that it is important for future research to pay close attention to the type of content under study when considering possible democratization of creation.";WHO CREATES CONTENT? Stratification and content creation on the Internet;Blank, G.;2013;Blank, G.;Information Geography and Inequality
This paper makes three contributions: first, we suggest a clear, concise definition of Web 2.0, something that has eluded other authors, including the Tim O'Reilly the originator of the concept. Second, prior work has focused largely on the implications of Web 2.0 for producers of content, usually corporations or government agencies. This paper is one of the few analyses of Web 2.0 from the point of view of users. Third, we characterize the creative activity of Web 2.0 users. In addition to their active content production, they are unusually active users of the Internet for entertainment. In multivariate models predicting Web 2.0, the most consistently important variables are technical ability, comfort revealing personal data and, particularly, Web 2.0 confidence. These variables suggest that despite the apparent simplicity of FaceBook or of typing a book review on Amazon, ability remains very important in the eyes of users. For many, there appears to be something daunting about contributing to Web 2.0 activity and many potential users remain, rightly or wrongly, uncertain of their ability to make a contribution. We conclude that the study of Web 2.0 can tell us much about how the Internet is unique, and that it warrants a significant scholarly attention.;THE PARTICIPATORY WEB: A user perspective on Web 2.0;Blank, G. & Reisdorf, B.;2012;Blank, G.;Information Geography and Inequality
"The authors describe changes in user’s trust on the Internet in Britain between 2003 and 2009, and show how the relationship between age and trust can be explained by a combination of experience with the Internet and general attitudes toward technology. The comparison uses 2003 results reported by Dutton and Shepherd (2006) versus similarly sampled 2009 data. The authors examine two sets of dependent variables—perceptions of trust and risk on the Internet and use of the Internet for e-commerce, an anticipated impact of trust. The authors find that indicators of trust are related to experience with the technology, although this relationship is less important in 2009 than it was in 2003. The authors also find that trust is influenced by general attitudes toward technology. When both experience on the Internet and technology attitudes are controlled, the relation between indicators of trust and age disappears. This finding is particularly interesting since age is usually an important predictor of many aspects of the Internet; it suggests that the role of age can be mitigated by addressing the degree to which older individuals tend to have less experience with the Internet and more scepticism about the role of technology in society. Interventions could address both of these determinants of distrust.";Age and Trust in the Internet: The Centrality of Experience and Attitudes Toward Technology in Britain;Blank, G. & Dutton, W.;2011;Blank, G.;Information Geography and Inequality
This paper provides a case study of the changing patterns of news production and consumption in the UK that are being shaped by the Internet and related social media. Theoretically, this focus addresses concern over whether the Internet is undermining the Fourth Estate role of the press in liberal democratic societies. The case study draws from multiple methods, including survey research of individuals in Britain from 2003–2011, analysis of log files of journalistic sites, and interviews with journalists. Survey research shows a step-jump in the use of online news since 2003 but a levelling off since 2009. However, the apparent stability in news consumption masks the growing role of social network sites. The analyses show that the Fourth Estate – the institutional news media – is using social media to enhance their role in news production and dissemination. However, networked individuals have used social media to source and distribute their own information in ways that achieve a growing independence from the Fourth Estate journalism. As more information moves online and individuals become routinely linked to the Internet, an emerging Fifth Estate, built on the activities of networked individuals sourcing and distributing their own information, is developing a synergy with the Fourth Estate as each builds on and responds to the other in this new news ecology. Comparative data suggests that this phenomenon is likely to characterize the developing news ecology in other liberal democratic societies as well, but more comparative research is required to establish the validity of this model.;Social Media in the Changing Ecology of News: The Fourth and Fifth Estate in Britain;Newman, N., Dutton, W. & Blank, G.;2012;Blank, G.;Information Geography and Inequality
This report focuses on the emergence of “next generation users” in Britain, Internet users who are developing a new pattern of Internet access. We follow the emerging next generation users throughout the next eight sections that summarise the details and highlights of the 2011 Oxford Internet Survey. The report closes with a methodological appendix. The first detailed section of the report focuses on describing the diffusion of the Internet as an innovation in information and communication technology (ICT). The second section focuses on the characteristics and attitudes of Internet users. The third part describes how people with different backgrounds use the Internet, followed by a fourth part which looks specifically at the use of the Internet in politics and government. The fifth section turns to the question of how the Internet is reshaping friendships and social networks. The sixth section looks at the social implications of Internet use. The seventh section examines beliefs and attitudes of individuals about the control and regulation of the Internet. The final section examines the key issue of exclusion, either by social and economic divides or by personal choice, describing non-users and former users. Each section opens with an overview of the trends described in the section.;Next Generation Users: The Internet in Britain;Dutton, W. & Blank, G.;2011;Blank, G.;Information Geography and Inequality
Sociologists have focused on the analysis of structural conditions, ideal types, typical experiences, institutions, and other ways to generalize about social life. They have had little to say about particular individuals or particular objects of study. Sociologists of art have followed the same approach and, consequently, have rarely confronted artworks directly. The contributors to Art from Start to Finish, edited by Howard S. Becker, Robert R. Faulkner, and Barbara Kirshenblatt-Gimblett, break new ground by studying individual works, focusing on a deceptively simple question: How do we know when an artwork is finished? This question deserves attention because the ambiguity of when a work is finished gives privileged access ses that go into making art.;Art from Start to Finish: Jazz, Painting, Writing, and Other Improvisations;Blank, G.;2006;Blank, G.;Information Geography and Inequality
ONE OF THE FIRST RULES OF EFFECTIVE TEACHING, AT LEAST IN CURRENT western educational theory, is taking students’ needs and in- terests into account when planning for classes. When I began to teach English to Chinese students in a private school in western China in August 2001, I asked my eighth-grade students what they enjoyed most about their previous English classes. ‘‘Music,’’ one student replied. ‘‘Learning American songs,’’ clarified another. ‘‘This could be enjoyable,’’ I thought, thinking of the CDs I had brought from the United States and the insight into adolescent music tastes gained from teaching high school students the year before. ‘‘What is your favorite American song?’’ I asked. ‘‘‘Yesterday Once More’’’ Wang Xiaowang, the class leader, asserted without hesitation. ‘‘I’m sorry, but I don’t know that one,’’ I said, puzzled. ‘‘Could you sing it for me?’’ Wang Xiaowang gave the starting note and the other twelve stu- dents in the class joined in, some of them closing their eyes to sing with passion Karen Carpenter’s ballad of lost love. As they sang, I wondered how all of these students knew this tune when I had never heard it. As I taught this class and others during my two years in China, I enjoyed singing American songs with my students, though I quickly learned that some tunes would go over well with my students, while others would induce little enthusiasm from them. The students had definitive tastes in American music, unexpected tastes which defied my own preferences and those of teenagers I had previously taught in the United States. After much trial and error, I narrowed the ‘‘surefire’’ genres of American music for Chinese students to easy listening, country, and softer covers of ‘‘oldies.’’;“Country Roads” to Globalization: Sociological Models for Understanding American Popular Music in China;Rupke, H. & Blank, G.;2009;Blank, G.;Information Geography and Inequality
When the answer is the number ‘‘42’’ (or actually ‘‘forty-two’’) some know instantly that the question is ‘‘The Great Question’’ concerning ‘‘Life, the Universe and Everything ...’’ (Adams 1986, p. 128). This demonstrates that even when we know the answer to a question its meaning and usefulness are not always obvious. Context is required. This is especially true for quantitative data. More information is needed in order to understand the numbers and transform data into useful knowledge. This further information is itself data, and thus metadata or ‘‘data about data’’. The importance of context and metadata are widely recognized in the social science community. This paper discusses a project to provide standardized metadata to document social science datasets: the Data Documentation Initiative (DDI). The two authors worked together on the DDI committee and are authors of a previous article on the DDI (Blank and Rasmussen 2004). We begin by discussing the fundamental problems of social science dataset documentation. We focus on the value that standardized documentation in the form of the DDI creates for the social science community. To understand the virtues of the new, we look brieﬂy at some historical documentation standards. We describe the fundamental features of the DDI standard by describing a current application. The DDI is a standard still being developed and we close with a discussion of future development plans and a summary of the value of the DDI for research.;The data documentation initiative: a preservation standard for research;Rasmussen, K. & Blank, G.;2007;Blank, G.;Information Geography and Inequality
The American Sociological Association section on Sociology and Computing, as it was named from 1995 to 2002, faced repeated challenges during the decade of the 1990s. This article traces changes in the sociological audiences concerned with computing during that decade and discusses how they influenced the way the section coped with the routinization of computer use in sociology, the rise of MicrosoftWindows, the increasing use of computers in teaching, and the rise of the Internet.;Communication and Information Technologies: A History of the Middle Years;Blank, G.;2006;Blank, G.;Information Geography and Inequality
Effective secondary analysis of social science data requires good documentation. Especially because Internet access has become standard, the problems of reading and understanding the contents of data files have become acute. Resolving these problems requires standards for documenting data, as well as standard formats for both data and documentation that can be read and displayed by computers and software anywhere in the world. To define a documentation standard, representatives of North American and European survey research and data archive organizations have created a Data Documentation Initiative (DDI). This article discusses the value and significance of that effort for the social sciences.;The Data Documentation Initiative: The Value and Significance of a Worldwide Standard;Blank, G. & Rasmussen, K.;2004;Blank, G.;Information Geography and Inequality
This article reports the author’s experience teaching sociology graduate students howto analyze qualitative data. The course focused on teaching practical skills of defining coding categories, coding text, analyzing coded text, and writing up the results of analysis. The assignments used Qualrus software to give students hands-on practice doing all of these. The course was enthusiastically received and will become a permanent part of the sociology methods course offerings at American University.;Teaching Qualitative Data Analysis to Graduate Students;Blank, G.;2004;Blank, G.;Information Geography and Inequality
Global debate over the impact of algorithms and search on shaping political opinions has increased following dramatic election results in Europe and the US. Powerful images of the Internet enabling access to a global treasure trove of information have shifted to worries over whether those who use search engines and social media are being fed inaccurate, false, or politically targeted information that distorts public opinion. There are serious questions over whether biases embedded in the algorithms that drive search engines and social media have major political consequences, such as creating filter bubbles or echo chambers. For example, do search engines and social media provide people with information that aligns with their beliefs and opinions or do they challenge them to consider countervailing perspectives? Most generally, the predominant concern is do these media have a major impact on public opinion and political viewpoints, and if so, for the better or worse. This study addresses these issues by asking Internet users how they use search, social media, and other important media to get information about political candidates, issues, and politics generally, as well as what difference it makes for individuals participating in democratic processes. We conducted an online survey of Internet users in seven nations: Britain, France, Germany, Italy, Poland, Spain, and the US. This report is part of a Quello Center search project at Michigan State University, entitled “The Part Played by Search in Shaping Political Opinion”, supported by Google. The authors are grateful to Jon Steinberg and his colleagues at Google for supporting independent, academic research in this area. All of the views expressed in this report are those of the authors and do not necessarily reflect the views of the Quello Center or any of the organizations supporting this research.;Search and Politics: The Uses and Impacts of Search in Britain, France, Germany, Italy, Poland, Spain, and the United States;Dutton, W., Reisdorf, B. & Dubois, E. et al.;2017;Blank, G.;Information Geography and Inequality
Global debate over the impact that algorithms and search on shaping political opinions has been increasing in the aftermath of controversial election results in Europe and the US. Powerful images of the Internet enabling access to a global treasure trove of information have shifted to worries over the degree to which those who use social media, and online tools such as search engines, are being fed inaccurate, fake, or politically-targeted information that could distort public opinion and political change. There are serious questions raised over the political implication of any biases embedded in the algorithms that drive search engines and social media. Do digital media biases shape access to information shaping public opinion? To address these issues, we conducted an online survey of stratified random samples in seven nations, including Britain, France, Germany, Italy, Poland, Spain, and the US. We asked Internet users how they use search, social media, and other media, for political information, and what difference it makes for them. The findings cast doubt on technologically deterministic perspectives on search, such as filter bubbles. For example, our findings show that search is among an array of media consulted by those interested in politics. Internet users are not trapped in a bubble on a single platform. Another deterministic narrative is around the concept of echo chambers, where social media enable users to cocoon themselves with likeminded people and viewpoints. However, most of those interested in politics search for and double check problematic political information, and expose themselves to a variety of viewpoints. Thus, prevailing views on search and politics not only over-estimate technical determinants, but also underestimate the social shaping of the Internet, social media, and search. National media cultures and systems play an important role in shaping search practices, along with individual differences in political and Internet orientations. The findings suggest there are disproportionate levels of concern, often approaching panic, over the bias of search and social media, and that targeted interventions could help reduce the risks associated with fake news, filter bubbles, and echo chambers.;Social Shaping of the Politics of Internet Search and Networking: Moving Beyond Filter Bubbles, Echo Chambers, and Fake News;Dutton, W., Reisdorf, B. & Dubois, E. et al.;2017;Blank, G.;Information Geography and Inequality
The digital transformation is driving revolutionary innovations and new market entrants threaten established sectors of the economy such as the automotive industry. Following the need for monitoring shifting industries, we present a network-centred analysis of car manufacturer web pages. Solely exploiting publicly-available information, we construct large networks from web pages and hyperlinks. The network properties disclose the internal corporate positioning of the three largest automotive manufacturers, Toyota, Volkswagen and Hyundai with respect to innovative trends and their international outlook. We tag web pages concerned with topics like e-mobility & environment or autonomous driving, and investigate their relevance in the network. Sentiment analysis on individual web pages uncovers a relationship between page linking and use of positive language, particularly with respect to innovative trends. Web pages of the same country domain form clusters of different size in the network that reveal strong correlations with sales market orientation. Our approach maintains the web content’s hierarchical structure imposed by the web page networks. It, thus, presents a method to reveal hierarchical structures of unstructured text content obtained from web scraping. It is highly transparent, reproducible and data driven, and could be used to gain complementary insights into innovative strategies of firms and competitive landscapes, which would not be detectable by the analysis of web content alone.;Mining the Automotive Industry: A Network Analysis of Corporate Positioning and Technological Trends;Stoehr, N., Braesemann, F. & Frommelt, M. et al.;2020;Braesemann, F.;Information Geography and Inequality
In today’s economies, knowledge is the key ingredient for prosperity. However, it is hard to measure this intangible asset appropriately. Standard economic models mostly rely on common measures such as enrollment rates and international test scores. However, these proxies focus rather on the quality of education of pupils than on the distribution of knowledge among the whole population, which is increasingly defined by alternative sources of education such as online learning platforms. As a consequence, the economically relevant stock of knowledge in a region is only roughly approximated. Furthermore, they are abstract in content, and both capital-, and time-consuming in census. This paper proposes to explore Wikipedia data as an alternative source of capturing the knowledge distribution on a narrow geographical scale. Wikipedia is by far the largest digital encyclopedia worldwide and provides data on usage and editing publicly. We compare Wikipedia usage worldwide and edits in the U.S. to existing measures of the acquisition and stock of knowledge. The results indicate that there is a significant correlation between Wikipedia interactions and knowledge approximations on different geographical scales. Considering these results, it seems promising to further explore Wikipedia data to develop a reliable, inexpensive, and real-time proxy of knowledge distribution around the world.;An Exploration of Wikipedia Data as a Measure of Regional Knowledge Distribution;Stephany, F. & Braesemann, F.;2017;Braesemann, F.;Information Geography and Inequality
Thanks to the availability of large online data sets, it has be- come possible to quantify success in different fields of human endeavour. The study presented here contributes to this literature in evaluating the effect of social media activity, as a means of ’self-branding’, to increase the chances of models being elected for the Playboy Magazine’s Play- mate of the Year award. We hypothesise that candidates who actively manage their Instagram accounts can increase their likelihood to win the award: they use social media to gain more followers, who then might vote for them in the award polls. The findings indicate that social media activity actually has predictive capacity to estimate the outcome of the award. We find evidence that candidates who manage their social media accounts more actively than other candidates have a higher probability to become Playmate of the Year. The findings underline the benefits of social media self-branding as a driver of popularity and success. ;Social media self-branding and success: Quantitative evidence from a model competition;Braesemann, F. & Stephany, F.;2019;Braesemann, F.;Information Geography and Inequality
This study presents a structured investigation of the most important causes for delay in commercial real estate transactions. It assesses the potential of digital technologies such as “Blockchain”, “Property Passports” or “Automated Valuation Models” to make transactions faster and cheaper. The authors conduct a focus group interview to identify the individual steps and the parties involved in real estate transactions. Subsequently, the authors discuss the prospects of digital technologies based on semi-structured interviews with real estate professionals and PropTech executives, and a comprehensive screening of technological solutions offered by PropTech firms. The lack of an up-to-date, single pool of standardised property information turns out to be the most critical cause for delay in real estate transactions. However, the most promising technologies to mitigate this problem, in particular digital property passports summarising all relevant building information, face substantial barriers to adoption. The real estate industry has so far not been willing to more openly share data, which is a pre-requiste for the successful introduction of property passports. In addition, the principle of caveat emptor makes a lengthy due diligence process essential for buyers. The authors conclude that industry-wide collaborations are necessary to help major efficiency gaining technologies to break through. Insurance products should accompany property data log books to guarantee the quality of data provided. This study considers the potential impact of technologies in the wider context of the complete real estate transaction process. It identifies the major phases of that process and the associated bottlenecks. The authors gather evidence both from industry experts and PropTech executives and contrast their views regarding the potential of digital technologies to remove those bottlenecks.;Can digital technologies speed up real estate transactions?;Saull, A., Baum, A. & Braesemann, F.;2020;Braesemann, F.;Information Geography and Inequality
While the coronavirus spreads, governments are attempting to reduce contagion rates at the expense of negative economic effects. Market expectations plummeted, foreshadowing the risk of a global economic crisis and mass unemployment. Governments provide huge financial aid programmes to mitigate the economic shocks. To achieve higher effectiveness with such policy measures, it is key to identify the industries that are most in need of support. In this study, we introduce a data-mining approach to measure industry-specific risks related to COVID-19. We examine company risk reports filed to the U.S. Securities and Exchange Commission (SEC). This alternative data set can complement more traditional economic indicators in times of the fast-evolving crisis as it allows for a real-time analysis of risk assessments. Preliminary findings suggest that the companies' awareness towards corona-related business risks is ahead of the overall stock market developments. Our approach allows to distinguish the industries by their risk awareness towards COVID-19. Based on natural language processing, we identify corona-related risk topics and their perceived relevance for different industries. The preliminary findings are summarised as an up-to-date online index. The CoRisk-Index tracks the industry-specific risk assessments related to the crisis, as it spreads through the economy. The tracking tool is updated weekly. It could provide relevant empirical data to inform models on the economic effects of the crisis. Such complementary empirical information could ultimately help policymakers to effectively target financial support in order to mitigate the economic shocks of the crisis.;The CoRisk-Index: A data-mining approach to identify industry-specific risk assessments related to COVID-19 in real-time;Stephany, F., Stoehr, N. & Darius, P. et al.;2020;Braesemann, F.;Information Geography and Inequality
In every social transaction there is an element of trust. The degree to which we trust others, called generalized trust, is assumed to benefit from interaction with different social groups. In the trust literature, it is opposed by particularized trust, which represents our mutual confidence in individuals close to us, for example, family members and friends. This study, based on a survey with 634 university students from Austria, questions the strict dichotomy between the two trust types. Our results advocate for a third, group determined type of trust. This additional trust dimension is measured by the number of groups individuals participate in. It changes fluently between particularized and generalized trust, depending on measures of group context, like frequency of interaction or group size. Our findings show that generalized trust increases with the number of groups one feels belonging to. People with less diverse social interaction, however, have more trust in their peers than in strangers.;Between Bonds and Bridges: Evidence from a Survey on Trust in Groups;Braesemann, F. & Stephany, F.;2020;Braesemann, F.;Information Geography and Inequality
The development of digital technologies such as Machine Learning can be described empirically as a co-evolving network based on online platform data. Here, we construct a network of technologies related to machine learning based on data from Stack Overflow, the world’s largest question-and-answer website for programming questions.1 This network reveals the changing centrality of machine learning topics, libraries, and related programming languages over time as the network links rewire when novel technologies are introduced. It thus allows for understanding the development of the field as combinatorial technological evolution, shaped by the replacement of older technologies by novel ones. The data can be used to test network models on innovation and novelty, and on creative destruction.;The Evolution of Digital Technologies: A Network Perspective on Machine Learning;Braesemann, F.;2019;Braesemann, F.;Information Geography and Inequality
Whether behavioural economics has a fundamental influence on economics is debated by behavioural and heterodox economists as well as by methodologists and historians of economics. At the core of this debate is the question whether behavioural economics is shaped by large-scale content imports from psychology or whether these transfers have been too selective to challenge dominant approaches in economics. This study contributes to the debate in analysing a variety of bibliographic data from the disciplinary boundary between economics and psychology. Two data sets from the boundary of behavioural economics and psychology are compared to sets of economic and psychology publications in quantifying the use of mathematics, the share of empirical contributions, the authors' academic background, and their cross-citations via network analysis. In contrast to proposals made by some methodologists and behavioural economists, the statistical results confirm content transfers from psychology via behavioural economics only to a limited extend. The observed level of interaction provides evidence for a selective import of specific psychological findings by a small number of established investigators in behavioural economics. These findings were then intensively debated as divergences from rationality within the growing, but econ-centred community of behavioural economists.;How behavioural economics relates to psychology – some bibliographic evidence;Braesemann, F.;2018;Braesemann, F.;Information Geography and Inequality
The Internet, like railways and roads in the past, is paving innovation and alters the way in which citizens, consumers, businesses, and governments function and interact with each other. This digital revolution is empowering societies. It opens new, effective, and scalable services for governments and the private sector. It provides us with a more adaptive, data-driven approach to decision making in many aspects of our life. The digitalisation is particularly relevant for developing countries, as they can seize the opportunity for leapfrogging in order to become part of the global digitalised economy. With the example of Eastern Europe and Central Asia, this work illustrates how openly available online data can be used to identify, monitor, and visualise trends in digital economic development. Our interactive online dashboard allows researchers, policy-makers, and the public to explore four aspects of digital development: E-services, online labour markets, online knowledge creation and access to online knowledge.;Measuring Digital Development with Online Data: Digital Economies in Eastern Europe and Central Asia;Braesemann, F. & Stephany, F.;2020;Braesemann, F.;Information Geography and Inequality
Whether behavioural economics has a fundamental influence on economics is debated by behavioural and heterodox economists as well as by methodologists and historians of economics. At the core of this debate is the question whether behavioural economics is shaped by large-scale content imports from psychology, or whether these transfers have been too selective to challenge dominant approaches in economics. This study contributes to the debate in analysing a variety of bibliographic data from the disciplinary boundary between economics and psychology. Two datasets from the boundary of behavioural economics and psychology are compared to sets of economic and psychology publications in quantifying the use of mathematics, the share of empirical contributions, the authors’ academic background, and their cross-citations via network analysis. In contrast to proposals made by some methodologists and behavioural economists, the statistical results confirm content transfers from psychology via behavioural economics only to a limited extend. The observed level of interaction provides evidence for a selective import of specific psychological findings by a small number of established investigators in behavioural economics. These findings were then intensively debated as divergences from rationality within the growing, but econ-centered community of behavioural economists.;How Behavioural Economics Relates to Psychology - Some Bibliographic Evidence;Braesemann, F.;2019;Braesemann, F.;Information Geography and Inequality
"Aiming to explain the European divide with respect to social and political values, scholars in the past have relied on a simplified four- (or even two-) dimensional regime model which tranches the continent according to the social capacities of its inhabitants. This ""cartography"" of ""Social Europe"" proves to be outdated by the results presented in this study which re-measures the social capital landscape in Europe. In this work, we apply a factor analysis model to the most commonly used approximations of social capital on the European Social Survey. In addition, we explore, as a novelty in social capital literature, a classification tree to model generalized trust. The analysis shows that three distinct dimensions of social capital measures are important in Europe: additionally to generalised social capital, which is usually approximated by generalised trust, there is one dimension of civic engagement and one of communitarian values. This distinction leads to a new social landscape of Europe, which highlights the relevance of considering regional and cross-border clusters in all relevant social capital dimensions.The results of the non-parametric model reveal that Protestantism and education are good benchmarks to classify trust on an individual level. Based on these findings we argue for the necessity of policies with a regional focus that take the different sub-national structures of social capacity in Europe into account. We re-measure the European Social Capital landscape using current data and provide a novel non-parametrical statistical method from data science for this purpose.";United in Diversity? An Empirical Investigation on Europe's Regional Social Capital;Braesemann, F. & Stephany, F.;2019;Braesemann, F.;Information Geography and Inequality
Taking advantage of the 'information revolution' is a priority in various national development strategies. Eager to tap into economic and social opportunities potentially afforded by increased access to digital information, many governments of developing countries have envisioned policies that guide their transformation into so-called 'knowledge economies'. However, despite its importance to contemporary economic development, the concept itself is rarely clearly defined, operationalised, or effectively measured. The few indices that have measured the state of knowledge and information economies around the world employ significantly different sets of variables from one another, further adding to the vagueness of the concept. These indices rely on the accuracy and cross-sectional as well as longitudinal representativeness of their data sources, which can be called into question in low-income contexts. We thus propose the construction of a Digital Knowledge Economy Index, which will additionally account for capacities and skills that are quantifiable via measuring content-creation and participation directly through digital platforms, such as the code-sharing platform GitHub, encyclopaedia Wikipedia and domain name registrations. With this approach, conventional data sources - national statistics and expert surveys - can be complemented by data that is collected via the internet and that reflects the underlying digital content creation, capacities and skills of the population. An index that combines traditional and novel data sources may provide a more revealing view of the status of the world's digital knowledge economy and highlight where the availability of digital resources may actually reinforce inequalities in the age of data.;Measuring the contours of the global knowledge economy with a digital index;Ojanperä, S., Graham, M. & Zook, M.;2016;Ojanperä, S.;"Information Geography and Inequality; Digital Economies "
The increasing digital connectivity has sparked many hopes about the democratization of information and knowledge production in Sub-Saharan Africa. To investigate the patterns of knowledge creation in the region and between other world regions we examine three key metrics: spatial distributions of academic articles (traditional knowledge production) and collaborative software development and Internet domain registrations (digitally-mediated knowledge production). We find that, contrary to the expectation of digital content to be more evenly geographically distributed than academic articles, the global and regional patterns of collaborative coding and domain registrations are more uneven than those of academic articles. Despite hopes of democratization afforded by the information revolution, Sub-Saharan Africa produces a lower share of digital content than academic articles. Our results suggest that the factors often framed as catalysts in the transformation to a knowledge economy do not relate to the three metrics uniformly. While connectivity is an important enabler of digital content creation, it seems to be only a necessary, not a sufficient condition: wealth, innovation capacity, and public spending on education are also important factors.;Engagement in the knowledge economy: Regional patterns of content creation with a focus on Sub-Saharan Africa;Ojanperä, S., Graham, M. & Straumann, R. et al.;2017;Ojanperä, S.;"Information Geography and Inequality; Digital Economies "
"In this document, we offer a review of recent literature on the future of work. This review was commissioned by The Alan Turing Institute for the purpose of informing the Institute’s research strategy aiming to further data science and artificial intelligence research to address real-world problems. Using a critical review method, the report synthesises key findings about the future of work focusing on three main areas: broad research findings, emerging research directions, and innovative data science research directions. The first part of the review summarises and discusses changes in the nature and creation of jobs, assignments, and tasks; changing organisation of work and production; varying impacts of the changing nature of work on society; and the governance of these changes through politics, policy and institutions. The second section addresses potential drivers of the changing nature of work; disparate impacts of technology on different tasks; challenges for young people to boost employability; impacts of the changing nature of work on the disenfranchised; and proposals for policies and governance models to manage the transitions related to the future of work. The third section discusses research approaches and findings around the susceptibility of tasks and assignments to computerisation; industrial diversification and data-driven policy tools; and development of online labour markets.";Data science, artificial intelligence and the futures of work;Ojanperä, S., O'Clery, N. & Graham, M.;2018;Ojanperä, S.;"Information Geography and Inequality; Digital Economies "
"Considerable work has been done to understand and improve the resilience of individual infrastructure components. However, systems of components, or even systems of systems, are far less well understood. Cascade effects, where the loss of one infrastructure affects others, is a major source of vulnerability which can lead to catastrophic disruptions of essential services. Interdependencies can also lead to large-scale failures when even a single component is disrupted and results in ‘cascading’ failures within and between networks. This is particularly true for power systems, as many other lifeline infrastructure systems rely on electricity. In this study we review the literature and give a primer on the vulnerabilities of networked energy infrastructure. Several recurrent themes emerge from across different systems: (1) Electricity is essential for many lifeline infrastructure systems to function; (2) Electrical distribution systems are particularly vulnerable to disruption from natural and manmade hazards; (3) Highly networked systems can be unstable even when their individual components are functioning as intended; (4) Redundancy and network density can increase reliability but also increase the likelihood of cascade effects when failures do occur; (5) Disruption of ports and roads can limit fuel supplies for generators and replacement components. Based on these insights, this study offers suggestions for further research and policy actions.";Vulnerabilities of Networked Energy Infrastructure: A Primer ;Schweikert, A., Nield, L. & Otto, E. et al.;2019;Ojanperä, S.;Information Geography and Inequality